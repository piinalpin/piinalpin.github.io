[{"categories":["Documentation"],"content":"Make Terminal or Powershell more powerfull using oh-my-posh and plugins.","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Programmers use a command-line interface (CLI) to issue text-commands to the Operating System (OS), instead of clicking on a Graphical User Interface (GUI). This is because command-line inerface is much more powerful and flexible than the graphical user interface. The Terminal application is a command-line Interface (or shell). By default, the Terminal in Ubuntu and macOS runs the so-called bash shell, which supports a set of commands and utilities; and has its own programming language for writing shell scripts. ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:0","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Oh My Posh Oh My Posh is a custom prompt engine for any shell that has the ability to adjust the prompt string with a function or variable. Colors: Oh My Posh enables you to use the full color set of your terminal by using colors to define and render the prompt. Customizable: Easily adjust existing themes or create your own. From standard segments all the way to custom implementations. Portable: No matter which shell you’re using, or even how many, you can carry the configuration from one shell and/or machine to another for the same prompt everywhere you work. ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:1","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Installation Windows Need have powershell 7, download at Microsoft Store or install on official website. Install oh-my-posh from winget. winget install JanDeDobbeleer.OhMyPosh -s winget or Manual installation Set-ExecutionPolicy Bypass -Scope Process -Force; Invoke-Expression ((New-Object System.Net.WebClient).DownloadString('https://ohmyposh.dev/install.ps1')) This installs a couple of things: oh-my-posh.exe - Windows executable themes - The latest Oh My Posh themes Linux Recommend using zsh shell instead of bash. Install oh-my-posh by manual download. curl -s https://ohmyposh.dev/install.sh | bash -s or using Homebrew brew install jandedobbeleer/oh-my-posh/oh-my-posh Font Installation Install nerd fonts from official website. And set nerd fonts on terminal profile. ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:2","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Configuration Windows Create powershell user profile $PROFILE. echo $PROFILE We will be got an output C:\\Users\\user\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1. This is our $PROFILE location to be edited. Create new file on C:\\Users\\user\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1. For my device i like use 1_shell theme. # Prompt Init oh-my-posh init pwsh --config \"$env:POSH_THEMES_PATH/1_shell.omp.json\" | Invoke-Expression Linux Add this below line on .zshrc file to load oh-my-posh theme engine. Because i use WSL i can directly load from powershell at /mnt/c/Users/Maverick/AppData/Local/Programs/oh-my-posh/themes directory. If not have themes yet can download first from GitHub here. export TERM=x-term-256color export PATH=$HOME/bin:/usr/local/bin:/snap/bin:$PATH # Plugins eval \"$(oh-my-posh init zsh --config /mnt/c/Users/Maverick/AppData/Local/Programs/oh-my-posh/themes/1_shell.omp.json)\" Usually i don’t like use transient prompt so i need to remove this line. { ... \"transient_prompt\": { \"background\": \"transparent\", \"foreground\": \"#FEF5ED\", \"template\": \"\\ue285 \" }, ... } Then execute source .zshrc or just close and reopen terminal. Oh My Posh already installed if looks like ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:3","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Powerful Plugin Windows Install Terminal-Icons Module to show if directory or files Install-Module -Name Terminal-Icons -Repository PSGallery -Force Install z module, it can be used directly jump in some folder. Install-Module -Name z -Force Install PSReadLine to auto suggestion or auto completion from history command line. Install-Module -Name PSReadLine -AllowPrerelease -Scope CurrentUser -Force -SkipPublisherCheck Update powershell profile C:\\Users\\user\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1. I added function which like on Linux command line to find where the program directory. # Aliases Set-Alias ll ls Set-Alias j z # Prompt Init oh-my-posh init pwsh --config \"$env:POSH_THEMES_PATH/1_shell.omp.json\" | Invoke-Expression # Import Module \u0026 Configuration Import-Module Terminal-Icons Set-PSReadLineOption -PredictionSource History Set-PSReadLineOption -PredictionViewStyle ListView # Function function which($command) { Get-Command -Name $command -ErrorAction SilentlyContinue | Select-Object -ExpandProperty Path -ErrorAction SilentlyContinue } Linux Clone zsh-autosuggestions and zsh-syntax-highlightings mkdir .zsh git clone https://github.com/zsh-users/zsh-autosuggestions.git .zsh/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git .zsh/zsh-syntax-highlightings Install jump shell from snap, on my case I use Ubuntu-20.04. Jump integrates with your shell and learns about your navigational habits by keeping track of the directories you visit. It gives you the most visited directory for the shortest search term you type. sudo snap install jump Install ruby gem and colorls to beautify list directory. sudo apt install ruby-full sudo gem install colorls Finally, update .zshrc profile to load configuration. # Set up the prompt export TERM=xterm-256colorexport PATH=$HOME/bin:/usr/local/bin:/snap/bin:$PATH # Keep 1000 lines of history within the shell and save it to ~/.zsh_history: HISTSIZE=1000 SAVEHIST=1000 HISTFILE=~/.zsh_history # Aliases command line alias ls='colorls -l --sort-dirs' alias la='colorls -A --sort-dirs' alias ll='colorls -lA --sort-dirs' alias tree='colorls --tree --sort-dirs' alias gs='colorls --git-status --tree --sort-dirs' alias vim='nvim' # Plugins eval \"$(jump shell)\" eval \"$(oh-my-posh init zsh --config /mnt/c/Users/maverick/AppData/Local/Programs/oh-my-posh/themes/1_shell.omp.json)\" source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlightings/zsh-syntax-highlighting.zsh ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:4","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Reference OhMyPosh Docs ASMR Set Up PowerShell with Oh-My-Posh on Windows 11 + Neovim Setup + Terminal Icons - No Talking jump Color LS ","date":"2023-11-26","objectID":"/2023/11/make-terminal-powerfull-using-oh-my-posh/:0:5","tags":["Terminal","Powershell","Oh-My-Posh","Oh My Posh","Bash","Zsh","Shell"],"title":"Make Terminal Powerfull Using Oh-My-Posh","uri":"/2023/11/make-terminal-powerfull-using-oh-my-posh/"},{"categories":["Documentation"],"content":"Transmission Control Protocol (TCP) is a communications standard that enables application programs and computing devices to exchange messages over a network. It is designed to send packets across the internet and ensure the successful delivery of data and messages over networks. TCP is one of the basic standards that define the rules of the internet and is included within the standards defined by the Internet Engineering Task Force (IETF). It is one of the most commonly used protocols within digital network communications and ensures end-to-end data delivery. TCP organizes data so that it can be transmitted between a server and a client. It guarantees the integrity of the data being communicated over a network. Before it transmits data, TCP establishes a connection between a source and its destination, which it ensures remains live until communication begins. It then breaks large amounts of data into smaller packets, while ensuring data integrity is in place throughout the process. As a result, high-level protocols that need to transmit data all use TCP Protocol. Examples include peer-to-peer sharing methods like File Transfer Protocol (FTP), Secure Shell (SSH), and Telnet. It is also used to send and receive email through Internet Message Access Protocol (IMAP), Post Office Protocol (POP), and Simple Mail Transfer Protocol (SMTP), and for web access through the Hypertext Transfer Protocol (HTTP). ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:0","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"How TCP works? The server is in a “passive open” state: Passive open is a network communication setting where a server process is waiting to establish a connection with a client. It is “listening” for a connection without establishing it. The client must initiate an “active open:” Once the server is in a “passive open” state, the client must establish a connection by sending a TCP synchronization or TCP SYN message. The server then expends resources to accept the connection. A reliable connection is established through a three-way handshake: The three-way handshake is one of the central features of TCP. It makes sure that the connection is set up securely and reliably by ensuring it follows three steps: SYN: The client sends a synchronization message to the server, essentially a unique numerical value. SYN-ACK: The server sends back a synchronization acknowledgment (or SYN-ACK) message, which comprises two parts – the SYN value +1 and an ACK message, which is also a numerical value. The client receives the SYN-ACK. ACK: The client responds with an acknowledgment of its own, which is the ACK value + 1. This step in the three-way handshake establishes the client-to-server connection. Applications hosted on the client can now communicate with server-hosted applications by leveraging the connection. Timeout: Timeout is the maximum interval allowed to pass between the data origination and receipt. It ensures that the connection does not remain open too long and minimizes exposure to online threats and bad actors. Acknowledgment: The server and the client exchange ACK values for data transmission validation. If a data stream is not acknowledged, then the protocol tries retransmission. Also, if three consecutive ACK values are the same, then TCP initiates retransmission. These rules make TCP among the most reliable communication protocols available across local area networks (LAN) and wide area networks (WAN). However, it does have a few vulnerabilities. The three-way handshake (SYN, SYN-ACK, and ACK) process takes time, and there is a stipulated interval allotted for a timeout. As a result, TCP connections have a greater latency and may slow down data transmission when heavy packets need to be delivered. ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:1","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"Spring Integration IP Spring Integration provides channel adapters for receiving and sending messages over internet protocols. Both UDP (User Datagram Protocol) and TCP (Transmission Control Protocol) adapters are provided. Each adapter provides for one-way communication over the underlying protocol. In addition, Spring Integration provides simple inbound and outbound TCP gateways. These are used when two-way communication is needed. Two flavors each of UDP inbound and outbound channel adapters are provided: UnicastSendingMessageHandler sends a datagram packet to a single destination. UnicastReceivingChannelAdapter receives incoming datagram packets. MulticastSendingMessageHandler sends (broadcasts) datagram packets to a multicast address. MulticastReceivingChannelAdapter receives incoming datagram packets by joining to a multicast address. TCP inbound and outbound channel adapters are provided: TcpSendingMessageHandler sends messages over TCP. TcpReceivingChannelAdapter receives messages over TCP. Prerequisities JDK 17 or later Spring Boot 3.1.x or later ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:2","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"Implement the TCP Server Before we implement the client, we need a TCP server. TCP server can be created using any language that supports TCP, but we will create it with Spring Boot as well. The server will listen the port 10001 on localhost, and will send message to the client. Dependency \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-integration\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.integration\u003c/groupId\u003e \u003cartifactId\u003espring-integration-ip\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-core\u003c/artifactId\u003e \u003cversion\u003e2.14.2\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-databind\u003c/artifactId\u003e \u003cversion\u003e2.14.2\u003c/version\u003e \u003c/dependency\u003e Add the port listen tcp.server.port=10001 on application.properties Configuration Create Bean Configuration called TCPServerConfiguration @Configuration public class TCPServerConfiguration { @Value(\"${tcp.server.port}\") private int port; public static final String TCP_DEFAULT_CHANNEL = \"tcp-channel-sample\"; @Bean public AbstractServerConnectionFactory serverConnectionFactory() { TcpNioServerConnectionFactory tcpNioServerConnectionFactory = new TcpNioServerConnectionFactory(port); tcpNioServerConnectionFactory.setUsingDirectBuffers(true); return tcpNioServerConnectionFactory; } @Bean(name = TCP_DEFAULT_CHANNEL) public MessageChannel messageChannel() { return new DirectChannel(); } @Bean public TcpInboundGateway tcpInboundGateway(AbstractServerConnectionFactory serverConnectionFactory, @Qualifier(value = \"tcp-channel-sample\") MessageChannel messageChannel) { TcpInboundGateway gateway = new TcpInboundGateway(); gateway.setConnectionFactory(serverConnectionFactory); gateway.setRequestChannel(messageChannel); return gateway; } } Data Transfer Object (DTO) Add the DTO MessageDTO to send response to client. @Data @Builder @NoArgsConstructor @AllArgsConstructor public class MessageDTO implements Serializable { @Serial private static final long serialVersionUID = 2780348667790818215L; private String message; private String sender; private String timestamp; } Service A service class to process message from client and generate response which is will be send to client. @Slf4j @Service public class MessageService { public String process(byte[] message) throws IOException { String messageJson = new String(message); log.info(\"Receive message as JSON: {}\", messageJson); Jackson2JsonObjectMapper mapper = new Jackson2JsonObjectMapper(); MessageDTO messageDTO = mapper.fromJson(messageJson, MessageDTO.class); log.info(\"Message: {}, from: {}, at: {}\", messageDTO.getMessage(), messageDTO.getSender(), messageDTO.getTimestamp()); MessageDTO response = MessageDTO.builder() .message(\"Hello this message from TCP server!\") .timestamp(LocalDateTime.now().toString()) .sender(\"tcp-server\") .build(); return mapper.toJson(response); } } TCP Endpoint FInally, we need to create Message Endpoint which will be register to the input channel on ServiceActivator @Slf4j @MessageEndpoint public class TCPMessageEndpoint { private final MessageService messageService; public TCPMessageEndpoint(MessageService messageService) { this.messageService = messageService; } @ServiceActivator(inputChannel = TCPServerConfiguration.TCP_DEFAULT_CHANNEL) public byte[] process(byte[] message) throws IOException { String response = messageService.process(message); log.info(\"Send message to client\"); return response.getBytes(); } } ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:3","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"Implement the TCP Client After we have TCP server, we can create our TCP Client using Spring Boot that listen the port of TCP server. We will create web service REST to try the client. Dependency \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-integration\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.integration\u003c/groupId\u003e \u003cartifactId\u003espring-integration-ip\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-databind\u003c/artifactId\u003e \u003cversion\u003e2.14.2\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-core\u003c/artifactId\u003e \u003cversion\u003e2.14.2\u003c/version\u003e \u003c/dependency\u003e Properties Update the application.properties like below. spring.application.name=tcp-client-sample server.port=8080 server.servlet.context-path=/api server.error.include-message=always tcp.server.host=localhost tcp.server.port=10001 tcp.client.connection.poolSize=50 Configuration Create Bean Configuration called TCPClientConfiguration @Configuration public class TCPClientConfiguration { @Value(\"${tcp.server.host}\") private String host; @Value(\"${tcp.server.port}\") private int port; @Value(\"${tcp.client.connection.poolSize}\") private int connectionPoolSize; public static final String TCP_DEFAULT_CHANNEL = \"tcp-channel-sample\"; @Bean public AbstractClientConnectionFactory clientConnectionFactory() { TcpNioClientConnectionFactory tcpNioClientConnectionFactory = new TcpNioClientConnectionFactory(host, port); tcpNioClientConnectionFactory.setUsingDirectBuffers(true); return new CachingClientConnectionFactory(tcpNioClientConnectionFactory, connectionPoolSize); } @Bean(name = TCP_DEFAULT_CHANNEL) public MessageChannel messageChannel() { return new DirectChannel(); } @Bean @ServiceActivator(inputChannel = TCP_DEFAULT_CHANNEL) public MessageHandler messageHandler(AbstractClientConnectionFactory clientConnectionFactory) { TcpOutboundGateway tcpOutboundGateway = new TcpOutboundGateway(); tcpOutboundGateway.setConnectionFactory(clientConnectionFactory); return tcpOutboundGateway; } } Data Transfer Object (DTO) Add the DTO MessageDTO like DTO on our tcp server to send message to server. @Data @Builder @NoArgsConstructor @AllArgsConstructor public class MessageDTO implements Serializable { @Serial private static final long serialVersionUID = 497439089321548030L; private String message; private String sender; private String timestamp; } The Gateway Create a TCP outbound gateway as gateway spring integration ip to the server. @Component @MessagingGateway(defaultRequestChannel = TCPClientConfiguration.TCP_DEFAULT_CHANNEL) public interface TCPClientGateway { byte[] send(byte[] message); } Service A service class to send message to server and mapping response our from TCP server. @Slf4j @Service public class MessageService { private final TCPClientGateway clientGateway; public MessageService(TCPClientGateway clientGateway) { this.clientGateway = clientGateway; } @SneakyThrows public MessageDTO send(String message) { MessageDTO messageRequest = MessageDTO.builder() .message(message) .sender(\"tcp-client\") .timestamp(LocalDateTime.now().toString()) .build(); Jackson2JsonObjectMapper mapper = new Jackson2JsonObjectMapper(); String messageRequestStr = mapper.toJson(messageRequest); log.info(\"Sending message: {}\", messageRequestStr); byte[] responseByte = clientGateway.send(messageRequestStr.getBytes()); MessageDTO response = mapper.fromJson(new String(responseByte), MessageDTO.class); log.info(\"Receive message: {}, from: {}, at: {}\", response.getMessage(), response.getSender(), response.getTimestamp()); return response; } } Controller The controller as REST controller that we used to receive request ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:4","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"Conclusion We can create TCP Client using spring integration IP. Thank you for reading this article, you can find the source code in my github here ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:5","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"References What is Transmission Control Protocol TCP/IP? What Is TCP (Transmission Control Protocol)? Spring TCP and UDP Support Spring Boot TCP Client Sample Spring Boot TCP Server Sample ","date":"2023-06-05","objectID":"/2023/06/spring-boot-tcp-integration-ip/:0:6","tags":["Spring Boot","TCP Integration","Spring Boot TCP","Spring Integration IP"],"title":"Spring Boot TCP Integration IP","uri":"/2023/06/spring-boot-tcp-integration-ip/"},{"categories":["Documentation"],"content":"Redis is a very popular, open-source, and very fast in-memory key-value data store. This is an example implementation high availability with Redis Sentinel and Spring Lettuce Client","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Redis is an open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. Redis Sentinel is high availability solution of redis. It provides monitoring, automatic failover, and configuration provider. Redis sentinel will monitor and automatically detect a master failure and bring all cluster to a stable mode. In this article we can run redis sentinel with 1 master and 2 replicas. And we will use 3 sentinel instance, 1 master and 2 slave, and the usual quorum value is 2. ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:0","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Lettuce Client To connect sentinel instances we need to use a client that also supports Sentinel itself. Lettuce is a scalable thread-safe Redis client for synchronous, asynchronous and reactive usage. Multiple threads may share one connection if they avoid blocking and transactional operations such as BLPOP and MULTI/EXEC. Lettuce is built with netty. Supports advanced Redis features such as Sentinel, Cluster, Pipelining, Auto-Reconnect and Redis data models. Lettuce client also can use connection pooling to a Redis node. Before implement redis client using Spring Lettuce Client, please run redis sentinel cluster properly. See my How to Run Redis Sentinel article. ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:1","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Configuration Add Spring Data Redis, Spring Web and Lombok for utility to our pom.xml \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e And create properties file using .yaml file like following application.yaml server: port: 8080 spring: application: name: redis-sentinel-demo cache: type: redis redis: time-to-live: 600 redis: database: 0 sentinel: master: mymaster nodes: - localhost:26379 - localhost:26380 - localhost:26381 lettuce: shutdown-timeout: 200ms Create a connection configuration RedisConfig.java using LettuceConnectionFactory @Configuration public class RedisConfig { private final RedisProperties redisProperties; @Autowired public RedisConfig(RedisProperties redisProperties) { this.redisProperties = redisProperties; } @Bean public CacheManager cacheManager(LettuceConnectionFactory lettuceConnectionFactory) { RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .disableCachingNullValues(); return RedisCacheManager.builder(lettuceConnectionFactory) .cacheDefaults(redisCacheConfiguration) .build(); } @Bean public LettuceConnectionFactory lettuceConnectionFactory() { final RedisSentinelConfiguration sentinelConfiguration = new RedisSentinelConfiguration() .master(redisProperties.getSentinel().getMaster()); final LettuceClientConfiguration lettuceClientConfiguration = LettuceClientConfiguration.builder() .readFrom(ReadFrom.ANY_REPLICA) .build(); redisProperties.getSentinel().getNodes().forEach(node -\u003e { final String[] sentinelNodes = node.split(\":\"); final String host = sentinelNodes[0]; final int port = Integer.parseInt(sentinelNodes[1]); sentinelConfiguration.sentinel(host, port); }); LettuceConnectionFactory connectionFactory = new LettuceConnectionFactory(sentinelConfiguration, lettuceClientConfiguration); connectionFactory.setDatabase(redisProperties.getDatabase()); return connectionFactory; } @Bean public RedisTemplate\u003cString, Object\u003e redisTemplate() { final RedisTemplate\u003cString, Object\u003e redisTemplate = new RedisTemplate\u003c\u003e(); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new JdkSerializationRedisSerializer()); redisTemplate.setConnectionFactory(lettuceConnectionFactory()); return redisTemplate; } } ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:2","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Redis Cache Helper To make it easier to use the RedisTemplate\u003cK, V\u003e we will create an interface helper class, namely RedisCacheHelper and define method for store cache, get cache, and remove cache. public interface RedisCacheHelper { void put(String cacheName, Object key, Object value, long ttl); Object get(String cacheName, Object key); void remove(String cacheName, Object key); } And we will create implementatins for HashOperations\u003cK, HK, HV\u003e and ValueOperations\u003cK, V\u003e for example usage. Hash Operations @Slf4j @Component(value = RedisCacheHelperHashOperations.BEAN_NAME) public class RedisCacheHelperHashOperations implements RedisCacheHelper { public static final String BEAN_NAME = \"redisCacheHelperHashOperations\"; @Autowired public RedisTemplate\u003cString, Object\u003e redisTemplate; @Override public void put(String cacheName, Object key, Object value, long ttl) { redisTemplate.opsForHash().put(cacheName, key, value); } @Override public Object get(String cacheName, Object key) { return redisTemplate.opsForHash().get(cacheName, key); } @Override public void remove(String cacheName, Object key) { redisTemplate.opsForHash().getOperations().delete(cacheName + key); } } Value Operations @Slf4j @Component public class RedisCacheHelperValueOperations implements RedisCacheHelper { public static final String BEAN_NAME = \"redisCacheHelperValueOperations\"; private static final String SEPARATOR_CACHE = \":\"; @Autowired private RedisTemplate\u003cString, Object\u003e redisTemplate; @Override public void put(String cacheName, Object key, Object value, long ttl) { redisTemplate.opsForValue().set(constructKey(cacheName, key), value, ttl, TimeUnit.SECONDS); } @Override public Object get(String cacheName, Object key) { return redisTemplate.opsForValue().get(constructKey(cacheName, key)); } @Override public void remove(String cacheName, Object key) { redisTemplate.opsForValue().getOperations().delete(constructKey(cacheName, key)); } private String constructKey(String cacheName, Object key) { return cacheName + SEPARATOR_CACHE + key; } } ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:3","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Service and Logic Now we create a POJO class for data transfer object. @Data @Builder @NoArgsConstructor @AllArgsConstructor public class DataCacheDto implements Serializable { private static final long serialVersionUID = -7799415185617798522L; private String name; private String message; } And create service logic to use RedisCacheHelper for redis operations. Our service should look like following. @Slf4j @Service public class DemoService { @Autowired @Qualifier(RedisCacheHelperHashOperations.BEAN_NAME) private RedisCacheHelper cacheHelperHash; @Autowired @Qualifier(RedisCacheHelperValueOperations.BEAN_NAME) private RedisCacheHelper cacheHelperValue; @Value(\"${spring.cache.redis.time-to-live:600}\") private Integer defaultTtl; private final static String DEMO = \"DEMO\"; private String generateKey() { return UUID.randomUUID().toString().replace(\"-\", \"\"); } public Map\u003cString, Object\u003e putHash(DataCacheDto request) { String key = this.generateKey(); try { log.info(\"Put data [{}] to cache by hash operations\", request); cacheHelperHash.put(DEMO, key, request, defaultTtl); } catch (Exception e) { log.error(\"Got exception when put cache by hash operations\", e); throw e; } return Map.of(\"key\", key); } public DataCacheDto getFromHash(String key) { DataCacheDto cache; try { log.info(\"Get data from cache by hash operation. Key :: [{}]\", key); cache = (DataCacheDto) cacheHelperHash.get(DEMO, key); log.info(\"Data cache :: {}\", cache); } catch (Exception e) { log.error(\"Got exception when get cache by hash operations\", e); throw e; } return cache; } public Map\u003cString, Object\u003e putValue(DataCacheDto request) { String key = this.generateKey(); try { log.info(\"Put data [{}] to cache by value operations\", request); cacheHelperValue.put(DEMO, key, request, defaultTtl); } catch (Exception e) { log.error(\"Got exception when put cache by value operations\", e); throw e; } return Map.of(\"key\", key); } public DataCacheDto getFromValue(String key) { DataCacheDto cache; try { log.info(\"Get data from cache by value operation. Key :: [{}]\", key); cache = (DataCacheDto) cacheHelperValue.get(DEMO, key); log.info(\"Data cache :: {}\", cache); } catch (Exception e) { log.error(\"Got exception when get cache by value operations\", e); throw e; } return cache; } } The @Qualifier annotation indicates which service is used for the operation. Finally, we create a controller to test via web service. @Slf4j @RestController @RequestMapping(value = \"\") public class DemoController { @Autowired private DemoService service; @PostMapping(value = \"/hash-operations\") public ResponseEntity\u003cMap\u003cString, Object\u003e\u003e storeCacheHash(@RequestBody DataCacheDto request) { return ResponseEntity.ok(service.putHash(request)); } @GetMapping(value = \"/hash-operations/{key}\") public ResponseEntity\u003cDataCacheDto\u003e getCacheHash(@PathVariable String key) { return ResponseEntity.ok(service.getFromHash(key)); } @PostMapping(value = \"/value-operations\") public ResponseEntity\u003cMap\u003cString, Object\u003e\u003e storeCacheValue(@RequestBody DataCacheDto request) { return ResponseEntity.ok(service.putValue(request)); } @GetMapping(value = \"/value-operations/{key}\") public ResponseEntity\u003cDataCacheDto\u003e getCacheValue(@PathVariable String key) { return ResponseEntity.ok(service.getFromValue(key)); } } ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:4","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Project structure The project structure should be look like the following content. src ├── main │ └── java │ │ └── com.piinalpin.redissentinel │ │ ├── component │ │ │ ├──impl │ │ │ │ ├── RedisCacheHelperHashOperations.java │ │ │ │ └── RedisCacheHelperValueOperations.java │ │ │ └── RedisCacheHelper.java │ │ ├── config │ │ │ └── RedisConfig.java │ │ ├── controller │ │ │ └── DemoController.java │ │ ├── dto │ │ │ └── DataCacheDto.java │ │ ├── service │ │ │ └── DemoService.java │ │ └── Application.java │ └── resources │ │ └── application.yaml └── test └── java └── com.piinalpin.redissentinel ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:5","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Run and Test Application Run spring boot application and test with postman or with cUrl. I will provide cUrl command at the following. Store by Hash Operation curl -X POST \\ 'http://localhost:8080/hash-operations' \\ --header 'Accept: */*' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"name\": \"maverick\", \"message\": \"Hello World!\" }' Get by Hash Operation curl -X GET \\ 'http://localhost:8080/hash-operations/71783f6348654baf9c844e4363d8e5f7' \\ --header 'Accept: */*'' Store by Value Operation curl -X POST \\ 'http://localhost:8080/value-operations' \\ --header 'Accept: */*' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"name\": \"maverick\", \"message\": \"Hello World!\" }' Get by Value Operation curl -X GET \\ 'http://localhost:8080/value-operations/71783f6348654baf9c844e4363d8e5f7' \\ --header 'Accept: */*'' ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:6","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Run as Container Before we run from container, create Dockerfile, docker-compose.yaml and application-docker.yaml file to used for application container configuration. Dockerfile The Dockerfile is used for build docker images. # Import base JDK from Linux FROM adoptopenjdk/openjdk11:alpine # Set work directory WORKDIR /app # Copy application files COPY application-docker.yaml application.yaml COPY target/*.jar app.jar # Expose PORT EXPOSE 8080 # Run application ENTRYPOINT [\"java\", \"-jar\", \"app.jar\", \"--spring.config.location=application.yaml\"] Docker Compose File The docker-compose.yaml file is used for defining multiple container. And we will run 4 container such as : Redis Master with Sentinel Mode Redis Replica 1 with Sentinel Mode Redis Replica 2 with Sentinel Mode Spring Boot Application version: \"3.8\" services: redis-master: hostname: redis-master.mylab.local container_name: redis-master image: redis:7.0.5 command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master.mylab.local 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru\" volumes: - \"redis-bitnami-data:/bitnami/redis/data\" - \"redis-data:/data\" redis-slave-1: hostname: redis-slave-1.mylab.local container_name: redis-slave-1 image: redis:7.0.5 command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master.mylab.local 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --port 6380 --slaveof redis-master.mylab.local 6379 --maxmemory 256mb --maxmemory-policy allkeys-lru\" volumes: - \"redis-bitnami-data:/bitnami/redis/data\" - \"redis-data:/data\" redis-slave-2: hostname: redis-slave-2.mylab.local container_name: redis-slave-2 image: redis:7.0.5 command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master.mylab.local 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --port 6381 --slaveof redis-master.mylab.local 6379 --maxmemory 256mb --maxmemory-policy allkeys-lru\" volumes: - \"redis-bitnami-data:/bitnami/redis/data\" - \"redis-data:/data\" application: hostname: redis-sentinel-demo container_name: redis-sentinel-demo-app image: piinalpin/redis-sentinel-demo:0.0.1-SNAPSHOT build: context: . ports: - \"8080:8080\" networks: default: driver: bridge volumes: redis-data: redis-bitnami-data: Application Properties Configuration The properties configuration file must setup sentinel node to internal network docker container. That why, the docker-compose.yaml defined hostname and the application will read sentinel nodes by hostname. server: port: 8080 spring: application: name: redis-sentinel-demo cache: type: redis redis: time-to-live: 600 redis: database: 0 sentinel: master: mymaster nodes: - redis-master.mylab.local:26379 - redis-slave-1.mylab.local:26379 - redis-slave-2.mylab.local:26379 lettuce: shutdown-timeout: 200ms Build the Application Before we run the container, we must build package ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:7","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Conclusion Thank you for reading this article. I explained this article by my experience for reasearch and documentation of High Availability with Redis Sentinel and Spring Lettuce Client. And also, maybe can be used high availability with redis sentinel solutions. You can find my source code here. ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:8","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Reference High Availability with Redis Sentinel and Spring Lettuce Client Redis — Using master node to save and replicas nodes to read with Springboot. How to use cacheDefaults method Spring Boot Caching with Redis Sentinel always connects to master node Class ReadFrom - Lettuce Docs Lettuce - Advanced Java Redis client ","date":"2022-10-26","objectID":"/2022/10/redis-sentinel-spring-lettuce-client/:0:9","tags":["Redis","Redis Sentinel","Redis Client","Spring Boot","Spring Lettuce Client"],"title":"High Availability with Redis Sentinel and Spring Lettuce Client","uri":"/2022/10/redis-sentinel-spring-lettuce-client/"},{"categories":["Documentation"],"content":"Redis Sentinel is the high-availability solution for open-source Redis server. It provides monitoring of all Redis nodes and automatic failover should the master node become unavailable. This guide provides a sample configuration for a three-node Redis cluster.","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Redis Sentinel is the high-availability solution for open-source Redis server. It provides monitoring of all Redis nodes and automatic failover should the master node become unavailable. This guide provides a sample configuration for a three-node Redis cluster. Sentinel itself is designed to run in a configuration where there are multiple Sentinel processes cooperating together. The advantage of having multiple Sentinel processes cooperating are the following : Failure detection is performed when multiple Sentinels agree about the fact a given master is no longer available. This lowers the probability of false positives. Sentinel works even if not all the Sentinel processes are working, making the system robust against failures. There is no fun in having a failover system which is itself a single point of failure, after all. The sum of Sentinels, Redis instances (masters and replicas) and clients connecting to Sentinel and Redis, are also a larger distributed system with specific properties. In this document concepts will be introduced gradually starting from basic information needed in order to understand the basic properties of Sentinel, to more complex information (that are optional) in order to understand how exactly Sentinel works. See the official documentation here ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:0","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Redis Sentinel Replication Topology Redis sentinel simple topology, I will explained with 1 master and 2 slaves. Assume we have 3 instance of Virtual Machine and each instances running redis-server and sentinel mode. The slaves is replica of redis master. ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:1","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Install Redis You can download Redis latest here or you can use brew to install redis. brew install redis ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:2","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Run Redis Sentinel First of all, we have to run redis server properly and we should create configuration file for each redis server. Redis Master Create configuration file redis-master.conf port 6379 daemonize yes dbfilename dump_6379.rdb The configuration file mean running on port 6379, runing on background process as daemonized and set db file name. Then run redis-server with command : redis-server redis-master.conf Redis Slave Create configuration file redis-slave-1.conf and redis-slave-2.conf redis-slave-1.conf port 6380 daemonize yes dbfilename dump_6380.rdb replicaof localhost 6379 replica-read-only yes redis-slave-2.conf port 6381 daemonize yes dbfilename dump_6380.rdb replicaof localhost 6379 replica-read-only yes Then run redis-server with command : redis-server redis-slave-1.conf redis-server redis-slave-2.conf We will have 2 redis server as replica and running on port 6380 and 6381. Redis Sentinel Mode Run redis on sentinel mode, and we will create 3 node of sentinels. First of all we will create configuration files sentinel-1.conf, sentinel-2.conf, and sentinel-3.conf to monitoring redis-master. sentinel-1.conf protected-mode no port 26379 daemonize yes sentinel monitor mymaster ::1 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 10000 sentinel resolve-hostnames yes sentinel-2.conf protected-mode no port 26380 daemonize yes sentinel monitor mymaster ::1 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 10000 sentinel resolve-hostnames yes sentinel-3.conf protected-mode no port 26381 daemonize yes sentinel monitor mymaster ::1 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 10000 sentinel resolve-hostnames yes And then run redis server as sentinel mode redis-server sentinel-1.conf --sentinel redis-server sentinel-2.conf --sentinel redis-server sentinel-3.conf --sentinel Redis sentinel nodes will running on port 26379, 26380, and 26381. ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:3","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Redis Sentinel Command To run sentinel command, we use redis-cli with port of sentinel redis-cli -h $SENTINEL_HOST -p $SENTINEL_PORT Basic usage redis sentinel command is sentinel get-master-addr-by-name \u003cmaster_name\u003e to get master address sentinel ckquorum \u003cmaster_name\u003e to get info quorum For more details, see the official documentation here. Redis master info replication role:master connected_slaves:2 slave0:ip=::1,port=6380,state=online,offset=76442,lag=0 slave1:ip=::1,port=6381,state=online,offset=76200,lag=1 master_failover_state:no-failover master_replid:2fe52acf2b457b936070ae2f86204efc3752f0cf master_replid2:4252dff2f509b85fcd78b02c763d99a371ebe652 master_repl_offset:76442 second_repl_offset:23098 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:23098 repl_backlog_histlen:53345 ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:4","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Stop Redis Sentinel To stopping the redis sentinel, use the redis-cli command to shutdown the server. Stop Redis Sentinel redis-cli -p 26381 shutdown redis-cli -p 26380 shutdown redis-cli -p 26379 shutdown Stop Redis Server redis-cli -p 6381 shutdown redis-cli -p 6380 shutdown redis-cli -p 6379 shutdown ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:5","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Run with Docker This section I will explain how to run redis sentinel using docker-compose. Create docker-compose.yml configuration. version: \"3.8\" services: redis-master: hostname: redis-master container_name: redis-master image: redis:latest command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru\" ports: - 6379:6379 - 26379:26379 networks: default: ipv4_address: 172.68.0.2 redis-slave-1: hostname: redis-slave-1 container_name: redis-slave-1 image: redis:latest command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --port 6380 --slaveof redis-master 6379 --maxmemory 256mb --maxmemory-policy allkeys-lru\" ports: - 6380:6380 - 26380:26379 networks: default: ipv4_address: 172.68.0.3 redis-slave-2: hostname: redis-slave-2 container_name: redis-slave-2 image: redis:latest command: \u003e bash -c \"echo 'port 26379' \u003e sentinel.conf \u0026\u0026 echo 'dir /tmp' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel resolve-hostnames yes' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel monitor mymaster redis-master 6379 2' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel down-after-milliseconds mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel parallel-syncs mymaster 1' \u003e\u003e sentinel.conf \u0026\u0026 echo 'sentinel failover-timeout mymaster 5000' \u003e\u003e sentinel.conf \u0026\u0026 cat sentinel.conf \u0026\u0026 redis-server sentinel.conf --sentinel \u0026 redis-server --port 6381 --slaveof redis-master 6379 --maxmemory 256mb --maxmemory-policy allkeys-lru\" ports: - 6381:6381 - 26381:26379 networks: default: ipv4_address: 172.68.0.4 networks: default: driver: bridge ipam: config: - subnet: 172.68.0.0/16 gateway: 172.68.0.1 When we run the docker compose It will create a local network with subnet 172.68.0.0/16 and gateway 172.68.0.1. redis-master assign to IP 172.68.0.2 redis-slave-1 assign to IP 172.68.0.3 redis-slave-2 assign to IP 172.68.0.4 Each container will publish port for redis server and sentinel redis-slave-x will be slave of redis-master using docker network Before run redis sentinel container, we should configure ip routing on local machine (host device) from docker ip to localhost. sudo ifconfig lo0 alias 172.68.0.2 sudo ifconfig lo0 alias 172.68.0.3 sudo ifconfig lo0 alias 172.68.0.4 Check if the docker container ip have an alias on local machine with the command ifconfig lo0. And we will get an output. lo0: flags=8049\u003cUP,LOOPBACK,RUNNING,MULTICAST\u003e mtu 16384 options=1203\u003cRXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP\u003e inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 inet 172.68.0.2 netmask 0xffff0000 inet 172.68.0.3 netmask 0xffff0000 inet 172.68.0.4 netmask 0xffff0000 nd6 options=201\u003cPERFORMNUD,DAD\u003e The inet 172.68.0.2 netmask 0xffff0000 indicates that the routing is running properly. Finally, we can run redis sentinel with docker using following commans : docker compose up -d It will run 3 container such as redis-master, redis-slave-1 and redis-slave-2. ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:6","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Reference High availability with Redis Sentinel Simple Redis sentinel setup Springboot App with Redis Sentinel, Running in Docker container ","date":"2022-10-24","objectID":"/2022/10/how-to-run-redis-sentinel/:0:7","tags":["Redis","Redis Sentinel","Docker"],"title":"How to Run Redis Sentinel","uri":"/2022/10/how-to-run-redis-sentinel/"},{"categories":["Documentation"],"content":"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":" Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:0","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"How Does Machine Learning Works? Similar to how the human brain gains knowledge and understanding, machine learning relies on input, such as training data or knowledge graphs, to understand entities, domains and the connections between them. With entities defined, deep learning can begin. Data is The Key : The algorithms that drive machine learning are critical to success. ML algorithms build a mathematical model based on sample data, known as “training data,” to make predictions or decisions without being explicitly programmed to do so. This can reveal trends within data that information businesses can use to improve decision making, optimize efficiency and capture actionable data at scale. AI is the Goal: Machine Learning provides the foundation for AI systems that automate processes and solve data-based business problems autonomously. It enables companies to replace or augment certain human capabilities. Common machine learning applications you may find in the real world include chatbots, self-driving cars and speech recognition. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:1","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Machine Learning Method Machine learning classifiers fall into three primary categories, such as: Supervised learning, also known as supervised machine learning, is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Supervised learning helps organizations solve for a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, naïve bayes, linear regression, logistic regression, random forest, support vector machine (SVM), and more. Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, image and pattern recognition. It’s also used to reduce the number of features in a model through the process of dimensionality reduction; principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering, probabilistic clustering methods, and more. Semi-supervised learning offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of having not enough labeled data (or not being able to afford to label enough data) to train a supervised learning algorithm. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:2","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Linear Regression Linear regression is a basic and commonly used type of predictive analysis. The overall idea of regression is to examine two things: Does a set of predictor variables do a good job in predicting an outcome (dependent) variable? Which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable? These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. The simplest form of the regression equation with one dependent and one independent variable is defined by the formula : Where: y : estimated dependent variable score c : constant b : regression coefficient x : score on the independent variable There are many names for a regression’s dependent variable. It may be called an outcome variable, criterion variable, endogenous variable, or regressand. The independent variables can be called exogenous variables, predictor variables, or regressors. Three major uses for regression analysis are determining the strength of predictors, forecasting an effect, and trend forecasting. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:3","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Salary Prediction Model First of all, we should provides the dataset. Dataset can be a excel file, csv file or etc. You can use my example dataset here. Import pandas library for building the data frames. import pandas as pd Then load the dataset, like below dataset = pd.read_excel('salary_dataset.xlsx') dataset knowledge technical logical year_experience salary 0 50 60 50 0 Rp 2,500,000.00 1 60 50 50 0 Rp 2,500,000.00 2 50 70 70 0 Rp 3,000,000.00 3 40 50 60 0 Rp 2,800,000.00 4 70 70 70 1.1 Rp 4,000,000.00 5 75 70 65 1.2 Rp 4,000,000.00 6 65 65 60 1.1 Rp 3,800,000.00 7 70 70 70 1.5 Rp 4,500,000.00 8 65 NaN 70 1 Rp 3,400,000.00 9 70 80 80 2 Rp 6,000,000.00 10 75 75 85 1.8 Rp 6,000,000.00 11 80 80 80 2 Rp 7,000,000.00 12 80 80 80 2.2 Rp 7,500,000.00 13 75 70 80 2.9 Rp 7,800,000.00 14 80 85 80 3 Rp 8,400,000.00 15 75 80 75 2.4 Rp 7,500,000.00 16 85 80 90 3.2 Rp 8,200,000.00 17 85 80 85 3.2 Rp 8,000,000.00 18 85 90 90 2.7 Rp 8,000,000.00 19 90 90 90 3.7 Rp 10,000,000.00 20 NaN NaN NaN 3 Rp 8,000,000.00 Cleaning Dataset Clean null or NaN values from data frame using dropna(). dataset = dataset.dropna() dataset.head() knowledge technical logical year_experience salary 0 50 60 50 0.0 Rp 2,500,000.00 1 60 50 50 0.0 Rp 2,500,000.00 2 50 70 70 0.0 Rp 3,000,000.00 3 40 50 60 0.0 Rp 2,800,000.00 4 70 70 70 1.1 Rp 4,000,000.00 Building Model Import train_test_split from scikit learn to split arrays or matrices into random train and test subsets. Quick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner. from sklearn.model_selection import train_test_split x = dataset.drop('salary', axis=1) y = dataset['salary'] x.head() knowledge technical logical year_experience 0 50 60 50 0.0 1 60 50 50 0.0 2 50 70 70 0.0 3 40 50 60 0.0 4 70 70 70 1.1 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) Prediction Import LinearRegression from scikit learn and use the linear regression function. And create object from LinearRegression. from sklearn.linear_model import LinearRegression linear = LinearRegression() linear.fit(x_train, y_train) Predict test data x_test with call function predict() and store to variable y_pred. The result is prediction salary with test data using LinearRegression. y_pred = linear.predict(x_test) y_pred array([2122535.96880463, 3980638.07697809, 6537626.01871658, 1078550.87649938]) Accuracy Machine learning model accuracy is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset based on the input, or training data. The better a model can generalize to ‘unseen’ data, the better predictions and insights it can produce, which in turn deliver more business value. linear.score(x_test, y_test) 0.8148593096952005 Our linear regression model accuracy score is 81.4% Implementation Implementation a Linear Regression with some input from user that have value of knowledge, techincal, logical and year of experience. Assumes, you are fresh graduate with have a knowledge score is 50, technical score is 50 and logical score is 60. In this case we will use a dictionary data and convert it into DataFrame like below. data_dict = { 'knowledge': 50, 'technical': 50, 'logical': 60, 'year_experience': 0 } input_data = pd.DataFrame([data_dict]) input_data knowledge technical logical year_experience 0 50 50 60 0.0 And predict using LinearRegression function like below. predicted_salary = linear.predict(input_data)[0] # Convert decimal to integer predicted_salary = int(predicted_salary) print(\"IDR {:,.2f}\".format(predicted_salary)) IDR 1,864,514.00 The result of the prediction of the case is IDR 1,864,514.00. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:4","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Conclusion Simple Linear Regression help us to predict a dependent variable for salary prediction model. It can estimated of a response variable for people with values of the carier variable within the knowledges. You can download my jupyter notebook Predict Salary - Linear Regression.ipynb. ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:5","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Reference Machine Learning What Is Machine Learning? A Definition. What is Linear Regression? ","date":"2022-05-21","objectID":"/2022/05/salary-prediction-with-machine-learning/:0:6","tags":["Python","Machine Learning","Linear Regression","Scikit Learn"],"title":"Salary Prediction with ML - Linear Regression","uri":"/2022/05/salary-prediction-with-machine-learning/"},{"categories":["Documentation"],"content":"Spring Cloud Gateway provides a library for building API gateways on top of Spring and Java. It provides a flexible way of routing requests based on a number of criteria, as well as focuses on cross-cutting concerns such as security, resiliency, and monitoring.","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":" An API gateway is an interface between clients and backend microservices. When a gateway is used, it becomes the single point of contact for clients; it receives their API calls and routes each one to the appropriate backend. It facilitates microservice architectures. When an API gateway is used, clients do not know (nor should they know) the structure of the backend. Modern architectures discourage the use of large monolithic services; rather, numerous small microservices are preferred. This approach provides some compelling advantages, but it does introduce significant complexity. An API gateway mitigates this for the client. ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:0","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"What is Spring Cloud Gateway? Spring Cloud Gateway provides a library for building API gateways on top of Spring and Java. It provides a flexible way of routing requests based on a number of criteria, as well as focuses on cross-cutting concerns such as security, resiliency, and monitoring. Spring Cloud allows developers to implement things such as distributed configuration, service registration, load balancing, the circuit breaker pattern, and more. It provides these tools to implement many common patterns in distributed systems. Spring cloud gateway is working by clients make requests to Spring Cloud Gateway. If the Gateway Handler Mapping determines that a request matches a Route, it is sent to the Gateway Web Handler. This handler runs sends the request through a filter chain that is specific to the request. The reason the filters are divided by the dotted line, is that filters may execute logic before the proxy request is sent or after. All “pre” filter logic is executed, then the proxy request is made. After the proxy request is made, the “post” filter logic is executed. ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:1","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Project Setup and Dependency I’m depending Spring Initializr for this as it is much easier. We need import spring bloot cloud gateway dependencies, such as: spring-cloud-starter-gateway spring-boot-starter-actuator Here the pom.xml like following below. \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-gateway\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-dependencies\u003c/artifactId\u003e \u003cversion\u003e${spring-cloud.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e Change configuration application.properties file to setup spring cloud gateway and actuator like following below. server.port=8080 spring.application.name=api-gateway spring.main.web-application-type=reactive management.endpoints.web.exposure.include=* management.endpoints.web.cors.allowed-origins=true ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:2","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Gateway Filter Factories Route filters allow the modification of the incoming HTTP request or outgoing HTTP response in some manner. Route filters are scoped to a particular route. Spring Cloud Gateway includes many built-in GatewayFilter Factories. In this step we will rewrite path from spring cloud gateway to another service, we will use my Spring Boot Hello World project as a microservice. Clone and run the project by mvn spring-boot:run command. Or you can run the Spring Boot Hello World Container and run with port 8081. Add gateway filter factory with creating new file application.yml inside main resources. spring: cloud: gateway: routes: - id: springboot-helloworld uri: http://localhost:8081 predicates: - Path=/springboot-helloworld/** filters: - RewritePath=/springboot-helloworld/(?\u003csegment\u003e/?.*),/api/$\\{segment} Run spring boot cloud gateway http://localhost:8080/springboot-helloworld/. The result should be has response from springboot-helloworld service. For more documentation of GatewayFilterFactories please visit Spring Cloud Gateway. ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:3","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Resilience4j: Circuit Breaker Implementation What is circuit breaker? The concept of a circuit breaker is to prevent calls to microservice when it’s known the call may fail or time out. This is done so that clients don’t waste their valuable resources handling requests that are likely to fail. Using this concept, you can give the server some spare time to recover. So, how do we know if a request is likely to fail? Yeah, this can be known by recording the results of several previous requests sent to other microservices. For example, 4 out of 5 requests sent failed or timeout, then most likely the next request will also encounter the same thing. In the circuit breaker, there are 3 states Closed, Open, and Half-Open. Closed: when everything is normal. Initially, the circuit breaker is in a Closed state. Open: when a failure occurs above predetermined criteria. In this state, requests to other microservices will not be executed and fail-fast or fallback will be performed if available. When this state has passed a certain time limit, it will automatically or according to certain criteria will be returned to the Half-Open state. Half-Open: several requests will be executed to find out whether the microservices that we are calling are working normally. If successful, the state will be returned to the Closed state. However, if it still fails it will be returned to the Open state. Prerequisites One of the libraries that offer a circuit breaker features is Resilience4J for reactive web flux. We will adding the dependency in pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-hystrix\u003c/artifactId\u003e \u003cversion\u003e2.2.10.RELEASE\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-circuitbreaker-reactor-resilience4j\u003c/artifactId\u003e \u003c/dependency\u003e Fallback Command In the main Application add annotation @EnableHystrix. And then create FallbackController and ApiResponse to handle when service backend is unavailable. @RestController public class FallbackController { @GetMapping(value = \"/fallback\") public ResponseEntity\u003cObject\u003e fallback() { Map\u003cString, Object\u003e response = new HashMap\u003c\u003e(); response.put(\"timestamp\", LocalDateTime.now()); response.put(\"message\", \"Gateway Timeout!\"); return new ResponseEntity\u003c\u003e( response, HttpStatus.GATEWAY_TIMEOUT ); } } So, that mean when service backend is unavailable will be redirected to /fallback as soon as possible. And add CircuitBreaker to application.yml like following below. spring: cloud: gateway: routes: - id: springboot-helloworld uri: http://localhost:8081 predicates: - Path=/springboot-helloworld/** filters: - RewritePath=/springboot-helloworld/(?\u003csegment\u003e/?.*),/api/$\\{segment} - name: CircuitBreaker args: name: fallbackcmd fallbackUri: forward:/fallback Let’s try to shutdown the springboot-helloworld service, then go to http://localhost:8080/springboot-helloworld/ ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:4","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Logging with Global Filter Global filters are executed for every route defined in the API Gateway. The main difference between pre-filter and post-filter class is that the pre-filter code is executed before Spring Cloud API Gateway routes the request to a destination web service endpoint. While the post-filter code will be executed after Spring Cloud API Gateway has routed the HTTP request to a destination web service endpoint. PreGlobalFilter Execute pre filter to call RequestBodyRewriter then can be used for logging. @Configuration public class PreGlobalFilter implements GlobalFilter, Ordered { @Autowired private ModifyRequestBodyGatewayFilterFactory filterFactory; public static final String ORIGINAL_REQUEST_BODY = \"originalRequestBody\"; @Override public int getOrder() { return NettyWriteResponseFilter.WRITE_RESPONSE_FILTER_ORDER; } @Override public Mono\u003cVoid\u003e filter(ServerWebExchange exchange, GatewayFilterChain chain) { return filterFactory .apply(new ModifyRequestBodyGatewayFilterFactory.Config() .setRewriteFunction(String.class, String.class, (newExchange, body) -\u003e { String originalBody = null; if (body != null) { originalBody = body; } exchange.getAttributes().put(ORIGINAL_REQUEST_BODY, originalBody); return Mono.just(originalBody); })) .filter(exchange, chain); } } PostGlobalFilter Execute post filter to call ResponseBodyRewriter and construct the log message and then can be used for logging. @Slf4j @Configuration public class PostGlobalFilter implements GlobalFilter, Ordered { @Autowired private ModifyResponseBodyGatewayFilterFactory filterFactory; public static final String ORIGINAL_RESPONSE_BODY = \"originalResponseBody\"; @Override public int getOrder() { return NettyWriteResponseFilter.WRITE_RESPONSE_FILTER_ORDER + 1; } @Override public Mono\u003cVoid\u003e filter(ServerWebExchange exchange, GatewayFilterChain chain) { GatewayFilter delegate = filterFactory .apply(new ModifyResponseBodyGatewayFilterFactory.Config() .setRewriteFunction(byte[].class, byte[].class, (newExchange, body) -\u003e { String originalBody = null; if (body != null) { originalBody = new String(body); } exchange.getAttributes().put(ORIGINAL_RESPONSE_BODY, originalBody); return Mono.just(originalBody.getBytes()); })); return delegate .filter(exchange, chain) .then(Mono.fromRunnable(() -\u003e { this.writeLog(exchange); })); } private void writeLog(ServerWebExchange exchange) { ServerHttpRequest request = exchange.getRequest(); ServerHttpResponse response = exchange.getResponse(); String requestBody = exchange.getAttribute(PreGlobalFilter.ORIGINAL_REQUEST_BODY); String responseBody = exchange.getAttribute(PostGlobalFilter.ORIGINAL_RESPONSE_BODY); StringBuilder sb = new StringBuilder(); sb.append(\"\\n\"); URI uri = exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR); sb.append(\"URI: \").append(uri).append(\"\\n\"); sb.append(\"Method: \").append(request.getMethod()).append(\"\\n\"); sb.append(\"Request Headers: \"); request.getHeaders().forEach((key, value) -\u003e { sb.append(key).append(\"=\").append(value).append(\", \"); }); sb.append(\"\\n\"); sb.append(\"Request Body: \").append(requestBody).append(\"\\n\"); sb.append(\"\\n\"); sb.append(\"Response Status: \").append(response.getRawStatusCode()).append(\"\\n\"); sb.append(\"Response Headers: \"); response.getHeaders().forEach((key, value) -\u003e { sb.append(key).append(\"=\").append(value).append(\", \"); }); sb.append(\"\\n\"); sb.append(\"Response Body: \").append(responseBody).append(\"\\n\"); log.info(sb.toString()); exchange.getAttributes().remove(PreGlobalFilter.ORIGINAL_REQUEST_BODY); exchange.getAttributes().remove(PostGlobalFilter.ORIGINAL_RESPONSE_BODY); } } ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:5","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Conclusion In this article, we explored some of the features and components that are part of Spring Cloud Gateway. This new API provides out-of-the-box tools for gateway and proxy support. ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:6","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Reference What is Spring Cloud Gateway? Resilience4J: Circuit Breaker Implementation on Spring Boot Spring Cloud API Gateway Global Filter Example Microservices using SpringBoot | Full Example Spring Cloud Gateway Spring Cloud Gateway - Proxy/Forward the entire sub part of URL Spring Cloud Circuit Breaker Writing Custom Spring Cloud Gateway Filters Get request body string from ServerHttpRequest / Flux How to get response body in Spring Gateway ","date":"2022-04-29","objectID":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/:0:7","tags":["API Gateway","Spring Boot Cloud Gateway","Hystrix","Netflix Hystrix","Spring Cloud Gateway","Spring Boot","Java","Circuit Breaker"],"title":"Api Gateway Using Spring Boot Cloud Gateway + Netflix Hystrix","uri":"/2022/04/api-gateway-using-spring-boot-cloud-gateway/"},{"categories":["Documentation"],"content":"Continuous integration (CI) and continuous delivery (CD), also known as CI/CD, embodies a culture, operating principles, and a set of practice that application development teams use to deliver code changes more fequently and reliably.","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":" Continuous integration is a coding philosophy and set of practices that drive development teams to frequently implement small code changes and check them in to a version control repository. Most modern applications require developing code using a variety of platforms and tools, so teams need a consistent mechanism to integrate and validate changes. Continuous integration establishes an automated way to build, package, and test their applications. Having a consistent integration process encourages developers to commit code changes more frequently, which leads to better collaboration and code quality. Continuous delivery picks up where continuous integration ends, and automates application delivery to selected environments, including production, development, and testing environments. Continuous delivery is an automated way to push code changes to these environments. ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:0","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"CI/CD Benefits There are 4 benefits of CI/CD: Increase speed of innovation and ability to complete in the marketplace. Code in production is making money instead of sitting in a queue waiting to be deployed. Greate ability to attract and retain talent. Higher quality code and operation due to specialization. Automated deployment is a practice that allows you to ship code fully or semy-automatically accross several stages of the development process from initial development right through to production. The automated deployment can: Reduce possibility of errors especially human error Saving time Consistency and repeatable ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:1","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"CI/CD Tools There are several CI/CD tools that are commonly used, namely: Jenkins CircleCI Bamboo Gitlab CI Github Actions Open source Yes No No No Yes Ease of use \u0026 setup Medium Medium Medium Medium Easy Built-in features 3/5 4/5 4/5 4/5 3/5 Integration Hosting On premise \u0026 Cloud On premise \u0026 Cloud On premise \u0026 Bitbucket as Cloud On premise \u0026 Cloud Cloud Free Version Yes Yes Yes Yes Yes Supported OS Windows, Linux, MacOS, Unix-like OS Linux or MacOS Windows, Linux, MacOS, Solaris Linux distributions: Ubuntu, Debian, CentOS, Oracle Linux Linux, Windows, MacOS ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:2","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"Github Actions At the most basic level, GitHub Actions brings automation directly into the software development lifecycle on GitHub via event-driven triggers. These triggers are specified events that can range from creating a pull request to building a new brand in a repository. GitHub Actions is a CI/CD tool for the GitHub flow. You can use it to integrate and deploy code changes to a third-party cloud application platform as well as test, track, and manage code changes. GitHub Actions also supports third-party CI/CD tools, the container platform Docker, and other automation platforms. Project Preparation Before we used a Github Actions, prepare a spring boot project. You can start from Spring Initializer to create a spring boot project. And create Dockerfile into a project. # Import base JDK from Linux FROM adoptopenjdk/openjdk11:alpine # Set work directory WORKDIR /app # Copy application files COPY target/*.jar app.jar # Expose PORT EXPOSE 8080 # Run application ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"] Pipeline In github action, we will create a pipeline like following image below. Github Actions Script Put .yaml file into .github/workflows, we will use a push trigger to run github actions jobs. First we create for unit testing step. name: CI/CD Pipeline on: push: branches: - main jobs: run_test: name: Unit Test runs-on: ubuntu-18.04 steps: - run: echo \"Starting execute unit test\" - uses: actions/checkout@v3 - name: Setup JDK 11 uses: actions/setup-java@v3 with: java-version: 11 distribution: 'adopt' - name: Maven Verify run: mvn clean verify - name: Maven Sonar Code Coverage run: mvn sonar:sonar -Dsonar.projectKey=springboot-cicd -Dsonar.login=${{ secrets.SONAR_TOKEN }} Description : Using ubuntu-18.04 runner Checkout latest code from main branch Using java 11 Execute command mvn clean verify Execute command mvn sonar:sonar -Dsonar.projectKey=springboot-cicd -Dsonar.login=${{ secrets.SONAR_TOKEN }} We can change the trigger using create a tags like following below. on: create: tags: - '*SNAPSHOT' Then, create another job for build package and build images. build: name: Build runs-on: ubuntu-18.04 needs: run_test steps: - run: echo \"Starting build package\" - uses: actions/checkout@v3 - name: Setup JDK 11 uses: actions/setup-java@v3 with: java-version: 11 distribution: 'adopt' - name: Maven Build run: mvn clean package -Dmaven.test.skip=true - name: Login to docker hub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build docker image uses: docker/build-push-action@v2 with: context: . push: true tags: user/images:tag Description : Using ubuntu-18.04 runner and need job run_test successfully Checkout latest code from main branch Setup java 11 Execute command mvn clean package -Dmaven.test.skip=true Login to docker hub with credential on github secrets Execute build image from Dockerfile and push into images registry Finally, create job for deploying into server using SSH. deployment: name: Deploy container using SSH runs-on: ubuntu-18.04 needs: build steps: - run: echo \"Starting deploy container\" - uses: actions/checkout@v3 - name: Copy environment file via ssh uses: appleboy/scp-action@master with: host: ${{ secrets.SSH_HOST }} port: 22 username: ${{ secrets.SSH_USERNAME }} key: ${{ secrets.SSH_PRIVATE_KEY }} source: .env target: /home/${{ secrets.SSH_USERNAME }} - name: Deploy using ssh uses: appleboy/ssh-action@master with: host: ${{ secrets.SSH_HOST }} port: 22 username: ${{ secrets.SSH_USERNAME }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | docker stop the_container docker rmi user/images:tag docker pull user/images:tag docker run -d --rm --name the_container -p 80:8080 --env-file=.env --network another_network user/images:tag ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:3","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"Summary Developing a CI/CD pipeline is a standard practice for businesses that frequently improve applications and require a reliable delivery process. Once in place, the CI/CD pipeline lets the team focus more on enhancing applications and less on the details of delivering it to various environments. CI/CD pipelines are typically complex and have a lot of tools that range from testing applications to integration tests to container platforms and application platforms, among other things. GitHub Actions simplifies the process with Node and Docker integrations and allows you to specify which version you want to use and then connect your code to a target environment and application platform. ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:4","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"Reference What is CI/CD? Continuous integration and continuous delivery explained 4 Benefits of CI/CD Best 14 CI/CD Tools You Must Know | Updated for 2022 What is GitHub Actions? Github Actions setup-java Docker Login Action Docker Build Push Action Appleboy SCP Action Appleboy SSH Action ","date":"2022-04-16","objectID":"/2022/04/ci-cd-using-github-action/:0:5","tags":["CI/CD","DevOps","Continuous Integration","Continuous Delivery","Github Actions","Pipeline"],"title":"CI/CD Using Github Action","uri":"/2022/04/ci-cd-using-github-action/"},{"categories":["Documentation"],"content":"JPA Specification is the sense of domain driven design. Spring data JPA specification provides a convinient and sophisticated manner to build dynamic SQL where clauses.","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Introduction Spring data JPA provides many ways to deal with entities including query methods and custom JPQL queries. However, sometimes we need a more programmatic approach: for example Criteria API or QueryDSL. Spring Data JPA Specification provides a convenient and sophisticated manner to build dynamic SQL where clauses. By adding some extra logic and considering some pitfalls, we are capable of offering API consumers a zero-effort generic mechanism for filtering entities. Specification are built on top of the Criteria API to simplify the developer experience. When building a Criteria query we are required to build and manage Root, Criteria Query and Criteria Builder object by ourselves. ","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:1","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Project Setup and Dependency I’m depending Spring Initializr for this as it is much easier. We need spring-boot-starter-data-jpa, spring-boot-starter-web, lombok and h2database. There is my pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-jpa\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.h2database\u003c/groupId\u003e \u003cartifactId\u003eh2\u003c/artifactId\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e Add configuration application.properties file like following below. server.port=8080 spring.application.name=search-request server.servlet.context-path=/api spring.datasource.url=jdbc:h2:mem:db; spring.datasource.driverClassName=org.h2.Driver spring.datasource.username=sa spring.datasource.password=password spring.jpa.database-platform=org.hibernate.dialect.H2Dialect spring.h2.console.enabled=true spring.jpa.show-sql=true spring.jpa.hibernate.ddl-auto=update ","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:2","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Implementation For the sake of simplicity, in the samples, we’ll implement the same query in multiple ways: finding operating system by the name, the name containing String, release date between date, and kernel version in a values. Designing Table In this case, I will use table operating_system to simulate data to be develope. Field Type id INT (Primary Key) name VARCHAR(255) version VARCHAR(255) kernel VARCHAR(255) release_date TIMESTAMP usages INT Domain Data Access Object (DAO) Create OperatingSystem for Entity like below. @Data @Builder @NoArgsConstructor @AllArgsConstructor @Entity @Table(name = \"operating_system\") public class OperatingSystem implements Serializable { private static final long serialVersionUID = -1730538653948604611L; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = \"name\", nullable = false) private String name; @Column(name = \"version\", nullable = false) private String version; @Column(name = \"kernel\", nullable = false) private String kernel; @Column(name = \"release_date\", nullable = false) private LocalDateTime releaseDate; @Column(name = \"usages\", nullable = false) private Integer usages; } Filter Using Specification Enumeration of Field Type Let’s define enum of field type which is can be used to parse into data type. So, we can parse value into BOOLEAN, CHAR, DATE, DOUBLE, INTEGER, LONG, and STRING. @Slf4j public enum FieldType { BOOLEAN { public Object parse(String value) { return Boolean.valueOf(value); } }, CHAR { public Object parse(String value) { return value.charAt(0); } }, DATE { public Object parse(String value) { Object date = null; try { DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"dd-MM-yyyy HH:mm:ss\"); date = LocalDateTime.parse(value, formatter); } catch (Exception e) { log.info(\"Failed parse field type DATE {}\", e.getMessage()); } return date; } }, DOUBLE { public Object parse(String value) { return Double.valueOf(value); } }, INTEGER { public Object parse(String value) { return Integer.valueOf(value); } }, LONG { public Object parse(String value) { return Long.valueOf(value); } }, STRING { public Object parse(String value) { return value; } }; public abstract Object parse(String value); } Filter Request A data contract for filter request there should be a key, operator, value and fieldType. @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonIgnoreProperties(ignoreUnknown = true) @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) public class FilterRequest implements Serializable { private static final long serialVersionUID = 6293344849078612450L; private String key; private Operator operator; private FieldType fieldType; private transient Object value; private transient Object valueTo; private transient List\u003cObject\u003e values; } Enumeration of Operator This is a logical for predicate of Criteria API likes EQUAL, NOT_EQUAL, LIKE, IN, and BETWEEN. @Slf4j public enum Operator { EQUAL { public \u003cT\u003e Predicate build(Root\u003cT\u003e root, CriteriaBuilder cb, FilterRequest request, Predicate predicate) { Object value = request.getFieldType().parse(request.getValue().toString()); Expression\u003c?\u003e key = root.get(request.getKey()); return cb.and(cb.equal(key, value), predicate); } }, NOT_EQUAL { public \u003cT\u003e Predicate build(Root\u003cT\u003e root, CriteriaBuilder cb, FilterRequest request, Predicate predicate) { Object value = request.getFieldType().parse(request.getValue().toString()); Expression\u003c?\u003e key = root.get(request.getKey()); return cb.and(cb.notEqual(key, value), predicate); } }, LIKE { public \u003cT\u003e Predicate build(Root\u003cT\u003e root, CriteriaBuilder cb, FilterRequest request, Predicate predicate) { Expression\u003cString\u003e key = root.get(request.getKey()); return cb.and(cb.like(cb.upper(key), \"%\" + request.getValue().toString().toUpperCase() + \"%\"), predicate); } }, IN { public \u003cT\u003e Predicate build(Root\u003cT\u003e root, CriteriaBuilder cb, FilterRequest request, Predicate predicate) { List\u003cObject\u003e values = request.getValues(); CriteriaBuilder.In\u003cObject\u003e inClause","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:3","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Conclusion JPA Specifications provide us with a way to write reusable queries and also fluent APIs with which we can combine and build more sophisticated queries. The problem of searching and filtering is trivial to all modern day applications and the Spring Data JPA Specification provides a neat and elegant way to create dynamic queries. Please share your thoughts and suggestions on how you would like to solve the problem of searching and filtering. Spring data JPA repository abstraction allows executing predicates via JPA Criteria API predicates wrapped into a Specification object. To enable this functionality you simply let your repository extend JpaSpecificationExecutor. ","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:4","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Clone on Github piinalpin/springboot-jpa-specification ","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:5","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Reference Interface Specification Spring Data JPA: A Generic Specification Query Language Searching and Filtering: Spring Data JPA Specification way Getting Started with Spring Data Specifications Use Criteria Queries in a Spring Data Application REST Query Language with Spring Data JPA Specifications Advanced Spring Data JPA - Specifications and Querydsl ","date":"2022-04-03","objectID":"/2022/04/searching-and-filtering-using-jpa-specification/:0:6","tags":["Spring Boot","Java","JPA","JPA Specification","Criteria Builder"],"title":"Searching And Filtering Using JPA Specification - Spring Boot","uri":"/2022/04/searching-and-filtering-using-jpa-specification/"},{"categories":["Documentation"],"content":"Implementing custom jpa soft deletes because sometimes there are business requirements to not permanently delete data from the database.","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Overview Deleting data permanently from a table is a common requirement when interacting with database. But, sometimes there are business requirements to not permanently delete data from the database. The solution is we just hide that data so that can’t be accessed from the front-end. In this documentation, I will share how I implementing custom JPA repository with soft deletes using JpaRepositoryFactoryBean. So, that data can be tracked or audited when is created, updated, or deleted. For example, let’s design a table with a book sale case study like this. There are created_at, created_by, updated_at and deleted_at fields. Some case updated_at can be replace with modified_at and modified_by. But, the point is deleted_at field. ","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:1","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Project Setup and Dependency I’m depending Spring Initializr for this as it is much easier. We need spring-boot-starter-data-jpa, spring-boot-starter-web, lombok and h2database. There is my pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-jpa\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.h2database\u003c/groupId\u003e \u003cartifactId\u003eh2\u003c/artifactId\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e Change configuration application.properties file like following below. server.port=8080 spring.application.name=custom-soft-deletes server.servlet.context-path=/api spring.datasource.url=jdbc:h2:mem:db; spring.datasource.driverClassName=org.h2.Driver spring.datasource.username=sa spring.datasource.password=password spring.jpa.database-platform=org.hibernate.dialect.H2Dialect spring.h2.console.enabled=true spring.jpa.show-sql=true ","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:2","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Implementation Soft Deletes Repository Interface Create an interface SoftDeletesRepository\u003cT, ID\u003e which will be used to replace the repository that inherit from JpaRepository\u003cT, ID\u003e. @SuppressWarnings(\"java:S119\") @Transactional @NoRepositoryBean public interface SoftDeletesRepository\u003cT, ID extends Serializable\u003e extends PagingAndSortingRepository\u003cT, ID\u003e { @Override Iterable\u003cT\u003e findAll(); @Override Iterable\u003cT\u003e findAll(Sort sort); @Override Page\u003cT\u003e findAll(Pageable page); Optional\u003cT\u003e findOne(ID id); @Modifying void delete(ID id); @Override @Modifying void delete(T entity); void hardDelete(T entity); } Create an implementation from SoftDeletesRepository\u003cT, ID\u003e interface class. @SuppressWarnings(\"java:S119\") @Slf4j public class SoftDeletesRepositoryImpl\u003cT, ID extends Serializable\u003e extends SimpleJpaRepository\u003cT, ID\u003e implements SoftDeletesRepository\u003cT, ID\u003e { private final JpaEntityInformation\u003cT, ?\u003e entityInformation; private final EntityManager em; private final Class\u003cT\u003e domainClass; private static final String DELETED_FIELD = \"deletedAt\"; public SoftDeletesRepositoryImpl(Class\u003cT\u003e domainClass, EntityManager em) { super(domainClass, em); this.em = em; this.domainClass = domainClass; this.entityInformation = JpaEntityInformationSupport.getEntityInformation(domainClass, em); } @Override public Optional\u003cT\u003e findOne(ID id) { return Optional.empty(); } @Override public void delete(ID id) { } @Override public void hardDelete(T entity) { } } Add method in SoftDeletesRepositoryImpl to check if field deletedAt is exist on super class, because some entity have deletedAt some case the don’t have. So, I create method returning boolean to handle that. private boolean isFieldDeletedAtExists() { try { domainClass.getSuperclass().getDeclaredField(DELETED_FIELD); return true; } catch (NoSuchFieldException e) { return false; } } Create predicate specification class to filter entity if deletedAt is null. So, if translated in a native query is SELECT * FROM table WHERE deleted_at is null. private static final class DeletedIsNUll\u003cT\u003e implements Specification\u003cT\u003e { private static final long serialVersionUID = -940322276301888908L; @Override public Predicate toPredicate(Root\u003cT\u003e root, CriteriaQuery\u003c?\u003e query, CriteriaBuilder criteriaBuilder) { return criteriaBuilder.isNull(root.\u003cLocalDateTime\u003eget(DELETED_FIELD)); } } private static \u003cT\u003e Specification\u003cT\u003e notDeleted() { return Specification.where(new DeletedIsNUll\u003c\u003e()); } Create predicate specification class to filter entity by ID. And can be reuse with notDeleted() or without notDeleted(). If I translated in sql is SELECT * FROM table WHERE id = ? or SELECT * FROM table WHERE id = ? AND deletedAt is null. private static final class ByIdSpecification\u003cT, ID\u003e implements Specification\u003cT\u003e { private static final long serialVersionUID = 6523470832851906115L; private final transient JpaEntityInformation\u003cT, ?\u003e entityInformation; private final transient ID id; ByIdSpecification(JpaEntityInformation\u003cT, ?\u003e entityInformation, ID id) { this.entityInformation = entityInformation; this.id = id; } @Override public Predicate toPredicate(Root\u003cT\u003e root, CriteriaQuery\u003c?\u003e query, CriteriaBuilder cb) { return cb.equal(root.\u003cID\u003eget(Objects.requireNonNull(entityInformation.getIdAttribute()).getName()), id); } } Then, create method to do updating deletedAt with LocalDateTime.now() when process delete data. private void softDelete(ID id, LocalDateTime localDateTime) { Assert.notNull(id, \"The given id must not be null!\"); Optional\u003cT\u003e entity = findOne(id); if (entity.isEmpty()) throw new EmptyResultDataAccessException( String.format(\"No %s entity with id %s exists!\", entityInformation.getJavaType(), id), 1); softDelete(entity.get(), localDateTime); } private void softDelete(T entity, LocalDateTime localDateTime) { Assert.notNull(entity, \"The entity must not be null!\"); CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaUpdate\u003cT\u003e update = cb.createCriteriaUpdate(domainClass); Root\u003cT\u003e root = update.from(domainClass); update.set(DELETED_FIELD,","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:3","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Spring Boot JPA Relational Many-to-One and One-to-One Let’s see our ERD in the top page, there are Many-to-One, One-to-Many, Many-to-Many and One-to-One relationship. I will implement the relation of M_AUTHOR, M_BOOK and M_BOOK_DETAIL first. Entity Class Book @EqualsAndHashCode(callSuper = true) @Data @Entity @SuperBuilder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) @Table(name = \"M_BOOK\") public class Book extends BaseEntityWithDeletedAt { private static final long serialVersionUID = 3000665212891573963L; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @ManyToOne @JoinColumn(name = \"author_id\", nullable = false) private Author author; @Column(name = \"title\", nullable = false) private String title; @Column(name = \"price\", nullable = false) private Integer price; @JsonIgnore @OneToOne(cascade = CascadeType.ALL) private BookDetail detail; } Author Add this method to mapping an author have books. @JsonIgnore @OneToMany(fetch = FetchType.LAZY, cascade = CascadeType.ALL, mappedBy = \"author\") private List\u003cBook\u003e books; BookDetail @EqualsAndHashCode(callSuper = true) @Data @Entity @SuperBuilder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) @Table(name = \"M_BOOK_DETAIL\") public class BookDetail extends BaseEntityWithDeletedAt { private static final long serialVersionUID = -4930414280222129820L; /** * @Id column should exists for one to one relationship */ @Id @JsonIgnore @GeneratedValue(strategy = GenerationType.IDENTITY) private Long bookId; @OneToOne(mappedBy = \"detail\") @JoinColumn(name = \"book_id\", nullable = false) private Book book; @Column(name = \"page\", nullable = false) private Integer page; @Column(name = \"weight\", nullable = false) private Integer weight; } In this case, BookDetail should not have field bookId but the JPA entity should have an id. So, I added bookId as id field but it is ignored from json. BookRequest @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) @JsonIgnoreProperties(ignoreUnknown = true) public class BookRequest implements Serializable { private static final long serialVersionUID = 7993247371386533518L; private Long authorId; private String title; private Integer price; private Integer page; private Integer weight; } Repository, Service and Controller BookRepository @Repository public interface BookRepository extends SoftDeletesRepository\u003cBook, Long\u003e { } BookDetailRepository @Repository public interface BookDetailRepository extends SoftDeletesRepository\u003cBookDetail, Long\u003e { } BookService @Slf4j @Service public class BookService { private final AuthorRepository authorRepository; private final BookRepository bookRepository; private final BookDetailRepository bookDetailRepository; @Autowired public BookService(AuthorRepository authorRepository, BookRepository bookRepository, BookDetailRepository bookDetailRepository) { this.authorRepository = authorRepository; this.bookRepository = bookRepository; this.bookDetailRepository = bookDetailRepository; } public ResponseEntity\u003cObject\u003e addBook(BookRequest request) { log.info(\"Save new book: {}\", request); log.info(\"Find author by author id\"); Optional\u003cAuthor\u003e author = authorRepository.findOne(request.getAuthorId()); if (author.isEmpty()) return ResponseEntity.notFound().build(); Book book = Book.builder() .author(author.get()) .detail(BookDetail.builder() .page(request.getPage()) .weight(request.getWeight()) .build()) .title(request.getTitle()) .price(request.getPrice()) .build(); return ResponseEntity.ok().body(bookRepository.save(book)); } public ResponseEntity\u003cObject\u003e getAllBook() { return ResponseEntity.ok().body(bookRepository.findAll()); } public ResponseEntity\u003cObject\u003e getBookDetail(Long bookId) { log.info(\"Find book detail by book id: {}\", bookId); Optional\u003cBookDetail\u003e bookDetail = bookDetailRepository.findOne(bookId); if (bookDetail.isEmpty()) return Respon","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:4","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Clone on Github piinalpin/springboot-data-jpa-soft-delete ","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:5","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"Reference Working with Spring Data Repositories Spring Boot JPA Soft Deletes with Spring Data Rest Composite Primary Keys in JPA Spring Boot With H2 Database ","date":"2022-03-20","objectID":"/2022/03/jpa-soft-deletes-implementation/:0:6","tags":["Spring Boot","Java","JPA","Soft Delete","Custom JPA Repository Factory Bean","JPA Repositoriy Factory Bean"],"title":"JPA Soft Deletes Implementation - Spring Boot","uri":"/2022/03/jpa-soft-deletes-implementation/"},{"categories":["Documentation"],"content":"A little things but important when uploading files through REST template. It should send the filename in the correct format.","date":"2022-02-19","objectID":"/2022/02/best-practice-uploading-file-using-rest-template/","tags":["Spring Boot","Java","REST Template","Upload File","Form Data","Multipart"],"title":"Best Practice Uploading File Using Spring RestTemplate","uri":"/2022/02/best-practice-uploading-file-using-rest-template/"},{"categories":["Documentation"],"content":"Overview Rest Template is used to create applications that consume RESTful Web Services. You can use the exchange() method to consume the web services for all HTTP methods. The code given below shows how to create Bean for Rest Template to auto wiring the Rest Template object. A little things but important when uploading files through REST template. It should send the filename in the correct format. A correct file upload request should be like this. HTTP Headers Content-Type: multipart/form-data; boundary=/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB HTTP Body --/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB Content-Disposition: form-data; name=\"file\"; filename=\"my-uploaded-file.png\" Content-Type: application/octet-stream Content-Length: ... \u003c...file data in base 64...\u003e --/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB-- There can be multiple sections like this in the body with additional data, when more data needs to be posted to the server, like multiple files or other metadata. The Content-Type could also be adjusted. application-octet-stream is the default Spring uses for byte[] data. So, I will create utility class to convert base64 string into http entity with file metadata. public class Base64Util { private Base64Util() {} public static String generateFilename(String base64String) { if (StringUtils.isBlank(base64String)) { return base64String; } String[] arrayString = base64String.split(\",\"); String header = arrayString[0].split(\";\")[0]; String uuid = UUID.randomUUID().toString(); String filename = String.format(\"%s.%s\", uuid, header.split(\"/\")[1]); return filename; } public static String stripStartBase64(String base64String) { if (StringUtils.isBlank(base64String)) { return base64String; } return base64String.replaceAll(\"^data:image/[^;]*;base64,?\", \"\"); } public static HttpEntity\u003cbyte[]\u003e convertToHttpEntity(String filename, String base64) { byte[] imageByte = Base64.decodeBase64(base64); ContentDisposition contentDisposition = ContentDisposition.builder(\"form-data\") .name(\"myImage\") .filename(filename) .build(); MultiValueMap\u003cString, String\u003e fileMap = new LinkedMultiValueMap\u003c\u003e(); fileMap.add(HttpHeaders.CONTENT_DISPOSITION, contentDisposition.toString()); return new HttpEntity\u003c\u003e(imageByte, fileMap); } } And a service will send request template for another API like this. @Slf4j @Service public class FileUploadService { @Autowired private RestTemplate restTemplate; public void sendFile(String base64Image) { log.info(\"Start executing send file through rest template.\"); try { log.info(\"Sending files...\"); ResponseEntity\u003cObject\u003e responseEntity = restTemplate.postForEntity(\"/some-url-to-send-file\", constructRequest(base64Image), Object.class); log.info(\"Done uploading files.\"); } catch (Exception e) { log.error(\"Error when uploading file. Error: {}\", e.getMessage()); throw e; } } private HttpEntity\u003cMultiValueMap\u003cString, Object\u003e\u003e constructRequest(String base64Image) throws IOException { MultiValueMap\u003cString, Object\u003e body = new LinkedMultiValueMap\u003c\u003e(); body.add(\"myImage\", Base64Util.convertToHttpEntity(Base64Util.generateFilename(base64Image), Base64Util.stripStartBase64(base64Image))); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.MULTIPART_FORM_DATA); headers.add(\"Some-Header\", \"Some-Value\"); return new HttpEntity\u003c\u003e(body, headers); } } Most solutions on google search, you find will not use the embedded HttpEntity, but will just add two entries to the MultiValueMap for the body like so: MultiValueMap\u003cString, Object\u003e body = new LinkedMultiValueMap\u003c\u003e(); body.add(\"filename\", \"some-file-name.jpg\"); body.add(\"file\", new ByteArrayResource(someByteArray)); This produces a different request body, where the file and the filename are embedded in two different sections like this: --/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB Content-Disposition: form-data; name=\"filename\" Content-Type: text/plain;charset=UTF-8 Content-Length: 6 some-f","date":"2022-02-19","objectID":"/2022/02/best-practice-uploading-file-using-rest-template/:0:1","tags":["Spring Boot","Java","REST Template","Upload File","Form Data","Multipart"],"title":"Best Practice Uploading File Using Spring RestTemplate","uri":"/2022/02/best-practice-uploading-file-using-rest-template/"},{"categories":["Documentation"],"content":"Reference Uploading a file with a filename with Spring RestTemplate ","date":"2022-02-19","objectID":"/2022/02/best-practice-uploading-file-using-rest-template/:0:2","tags":["Spring Boot","Java","REST Template","Upload File","Form Data","Multipart"],"title":"Best Practice Uploading File Using Spring RestTemplate","uri":"/2022/02/best-practice-uploading-file-using-rest-template/"},{"categories":["Documentation"],"content":"Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing.","date":"2021-12-10","objectID":"/2021/12/image-processing-using-tensorflow/","tags":["Image Processing","Tensorflow","Artificial Intelligence","Keras","Python"],"title":"Image Processing Using Tensorflow - Convolutional Neural Network (CNN)","uri":"/2021/12/image-processing-using-tensorflow/"},{"categories":["Documentation"],"content":"Overview Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. Machine learning is a complex discipline. But implementing machine learning models is far less daunting and difficult than it used to be, thanks to machine learning frameworks—such as Google’s TensorFlow—that ease the process of acquiring data, training models, serving predictions, and refining future results. Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. TensorFlow bundles together a slew of machine learning and deep learning (aka neural networking) models and algorithms and makes them useful by way of a common metaphor. It uses Python to provide a convenient front-end API for building applications with the framework, while executing those applications in high-performance C++. ","date":"2021-12-10","objectID":"/2021/12/image-processing-using-tensorflow/:0:1","tags":["Image Processing","Tensorflow","Artificial Intelligence","Keras","Python"],"title":"Image Processing Using Tensorflow - Convolutional Neural Network (CNN)","uri":"/2021/12/image-processing-using-tensorflow/"},{"categories":["Documentation"],"content":"Study Case In this case, we will detect the image whether the image is an indication of malaria infection or not. We will use dataset which can be download here. Directory Structure . ├── ... ├── dataset │ └── parasitized/ │ ├── uninfected/ ├── test-data/ │ └── ... └── research.ipynb └── ... Import Dependency and Count Dataset We will use tensorflow.keras library like following code. from tensorflow import keras from tensorflow.keras import Sequential from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import load_model from PIL import Image from skimage import transform import os import numpy as np import glob Count our dataset in this case. parasitized = glob.glob('dataset/parasitized/*.png') uninfected = glob.glob('dataset/uninfected/*.png') print(\"Parasitized data: {}\\nUninfected data: {}\".format(len(parasitized), len(uninfected))) Preprocessing Data augmentation, based on wikipedia data augmentation is techniques are used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. dimension = 128 batch = 32 data_dir = \"dataset\" datagen = ImageDataGenerator(rescale=1/255.0, validation_split=0.2, zoom_range=0.05, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True) train_data = datagen.flow_from_directory(data_dir, target_size=(dimension, dimension), batch_size=batch, class_mode='categorical', subset='training') validation_data = datagen.flow_from_directory(data_dir, target_size=(dimension, dimension), batch_size=batch, class_mode='categorical', subset='validation', shuffle=False) test_data = datagen.flow_from_directory(data_dir, target_size=(dimension, dimension), batch_size=1, shuffle=False) Training Data and Modelling The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process. model = Sequential() model.add(Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=(dimension, dimension, 3))) model.add(MaxPooling2D(pool_size=2)) model.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\")) model.add(MaxPooling2D(pool_size=2)) model.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")) model.add(MaxPooling2D(pool_size=2)) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(64, activation=\"relu\")) model.add(Dropout(0.2)) model.add(Dense(2, activation=\"softmax\")) # Compile the model model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Save the best trained model by monitoring validation loss model_name = \"train_data_model.b\" model_checkpoint = ModelCheckpoint(model_name, save_weights_only=False, monitor='val_loss', verbose=1, mode='auto', save_best_only=True) # Training dataset history = model.fit(train_data, batch_size=batch, epochs=30, validation_data=validation_data, callbacks = [model_checkpoint], verbose=1) Model Evaluation Evaluation is a process during development of the model to check whether the model is best fit for the given problem and corresponding data. Keras model provides a function, evaluate which does the evaluation of the model. It has three main arguments : Test data Test data label verbose - true or false Let us evaluate the model, which we created in the previous chapter using test data. # Load compiled model model = load_model(\"train_data_model.b\") model.evaluate(test_data, verbose=0) Prediction Prediction is fitting a shape that gets as close to the data as possible. The object we’re fitting is more of a skeleton that goes through one body of data instead of a fence that goes between separate bodies of data. # Load image from filename def load(filename): np_image = Image.open(filename)","date":"2021-12-10","objectID":"/2021/12/image-processing-using-tensorflow/:0:2","tags":["Image Processing","Tensorflow","Artificial Intelligence","Keras","Python"],"title":"Image Processing Using Tensorflow - Convolutional Neural Network (CNN)","uri":"/2021/12/image-processing-using-tensorflow/"},{"categories":["Documentation"],"content":"Source You can clone or download the source in https://github.com/piinalpin/research-collection/tree/master/tensorflow/malaria-detection ","date":"2021-12-10","objectID":"/2021/12/image-processing-using-tensorflow/:1:0","tags":["Image Processing","Tensorflow","Artificial Intelligence","Keras","Python"],"title":"Image Processing Using Tensorflow - Convolutional Neural Network (CNN)","uri":"/2021/12/image-processing-using-tensorflow/"},{"categories":["Documentation"],"content":"Reference Digital Image Processing - Wikipedia Tensorflow Sequential Classification, regression, and prediction — what’s the difference? Model Evaluation and Model Prediction Training ML Models What is Data Augmentation? Techniques, Benefit \u0026 Examples ","date":"2021-12-10","objectID":"/2021/12/image-processing-using-tensorflow/:1:1","tags":["Image Processing","Tensorflow","Artificial Intelligence","Keras","Python"],"title":"Image Processing Using Tensorflow - Convolutional Neural Network (CNN)","uri":"/2021/12/image-processing-using-tensorflow/"},{"categories":["Documentation"],"content":"Object-Relational Mapping (ORM) is a technique that lets you query and manipulate data from a database using an object-oriented paradigm.","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"Prerequisites Node JS ","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:1","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"Overview Object-Relational Mapping (ORM) is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. When talking about ORM, most people are referring to a library that implements the Object-Relational Mapping technique, hence the phrase “an ORM”. ","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:2","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"Project Setup and Dependencies Create account in Clever Cloud and create a database account, we will use a Postgre SQL for the database. Choose Personal Space -\u003e Create -\u003e ad add-on then choose a Postgre SQL and click select. After that choose a plan like below. Create a project, I use yarn package manager for creating this project. And fill the answer from the question of creating a project. mkdir orm-nodejs cd orm-nodejs yarn init Create .env file and fill like below. # Server SERVER_PORT=8000 # Database DB_HOST=\u003cYOUR DATABASE HOST\u003e DB_USER=\u003cYOUR DATABASE USER\u003e DB_PASSWORD=\u003cYOUR DATABASE PASSWORD\u003e DB_PORT=\u003cYOUR DATABASE PORT\u003e DB_NAME=\u003cYOUR DATABASE NAME\u003e DB_DIALECT=\u003cYOUR DATABASE DIALECT\u003e Project Structure . ├── ... ├── app │ └── config/ │ ├── model/ │ ├── routes/ │ ├── service/ │ ├── util/ │ └── ... └── .env └── package.json └── routes.js └── schemas.sql └── server.js └── yarn.lock └── ... Database Design I design table for this case using dbdiagram.io and I was create a schemas that can be use to import like following code. CREATE SEQUENCE id_transaction_seq start 1 increment 1; CREATE SEQUENCE id_transaction_detail_seq start 1 increment 1; CREATE TABLE \"transaction\" ( \"id\" bigint PRIMARY KEY NOT NULL DEFAULT nextval('id_transaction_seq'), \"created_at\" TIMESTAMP NOT NULL, \"updated_at\" TIMESTAMP, \"transaction_ref_number\" varchar(255) NOT NULL, \"transaction_date\" TIMESTAMP NOT NULL, \"customer_name\" varchar(255) NOT NULL, \"cost\" bigint NOT NULL, \"shipping_cost\" bigint NOT NULL ); CREATE TABLE \"transaction_detail\" ( \"id\" bigint PRIMARY KEY NOT NULL DEFAULT nextval('id_transaction_detail_seq'), \"created_at\" TIMESTAMP NOT NULL, \"updated_at\" TIMESTAMP, \"transaction_id\" bigint NOT NULL, \"stuff\" varchar(255) NOT NULL, \"price\" bigint NOT NULL, \"qty\" int NOT NULL, \"total\" bigint NOT NULL, CONSTRAINT fk_transaction FOREIGN KEY (transaction_id) REFERENCES transaction(id) ); Install Dependency Install the required dependencies, type yarn command in terminal like below. yarn add express sequelize dotenv pg pg-hstore cors nodemon Add codes below into package.json. \"scripts\": { \"dev\": \"nodemon server\" }, \"type\": \"module\", ","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:3","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"Implementation Create Initial Routes and Server Create an initial routes like below. import { Router } from 'express'; const router = Router(); router.get('/', (req, res, next) =\u003e { res.json({message: 'ok'}); }); export default router; Create a server to provide the api, fill the code like below. import express, { json, urlencoded } from \"express\"; import cors from \"cors\"; import dotenv from \"dotenv\"; import routes from \"./routes.js\"; dotenv.config(); const app = express(); const PORT = process.env.SERVER_PORT; var corsOptions = { origin: \"http://localhost:8000\" }; app.use(cors(corsOptions)); app.use(json()); app.use(urlencoded({ extended: true })); app.use('/api', routes); app.listen(PORT, () =\u003e { console.log(\"Running on port \", PORT); }); Try to run with yarn run dev and consume base path with curl http://localhost:8000/api/ in terminal like below. And we should get an output {\"message\":\"ok\"} Database Configuration Create database configuration with .env file to load properties in app/config/database.js like following code. import dotenv from 'dotenv'; import { Sequelize } from 'sequelize'; dotenv.config(); const db = new Sequelize( process.env.DB_NAME, process.env.DB_USER, process.env.DB_PASSWORD, { host: process.env.DB_HOST, dialect: process.env.DB_DIALECT, pool: { max: 5, min: 0, acquire: 30000, idle: 10000 } } ) export default db; Model Create model transaction and transaction_detail in app/model/**. First, we create transaction.js like following code. import Sequelize from 'sequelize'; import db from '../config/database.js'; const Transaction = db.define('transaction', { id: { type: Sequelize.INTEGER, primaryKey: true, allowNull: false, defaultValue: db.Sequelize.literal(\"nextval('id_transaction_seq')\") }, createdAt: { type: Sequelize.DATE, defaultValue: Sequelize.NOW, allowNull: false, allowNull: false }, updatedAt: { type: Sequelize.DATE, defaultValue: Sequelize.NOW, allowNull: false }, transactionRefNumber: { type: Sequelize.STRING, allowNull: false }, transactionDate: { type: Sequelize.DATE, allowNull: false }, customerName: { type: Sequelize.STRING, allowNull: false }, cost: { type: Sequelize.BIGINT, allowNull: false }, shippingCost: { type: Sequelize.BIGINT, allowNull: false } }, { tableName: 'transaction', underscored: true }); export default Transaction; And transactionDetail.js model like following code. import Sequelize from 'sequelize'; import db from '../config/database.js'; import Transaction from './transaction.js'; const TransactionDetail = db.define('transaction_detail', { id: { type: Sequelize.INTEGER, primaryKey: true, allowNull: false, defaultValue: db.Sequelize.literal(\"nextval('id_transaction_detail_seq')\") }, createdAt: { type: Sequelize.DATE, defaultValue: Sequelize.NOW, allowNull: false, allowNull: false }, updatedAt: { type: Sequelize.DATE, defaultValue: Sequelize.NOW, allowNull: false }, transactionId: { type: Sequelize.BIGINT, allowNull: false, references: { model: 'transaction', // 'transaction' refer to table name key: 'id' } }, stuff: { type: Sequelize.STRING, allowNull: false }, price: { type: Sequelize.BIGINT, allowNull: false }, qty: { type: Sequelize.BIGINT, allowNull: false }, total: { type: Sequelize.BIGINT, allowNull: false } }, { tableName: 'transaction_detail', underscored: true }); TransactionDetail.belongsTo(Transaction, { as: 'transaction', foreignKey: 'transactionId', targetKey: 'id' }); export default TransactionDetail; Take note for this line TransactionDetail.belongsTo(Transaction, { as: 'transaction', foreignKey: 'transactionId', targetKey: 'id' }); That mean, TransactionDetail have a constraint to Transaction model. Service to Create a new Transaction Create app/service/transactionService.js and create method to insert a new transaction like following code. import Transaction from '../model/transaction.js'; import TransactionDetail from '../model/transactionDetail.js'; import db from '../config/database.js' export const create = async (req, res, next) =\u003e { console.log('Sta","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:4","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"Clone or download You can clone or download this project at https://github.com/piinalpin/nodejs-orm.git ","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:5","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"References IndahSRamonasari - NodeJS User Product Sequelize - Model Basic Sequelize - Transactions Auto Increment Id with Sequelize Sequelize Belong to Relationship Sequelize Foreign Key mesaugat - Express API ES6 Starter ","date":"2021-12-07","objectID":"/2021/12/orm-nodejs-sequelize/:0:6","tags":["Node JS","Express JS","Javascript","Sequelize","Object Relational Mapping","ORM","ES6","Ecma Script"],"title":"Object Relational Mapping Node JS using Sequelize","uri":"/2021/12/orm-nodejs-sequelize/"},{"categories":["Documentation"],"content":"MinIO is a High Performance Object Storage released under GNU Affero General Public License v3.0. It is API compatible with Amazon S3 cloud storage service. Use MinIO to build high performance infrastructure for machine learning, analytics and application data workloads.","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Overview MinIO is a High Performance Object Storage released under GNU Affero General Public License v3.0. It is API compatible with Amazon S3 cloud storage service. Use MinIO to build high performance infrastructure for machine learning, analytics and application data workloads. Minio allows the upload and download of files for containerized applications, respecting the interfaces of Amazon S3 solution. The Minio API is requested as HTTP, which allows interoperability regardless of the framework or language used. In the article, I would use the following terms, which are specific to Minio or S3 Bucket: Contains a set of files. Prefix : Virtually, this is a set of directories in which the file is located. All the files are arranged at the root of the bucket, and have a prefix of kind my/prefix/file.pdf. Minio is a self-hosted solution, you can install it by following instructions here. There is also a public instance to test on https://play.min.io/minio/. You can use the following credentials : Access Key : minioadmin Secret Key : minioadmin Run Standalone MinIO on Docker MinIO needs a persistent volume to store configuration and application data. However, for testing purposes, you can launch MinIO by simply passing a directory (/data in the example below). This directory gets created in the container filesystem at the time of container start. But all the data is lost after container exits. docker run \\ -p 9000:9000 \\ -p 9001:9001 \\ -e \"MINIO_ROOT_USER=\u003cUSERNAME\u003e\" \\ -e \"MINIO_ROOT_PASSWORD=\u003cPASSWORD\u003e\" \\ quay.io/minio/minio server /data --console-address \":9001\" ","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/:0:1","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Project Setup and Dependencies I’m depending Spring Initializr for this as it is much easier. Our example application will be a Spring Boot application. So we need to add some dependencies to our pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cversion\u003e1.18.12\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.minio\u003c/groupId\u003e \u003cartifactId\u003eminio\u003c/artifactId\u003e \u003cversion\u003e8.3.0\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.squareup.okhttp3\u003c/groupId\u003e \u003cartifactId\u003eokhttp\u003c/artifactId\u003e \u003cversion\u003e4.9.1\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecommons-io\u003c/groupId\u003e \u003cartifactId\u003ecommons-io\u003c/artifactId\u003e \u003cversion\u003e2.11.0\u003c/version\u003e \u003c/dependency\u003e Change configuration application.properties file like following below. I will use play.min.io to this documentation which is open source and for demo the other projects. server.port=8080 spring.servlet.multipart.max-file-size=2MB # Minio minio.bucket.name=minio-example-demo minio.access.key=minioadmin minio.access.secret=minioadmin minio.url=https://play.min.io Upload Some Image on Minio I will create folder inside the minio-example-demo bucket is myfolder and upload some images. ","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/:0:2","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Implementation Bean Configuration Create bean configuration that can be used for dependency injection on com.piinalpin.minio.config.MinioConfiguration like following below. @Configuration public class MinioConfiguration { @Value(\"${minio.access.key}\") private String accessKey; @Value(\"${minio.access.secret}\") private String secretKey; @Value(\"${minio.url}\") private String minioUrl; @Bean @Primary public MinioClient minioClient() { return new MinioClient.Builder() .credentials(accessKey, secretKey) .endpoint(minioUrl) .build(); } } Data Transfer Object Create a dto class to construct object even for request or response com.piinalpin.minio.http.dto.FileDto like following below. @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) @JsonInclude(JsonInclude.Include.NON_NULL) public class FileDto implements Serializable { private static final long serialVersionUID = 232836038145089522L; private String title; private String description; @SuppressWarnings(\"java:S1948\") private MultipartFile file; private String url; private Long size; private String filename; } Get List Objects from Bucket Create service to get list objects inside bucket at om.piinalpin.minio.service.MinioService like following below. First we should inject MinioClient to use minio and define bucket name get from properties file. @Autowired private MinioClient minioClient; @Value(\"${minio.bucket.name}\") private String bucketName; Create method to get list objects from bucket in minio like following below. @Slf4j @Service public class MinioService { @Autowired private MinioClient minioClient; @Value(\"${minio.bucket.name}\") private String bucketName; public List\u003cFileDto\u003e getListObjects() { List\u003cFileDto\u003e objects = new ArrayList\u003c\u003e(); try { Iterable\u003cResult\u003cItem\u003e\u003e result = minioClient.listObjects(ListObjectsArgs.builder() .bucket(bucketName) .recursive(true) .build()); for (Result\u003cItem\u003e item : result) { objects.add(FileDto.builder() .filename(item.get().objectName()) .size(item.get().size()) .url(getPreSignedUrl(item.get().objectName())) .build()); } return objects; } catch (Exception e) { log.error(\"Happened error when get list objects from minio: \", e); } return objects; } private String getPreSignedUrl(String filename) { return \"http://localhost:8080/file/\".concat(filename); } } Create a controller to interacted with user to get list objects of bucket com.piinalpin.minio.http.controller.FileController like following below. @Slf4j @RestController @RequestMapping(value = \"/file\") public class FileController { @Autowired private MinioService minioService; @GetMapping public ResponseEntity\u003cObject\u003e getFiles() { return ResponseEntity.ok(minioService.getListObjects()); } } Let’s try to running spring boot application by typing command on your terminal like following below. mvn spring-boot:run And open your postman or thunder client or anything else to get list objects of bucket by accessing http://localhost:8080/file. Then we should get an output. [ { \"url\": \"http://localhost:8080/file/myfolder/microservices-portable-networkjpg.jpg\", \"size\": 25816, \"filename\": \"myfolder/microservices-portable-networkjpg.jpg\" }, { \"url\": \"http://localhost:8080/file/surrounding.png\", \"size\": 37600, \"filename\": \"surrounding.png\" } ] Upload File into Minio Create method in MinioService to upload file like following below. public FileDto uploadFile(FileDto request) { try { minioClient.putObject(PutObjectArgs.builder() .bucket(bucketName) .object(request.getFile().getOriginalFilename()) .stream(request.getFile().getInputStream(), request.getFile().getSize(), -1) .build()); } catch (Exception e) { log.error(\"Happened error when upload file: \", e); } return FileDto.builder() .title(request.getTitle()) .description(request.getDescription()) .size(request.getFile().getSize()) .url(getPreSignedUrl(request.getFile().getOriginalFilename())) .filename(request.getFile().getOriginalFilename()) .build(); } Create another controller to handle upload ","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/:0:3","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project at https://github.com/piinalpin/minio-example.git ","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/:0:4","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Reference MinIO Quickstart Guide MinIO Docker Quickstart Guide Minio and Spring Boot with Minio starter Spring boot: uploading and downloading file from Minio object store ","date":"2021-11-13","objectID":"/2021/11/cloud-object-storage-spring-boot-with-minio/:0:5","tags":["Spring Boot","Java","Cloud Object Storage","Cloud Storage","Minio","Minio Server","Minio Client"],"title":"Cloud Object Storage Spring Boot With Minio","uri":"/2021/11/cloud-object-storage-spring-boot-with-minio/"},{"categories":["Documentation"],"content":"Build real time chat with bot using spring boot web socket and contextual chat bot python.","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Overview WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C. The WebSocket protocol enables interaction between a web browser (or other client application) and a web server with lower overhead than half-duplex alternatives such as HTTP polling, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, a two-way ongoing conversation can take place between the client and the server. ","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/:0:1","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Project Setup and Dependencies I’m depending Spring Initializr for this as it is much easier. And we have to create two spring boot projects and started with maven project. Our example application will be a Spring Boot application. So we need to add spring-kafka and spring-boot-starter-web dependency to our pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-websocket\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e This project will use existing chat bot python on previous documentation, see Contextual Chat Bot using NLP. Change configuration file in src/main/resources/application.properties like following below. server.port=8081 chatbot.url=http://localhost:8000 ","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/:0:2","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Implementation Bean Configuration Create bean configuration that can be used for dependency injection at com.piinalpin.websocketserver.config.BeanConfiguration like following code. @Configuration public class BeanConfiguration { @Bean public ObjectMapper objectMapper() { ObjectMapper mapper = new ObjectMapper(); mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); mapper.registerModule(new JavaTimeModule()); mapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS); mapper.setDateFormat(new SimpleDateFormat(DateTimeFormat.DATE_TIME)); mapper.setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE); return mapper; } @Bean public RestTemplate restTemplate() { return new RestTemplate(); } } And also create WebSocketConfiguration on package com.piinalpin.websocketserver.config like following code. @Configuration @EnableWebSocket @Slf4j public class WebSocketConfiguration implements WebSocketConfigurer { @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry webSocketHandlerRegistry) { webSocketHandlerRegistry.addHandler(getChatWebSocketHandler(), \"/chat\").setAllowedOrigins(\"*\"); } @Bean public WebSocketHandler getChatWebSocketHandler() { return new ChatWebSocketHandler(); } } Constant and Data Transfer Object Create constant DateTimeFormat in package com.piinalpin.websocketserver.config like following code. public class DateTimeFormat { private DateTimeFormat() {} public static final String DATE_TIME = \"dd-MM-yyyy HH:mm:ss\"; } And create data transfer object in package com.piinalpin.websocketserver.domain.dto Create MessageDto like following code. @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class) public class MessageDto implements Serializable { private static final long serialVersionUID = -5912093781671152609L; private String message; } Create MessageResponse to mapping response from chat bot, like following code.. @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class) public class MessageResponse\u003cT extends Serializable\u003e implements Serializable { private static final long serialVersionUID = -7611957408262340406L; private String type; private T data; } Create RiddlesDto like following code. @Data @Builder @NoArgsConstructor @AllArgsConstructor @JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class) public class RiddlesDto implements Serializable { private static final long serialVersionUID = 1146262887785483279L; private String question; private String answer; } Message Handler Create chat web socket handler on package com.piinalpin.websocketserver.handler, we will create a class ChatWebSocketHandler like following code. @Slf4j public class ChatWebSocketHandler extends TextWebSocketHandler { @Autowired private ObjectMapper mapper; @Autowired private RestTemplate restTemplate; @Value(\"${chatbot.url}\") private String chatbotUrl; @Override protected void handleTextMessage(WebSocketSession session, TextMessage message) throws Exception { super.handleTextMessage(session, message); log.info(\"Payload: \" + message.getPayload()); MessageDto messageDto = mapper.readValue(message.getPayload(), MessageDto.class); if (StringUtils.isEmpty(messageDto.getMessage())) return; ResponseEntity\u003cMessageResponse\u003e responseEntity; try { responseEntity = restTemplate.postForEntity(chatbotUrl, messageDto, MessageResponse.class); } catch (Exception e) { log.info(\"Session ID: \" + session.getId()); log.error(\"Happened error\", e); session.sendMessage(new TextMessage(mapper.writeValueAsString(MessageDto.builder().message(\"Maaf bot tidak tersedia saat ini.\").build()))); return; } MessageResponse messageResponse = responseEntity.getBody(); if (Objects.requireNonNull(messageResponse).getType().equalsIgnoreCase(\"riddles\")) { RiddlesDto dto = mapper.convertValue(messageResponse.getData(), RiddlesDto.class); log.info(\"RiddlesDto:: \" + dto); TextMessage question = new TextMessage(mapper.writeValu","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/:0:3","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project at https://github.com/piinalpin/springboot-websocket-chatbot.git ","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/:0:4","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Reference Wikipedia - WebSocket Youtube - Spring Boot + Angular 8 + WebSocket Example Tutorial ","date":"2021-10-14","objectID":"/2021/10/spring-boot-web-socket-chat-bot/:0:5","tags":["Java","Spring Boot","Spring Boot Web Socket","Javascript","Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Spring Boot Web Socket Chat Bot","uri":"/2021/10/spring-boot-web-socket-chat-bot/"},{"categories":["Documentation"],"content":"Build contextual chat bot with python and tensorflow library to talking about riddles. We will build a model for a puzzle conversation.","date":"2021-10-14","objectID":"/2021/10/contextual-chat-bot/","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Contextual Chat Bot using NLP","uri":"/2021/10/contextual-chat-bot/"},{"categories":["Documentation"],"content":"Overview Contextual chat bots are bots that can chat with humans using everyday language that is appropriate to the topic humans are talking about. We will build a model for a puzzle conversation. ","date":"2021-10-14","objectID":"/2021/10/contextual-chat-bot/:0:1","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Contextual Chat Bot using NLP","uri":"/2021/10/contextual-chat-bot/"},{"categories":["Documentation"],"content":"Prerequisites Python 3.x Flask NLTK TF Learn Sastrawi Keras ","date":"2021-10-14","objectID":"/2021/10/contextual-chat-bot/:0:2","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Contextual Chat Bot using NLP","uri":"/2021/10/contextual-chat-bot/"},{"categories":["Documentation"],"content":"Implementation Transform Conversational Intent Definitions to a Tensorflow Model A chatbot framework needs a structure in which conversational intents are defined. One clean way to do this is with a JSON file, like following bellow. { \"intents\": [ { \"tag\": \"greeting\", \"patterns\": [ \"Hi\", \"Halo\", \"Hai\", \"Hello\" ], \"responses\": [ \"Hai, ada yang bisa dibantu?\", \"Halo, ada yang bisa saya bantu?\", \"Hai, saya adalah WebSocket Bot.\" ], \"context_set\": \"\" }, { \"tag\": \"thanks\", \"patterns\": [ \"Terimakasih\", \"Okay\", \"Ok\", \"Makasih\", \"Makasih ya\", \"Thanks\" ], \"responses\": [ \"Senang membantu anda.\", \"Okay, sama sama.\", \"Hubungi saya kalau anda ingin mencari referensi lagi\", \"Okay, sama sama. Jangan lupa matikan notifikasi botnya biar gak mengganggu karena masih dalam tahap pengembangan\" ] }, { \"tag\": \"kabar\", \"patterns\": [ \"Apa kabar?\", \"Bagaimana kabarnya?\", \"Gimana kabarnya?\", \"Gmn kabarnya?\" ], \"responses\": [ \"Aaa, Kabar saya baik.\", \"Luar biasa.\", \"Baik sekali hari ini\", \"Alhamdulillah, baik.\" ] }, { \"tag\": \"riddles\", \"patterns\": [ \"tebak tebakan\", \"tebak-tebakan\", \"tebakan\", \"tebak\", \"humor\", \"lelucon\", \"lucu\" ], \"responses\": [ { \"question\": \"Telor apa yang sangar?\", \"answer\": \"Telor asin, soalnya ada tatonya.\" }, { \"question\": \"Masak apa yang ragu-ragu?\", \"answer\": \"Masak iya sih?\" }, { \"question\": \"Hewan apa yang paling kurang ajar?\", \"answer\": \"Kutu rambut, soalnya kepala orang diijak-injak.\" }, { \"question\": \"Benda mana yang lebih berat: kapas 10 kilo atau besi 10 kilo?\", \"answer\": \"Sama beratnya karena keduanya sama-sama 10 kilo.\" }, { \"question\": \"Apa yang mempunyai 12 kaki dan bisa terbang?\", \"answer\": \"6 ekor burung.\" }, { \"question\": \"Apa yang ada di ujung langit?\", \"answer\": \"Huruf T\" }, { \"question\": \"Tamunya sudah masuk, tapi yang punya malah keluar. Apakah itu?\", \"answer\": \"Tukang becak lagi narik penumpang.\" }, { \"question\": \"Ikan apa yang bisa terbang hayo?\", \"answer\": \"Lele-lawar.\" }, { \"question\": \"Hewan apa ya yang banyak keahlian?\", \"answer\": \"Kukang. Ada kukang tambal ban, kukang servis motor, kukang ngomporin temen juga ada. Ups.\" }, { \"question\": \"Sayur apa yang paling dingin?\", \"answer\": \"Kembang cold.\" }, { \"question\": \"Sayur, sayur apa yang kasihan?\", \"answer\": \"Di-rebung semut.\" }, { \"question\": \"Kenapa sapi bisa jalan sendiri?\", \"answer\": \"Karena ada huruf i. Coba kalau diganti huruf u, bakal seram deh kalau gerak sendiri.\" }, { \"question\": \"Kebo apa yang bikin kita lelah?\", \"answer\": \"Kebogor jalan kaki.\" }, { \"question\": \"Sepatu, sepatu apa yang bisa di pakai masak?\", \"answer\": \"Sepatula (Spatula).\" }, { \"question\": \"Telor, telor apa yang diinjak nggak pecah?\", \"answer\": \"Telortoar.\" }, { \"question\": \"Sayur apa yang pintar nyanyi?\", \"answer\": \"Kolplay.\" }, { \"question\": \"Bisnis apa yang sangat terkenal?\", \"answer\": \"Bisnispears (Britney Spears).\" }, { \"question\": \"Sayur, sayur apa yang bersinar?\", \"answer\": \"Habis gelap, terbitlah terong.\" }, { \"question\": \"Mobil tabrakan di jalan tol, yang turun apanya dulu?\", \"answer\": \"Speedometer.\" } ], \"context_set\": \"\" } ] } Tensorflow Engine First we take care our imports and global variables. import json import nltk import pickle import random import tflearn import numpy as np import tensorflow as tf from Sastrawi.Stemmer.StemmerFactory import StemmerFactory stemmer = StemmerFactory().create_stemmer() nltk.download('punkt') tf.disable_v2_behavior() tf.disable_eager_execution() Create file tensor_flow.py for our tensorflow engine. And first fill like below to create constructor. class TensorFlow(object): def __init__(self, intents): self.classes = list() self.documents = list() self.ERROR_THRESHOLD = 0.25 self.ignore_words = [\"?\"] self.intents = json.load(open(intents)) self.output = list self.training = list() self.train_x = None self.train_y = None self.words = list() for intent in self.intents[\"intents\"]: for pattern in intent[\"patterns\"]: w = nltk.word_tokenize(pattern) self.words.extend(w) self.documents.append((w, intent['tag'])) if intent['tag'] not in self.cl","date":"2021-10-14","objectID":"/2021/10/contextual-chat-bot/:0:3","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Contextual Chat Bot using NLP","uri":"/2021/10/contextual-chat-bot/"},{"categories":["Documentation"],"content":"References Contextual Chatbots with Tensorflow What Are Contextual Chatbots? How They Can Make A World Of Difference In User Experience? ","date":"2021-10-14","objectID":"/2021/10/contextual-chat-bot/:0:4","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow"],"title":"Contextual Chat Bot using NLP","uri":"/2021/10/contextual-chat-bot/"},{"categories":["Documentation"],"content":"Bash is a command processor that typically runs in a text window where the user types commands that cause actions. Bash can also read and execute commands from a file, called a shell script.","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"What is Bash? Bash is a command processor that typically runs in a text window where the user types commands that cause actions. Bash can also read and execute commands from a file, called a shell script. Like most Unix shells, it supports filename globbing (wildcard matching), piping, here documents, command substitution, variables, and control structures for condition-testing and iteration. The keywords, syntax, dynamically scoped variables and other basic features of the language are all copied from sh. Other features, e.g., history, are copied from csh and ksh. Bash is a POSIX-compliant shell, but with a number of extensions. ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:1","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"Basic Command Let’s start learn basic command line interface. ls show list content, example ls Documents cd change directory, example cd Documents mv rename or move file, example mv foo.txt Documents/bar.txt mkdir create a new directory, example mkdir cloudjumper touch create a new file, example touch foo.txt rm remove file or directory, example rm foo.txt or rm -f cloudjumper clear clear command line screen cp copy file, example cp foo.txt Documents cat display content of a file to the screen, example cat foo.txt chown change owner file, example chown foo.txt chmod change file permission, example chmod 777 foo.txt or chmod +x foo.txt sudo perform task to root permission grep search file or output particular string or expression, example grep ssh foo.txt etc. ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:2","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"Why use Bash Script? Shell script or bash script can be used to : Eliminate repetitive tasks. Saving time. Presents a structured, modular, and formatted sequence of activities. With bash functions, you can supply dynamic values to commands by using command line arguments. Simplify complex commands into one active, executable command. Used as often as possible by users. One bash function for multiple uses. Create a logical flow. Used at the start of the server (server start-up) or by adding a scheduled cron job. Debug command. Create interactive shell commands. ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:3","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"Using Bash Create file hello.sh and change permission to 777 or +x to make it executable. Every bash script must start with the following line : #!/bin/bash Hello World #!/bin/bash echo \"Hello world!!!\" or with function #!/bin/bash hello() { echo \"Hello world!!!\" } hello Run file with command ./hello.sh and we got an output Hello world!!! Conditions If Conditions #!/bin/bash var=true if [ $var == true ]; then echo \"var value is: $var\" fi Run file with command ./hello.sh and we got an output var value is: true Case Conditions #!/bin/bash var=1 case $var in 1) echo \"var value is $var\" ;; *) echo \"Invalid value\" ;; esac Run file with command ./hello.sh and we got an output var value is: true Looping While Do #!/bin/bash isvalid=true count=1 while [ $isvalid ]; do echo $count if [ $count -eq 5 ]; then break fi ((count++)) done For #!/bin/bash for (( count=10; count\u003e0; count-- )); do echo -n \"$count \" done ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:4","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"My Bash Script Usage I use bash script to saving time and eliminate repetitive task. So, I can be more productive for lazy people like me. Push into Git #!/bin/bash # Pull latest code git pull # Add to stagged file git add -A # Create a new commit message msg=\"rebuilding site `date`\" if [ $# -eq 1 ] then msg=\"$1\" fi git commit -m \"$msg\" # Push or upload to Github git push origin master Run this file with command ./deploy.sh -m \"Your message\" Running Docker Compose Command I use shell script to simplified task. For example, I will run RabbitMQ as a container using docker with volume mount. First, I will download docker compose configuration file in here. So, I will execute cUrl command like below. curl -o rabbitmq.yml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/rabbitmq.yaml Then, I should create a docker network. docker network create my-network Then, I should create a volume rabbitmq-data and rabbitmq-log to persist data. docker volume create rabbitmq-data docker volume create rabbitmq-log Then, I will execute rabbitmq.yaml to run RabbitMQ with docker compose docker compose -f rabbitmq.yaml up -d And to stop it, I will execute command docker compose -f rabbitmq.yaml down -v With bash script, I can simplified all task with one command line. First, I will create file rabbitmq.sh and fill code like below. #!/bin/bash FILE=rabbitmq.yaml NAME=RabbitMQ if [ $# = 1 ]; then # Check if configuration file is not exists will download configuration if [[ ! -f \"$FILE\" ]]; then curl -o $FILE https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/$FILE fi # Check network if not exists will create a docker network net=`docker network ls -q -f name=my-network` if [ -z \"$net\" ]; then net=`docker network create my-network` echo \"Create docker network: $net\" fi # Check volume is not exists will create volume rabbitmqData=`docker volume ls -q -f name=rabbitmq-data` if [ -z \"$rabbitmqData\" ]; then rabbitmqData=`docker volume create rabbitmq-data` echo \"Create docker volume: $rabbitmqData\" rabbitmqLog=`docker volume ls -q -f name=rabbitmq-log` if [ -z \"$rabbitmqLog\" ]; then rabbitmqLog=`docker volume create rabbitmq-log` echo \"Create docker volume: $rabbitmqLog\" fi fi # If command args start then run docker compose up if [ $* = \"start\" ]; then docker compose -f $FILE up -d echo \"$NAME has started.\" # Or if command args is stop then run docker compose down elif [ $* = \"stop\" ]; then docker compose -f $FILE down -v echo \"$NAME has stopped.\" else echo \"Invalid command\" exit 0 fi else echo \"Invalid command\" exit 0 fi To start RabbitMQ service by typing following command. ./rabbitmq.sh start To stop RabbitMQ service by typing following command. ./rabbitmq.sh stop ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:5","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"My Pouncher.sh I create shell script file pouncher.sh to saving my time for running docker container with docker compose like Kafka-CLI, MySQL, PostgreSQL, RabbitMQ, Redis, Sonarqube and SQLServer. I took the pouncher name from the baby dragon’s name in the How to Train Your Dragon file because it fits the character’s energetic and playful nature. Just download file from my Github here or you can use cUrl. curl -o pouncher.sh https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/sh/pouncher.sh Then make file is executable chmod +x pouncher.sh. See help command by typing ./pouncher.sh --help For example, I will run MySQL database. I just typing a command ./pouncher.sh -n mysql -c start or stop service by typing ./pouncher.sh -n mysql -c stop. See, how it saves my time. ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:6","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"Reference Wikipedia - Bash (Unix Shell) Hostinger - Petunjuk Penggunaan Bash Script ","date":"2021-08-17","objectID":"/2021/08/bash-script-for-lazy-people/:0:7","tags":["Shell Script","Shell","Bash","Bash Script","CLI","Command Line Interface"],"title":"Bash Script for Lazy People","uri":"/2021/08/bash-script-for-lazy-people/"},{"categories":["Documentation"],"content":"I created this docker configuration pool so that when I forget the configuration environment, I can reuse it.","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"Basic command docker-compose -f $FILENAME up -d docker-compose -f $FILENAME down -v ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:1","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"Kafka CLI Download docker compose configuration using curl curl -o kafka-cli.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/kafka-cli.yaml This yaml kafka-cli.yaml will create a new container zookeeper and exposed port 2181 on host port. Also create a new container kafka and exposed port 29092, 9092 and 9101 on host port. ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:2","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"Redis By default there are 16 databases (indexed from 0 to 15) and you can navigate between them using select command. Number of databases can be changed in redis config file with databases setting. Download docker compose configuration using curl curl -o redis.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/redis.yaml This yaml redis.yaml will create a new container redis and exposed port 6379 on host port. Redis CLI Run redis-cli docker exec -it redis redis-cli Select the Redis logical database having the specified zero-based numeric index. New connections always use the database 0. Change database. select [INDEX] Set and get key set [KEY_NAME] [VALUE] get [KEY_NAME] Another Redis Desktop Manager Install Another Redis Desktop Manager with homebrew. You can run with following command on your terminal. brew install cask brew install --cask another-redis-desktop-manager Open the application, and should looks like below. Another Redis Desktop Manager dashboard Another Redis Desktop Manager add new key. ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:3","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"MySQL Download docker compose configuration using curl curl -o mysql.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/mysql.yaml This yaml mysql.yaml will create a new container mysql and exposed port 3306 on host port. Create network my-network if does not exists docker network create my-network Create volume mysql-data to persist data then run docker-compose docker volume create mysql-data MySQL CLI Run this command to run MySQL command on container. Default user is root and password SevenEightTwo782 you can change the password in yaml file. docker exec -it mysql bash Basic command: Login into database, then type your password mysql -u root -p Get list of databases show databases; Create database and use database create database $DB_NAME; use $DB_NAME; ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:4","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"PostgreSQL Download docker compose configuration using curl curl -o postgresql.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/postgresql.yaml This yaml postgresql.yaml will create a new container postgresql and exposed port 5432 on host port. Create network my-network if does not exists docker network create my-network Create volume postgre-data to persist data then run docker-compose docker volume create postgre-data Postgre SQL CLI Run this command to run PostgreSQL command on container. Default user is postgres and password SevenEightTwo782 you can change the password in yaml file. docker exec -it postgresql bash Basic command: Login into database psql -Upostgres -w Get list of databases \\l Create database and use database create database $DB_NAME; \\c $DB_NAME ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:5","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"SQL Server Download docker compose configuration using curl curl -o sqlserver.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/sqlserver.yaml This yaml sqlserver.yaml will create a new container sqlserver and exposed port 1433 on host port. Create network my-network if does not exists docker network create my-network Create volume sqlserver-data and sqlserver-user to persist data then run docker-compose docker volume create sqlserver-data \u0026\u0026 docker volume create sqlserver-user SQL Server CLI Run this command to run SQL Server command on container. Default user is sa and password SevenEightTwo782 you can change the password in yaml file. docker exec -it sqlserver /opt/mssql-tools/bin/sqlcmd -U sa -P SevenEightTwo782 Basic command: Get list of databases select name from sys.databases go Create database and use database create database $DB_NAME go ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:6","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"RabbitMQ Download docker compose configuration using curl curl -o rabbitmq.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/rabbitmq.yaml This yaml rabbitmq.yaml will create a new container rabbitmq and exposed port 5672 and 15672 on host port. Create network my-network if does not exists docker network create my-network Create volume rabbitmq-data and rabbitmq-log to persist data then run docker-compose docker volume create rabbitmq-data \u0026\u0026 docker volume create rabbitmq-log RabbitMq Management Default RabbitMQ management user is guest and password is guest. Go to localhost:15672 to acess RabbitMQ management. ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:7","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"Sonarqube Download docker compose configuration using curl curl -o sonarqube.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/sonarqube.yaml This yaml sonarqube.yaml will create a new container sonarqube and exposed port 9000 and 9002 on host port. Create network my-network if does not exists docker network create my-network Create volume to persist data then run docker-compose docker volume create sonarqube-data \u0026\u0026 docker volume create sonarqube-extensions \u0026\u0026 docker volume create sonarqube-logs \u0026\u0026 docker volume create sonarqube-temp Sonarqube Management Default Sonarqube management user is admin and password is admin. Go to localhost:9000 to acess Sonarqube management and then change the default password first. ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:8","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"MongoDB Download docker compose configuration using curl curl -o mongodb.yaml https://raw.githubusercontent.com/piinalpin/docker-compose-collection/master/mongodb.yaml This yaml mongodb.yaml will create a new container mongodb and exposed port 27017 on host port. Create network my-network if does not exists docker network create my-network Create volume to persist data then run docker-compose docker volume create mongodb-data \u0026\u0026 docker volume create mongodb-config MongoDB CLI Run this command to run MongoDB command on container. Default user is root and password SevenEightTwo782 you can change the password in yaml file. docker exec -it mongodb mongo -u root -p SevenEightTwo782 --authenticationDatabase admin \u003cSOME_DATABASE\u003e Basic command: Get list of databases show dbs Create database and use database use some_db db Create collection db.createCollection('some_collection'); Insert row db.some_db.insertMany([ { _id: 1, first_name: \"Maverick\", last_name: \"Johnson\", gender: 1 }, { _id: 2, first_name: \"Calvin\", last_name: \"Joe\", gender: 1 }, { _id: 3, first_name: \"Kagura\", last_name: \"Otsusuki\", gender: 0 } ]); Find row db.some_db.find(); Update row db.test.updateOne({_id: 1}, {$set: { first_name: \"Maverick\", last_name: \"Johnson Updated\", gender: 1 }}); ","date":"2021-08-15","objectID":"/2021/08/docker-compose-collection/:0:9","tags":["Docker","Docker Compose","Collection","YAML"],"title":"My Docker Compose Collection","uri":"/2021/08/docker-compose-collection/"},{"categories":["Documentation"],"content":"Jenkins project offers a public community-driven roadmap. It aggregates key initiatives in all areas: features, infrastructure, documentation, community, etc.","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Overview Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed (Jenkins.io). ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:1","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Prerequisites Vagrant, see how to install Vagrant here Add the following host entries on local device /etc/hosts : 192.168.0.1 jenkins.local 192.168.1.1 agent.local Startup Vagrant Create Vagrantfile and fill the following code. Vagrant.configure(2) do |config| config.vm.box = \"bento/centos-7\" config.vm.define \"jenkins\" do |jenkins| jenkins.vm.network \"private_network\", ip: \"192.168.0.1\", name: 'vboxnet0' jenkins.vm.hostname = \"jenkins.local\" jenkins.vm.network :forwarded_port, guest: 22, host: 2222, id: \"ssh\", disabled: true jenkins.vm.network :forwarded_port, guest: 22, host: 2230, auto_correct: true jenkins.vm.network :forwarded_port, guest: 8080, host: 8080, auto_correct: true jenkins.ssh.port = 2230 jenkins.vm.provider \"jenkins\" do |vb| vb.cpus = 1 vb.memory = 1024 end end config.vm.define \"agent\" do |agent| agent.vm.network \"private_network\", ip: \"192.168.1.1\", name: 'vboxnet1' agent.vm.hostname = \"agent.local\" agent.vm.network :forwarded_port, guest: 22, host: 2222, id: \"ssh\", disabled: true agent.vm.network :forwarded_port, guest: 22, host: 2231, auto_correct: true agent.ssh.port = 2231 agent.vm.provider \"agent\" do |vb| vb.cpus = 1 vb.memory = 1024 end end end And start vagrant by typing vagrant up on your terminal. ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:2","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Install Jenkins SSH into jenkins machine, you can use vagrant ssh jenkins or ssh from host ssh -p 2230 vagrant@jenkins.local -i .vagrant/machines/jenkins/virtualbox/private_key Login as root sudo su - Install wget yum install -y wget Create jenkins limit file /etc/security/limits.d/30-jenkins.conf jenkins soft core unlimited jenkins hard core unlimited jenkins soft fsize unlimited jenkins hard fsize unlimited jenkins soft nofile 4096 jenkins hard nofile 8192 jenkins soft nproc 30654 jenkins hard nproc 30654 Setup firewall systemctl start firewalld firewall-cmd --permanent --add-port=22/tcp firewall-cmd --permanent --add-port=8080/tcp firewall-cmd --reload firewall-cmd --list-all We should get an outuput public (active) target: default icmp-block-inversion: no interfaces: eth0 eth1 sources: services: dhcpv6-client ssh ports: 22/tcp 8080/tcp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: Add the following entries to /etc/hosts 192.168.0.1 jenkins.local 192.168.1.1 agent.local Add AdoptOpenJDK repository cat \u003c\u003c'EOF' \u003e /etc/yum.repos.d/adoptopenjdk.repo [AdoptOpenJDK] name=AdoptOpenJDK baseurl=http://adoptopenjdk.jfrog.io/adoptopenjdk/rpm/centos/$releasever/$basearch enabled=1 gpgcheck=1 gpgkey=https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public EOF Add repository to get latest Git yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.9-1.x86_64.rpm Add jenkins repository wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key Create directory mkdir -p /var/cache/jenkins/tmp mkdir -p /var/cache/jenkins/heapdumps Uninstall old Git by typing yum remove git* on your terminal Install AdoptOpenJDK, Git, Jenkins and Fontconfig yum -y install adoptopenjdk-11-hotspot git jenkins fontconfig Edit the /etc/sysconfig/jenkins file JENKINS_JAVA_OPTIONS=\"-Djava.awt.headless=true -Djava.io.tmpdir=/var/cache/jenkins/tmp -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Jakarta -Duser.timezone=Asia/Jakarta\" JENKINS_ARGS=\"--pluginroot=/var/cache/jenkins/plugins\" Change owner jenkins config file chown -R jenkins:jenkins /var/cache/jenkins Start jenkins by typing systemctl start jenkins and get status systemctl -l status jenkins We should get an output status. ● jenkins.service - LSB: Jenkins Automation Server Loaded: loaded (/etc/rc.d/init.d/jenkins; bad; vendor preset: disabled) Active: active (running) since Sun 2021-08-01 01:06:57 UTC; 16s ago Docs: man:systemd-sysv-generator(8) Process: 3697 ExecStart=/etc/rc.d/init.d/jenkins start (code=exited, status=0/SUCCESS) CGroup: /system.slice/jenkins.service └─3718 /etc/alternatives/java -Dcom.sun.akuma.Daemon=daemonized -Djava.awt.headless=true -Djava.io.tmpdir=/var/cache/jenkins/tmp -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Jakarta -Duser.timezone=Asia/Jakarta -DJENKINS_HOME=/var/lib/jenkins -jar /usr/lib/jenkins/jenkins.war --logfile=/var/log/jenkins/jenkins.log --webroot=/var/cache/jenkins/war --daemon --httpPort=8080 --debug=5 --handlerCountMax=100 --handlerCountMaxIdle=20 --pluginroot=/var/cache/jenkins/plugins ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:3","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Setup Jenkins UI Access http://jenkins.local:8080 on your host and should be like below. Type sudo cat /var/lib/jenkins/secrets/initialAdminPassword to get password and paste it on field then continue. Install suggested plugin Create first admin user. Setup jenkins url and click start using jenkins. And restart jenkins then login with admin user http://jenkins.local:8080/restart ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:4","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Agent Installation SSH into jenkins machine, you can use vagrant ssh agent or ssh from host ssh -p 2231 vagrant@agent.local -i .vagrant/machines/agent/virtualbox/private_key Login as root sudo su - Setup firewall systemctl start firewalld firewall-cmd --permanent --add-port=22/tcp firewall-cmd --reload firewall-cmd --list-all We should get an outuput public (active) target: default icmp-block-inversion: no interfaces: eth0 eth1 sources: services: dhcpv6-client ssh ports: 22/tcp 8080/tcp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: Add the following entries to /etc/hosts 192.168.0.1 jenkins.local 192.168.1.1 agent.local Add AdoptOpenJDK repository cat \u003c\u003c'EOF' \u003e /etc/yum.repos.d/adoptopenjdk.repo [AdoptOpenJDK] name=AdoptOpenJDK baseurl=http://adoptopenjdk.jfrog.io/adoptopenjdk/rpm/centos/$releasever/$basearch enabled=1 gpgcheck=1 gpgkey=https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public EOF Add repository to get latest Git yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.9-1.x86_64.rpm Uninstall old Git by typing yum remove git* on your terminal Install AdoptOpenJDK, Git, Fontconfig and Wget yum -y install adoptopenjdk-11-hotspot git fontconfig wget Install docker and unzip from Install Docker Engine on CentOS Remove old docker if any yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine yum -y install yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce docker-ce-cli containerd.io unzip systemctl enable docker systemctl start docker groupadd docker systemctl -l status docker exit sudo usermod -aG docker $USER exit SSH again to vagrant agent ssh -p 2231 vagrant@agent.local -i .vagrant/machines/agent/virtualbox/private_key Try running container by typing docker run hello-world on your terminal. Installing maven sudo su - mkdir -p /opt/tools/maven cd /opt/tools/maven wget https://downloads.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar zxvf apache-maven-3.6.3-bin.tar.gz rm -f apache-maven-3.6.3-bin.tar.gz ln -s apache-maven-3.6.3 latest Installing gradle mkdir -p /opt/tools/gradle cd /opt/tools/gradle wget https://services.gradle.org/distributions/gradle-7.1.1-bin.zip unzip gradle-7.1.1-bin.zip rm -f gradle-7.1.1-bin.zip ln -s gradle-7.1.1 latest Export maven and gradle to profile echo \"PATH=/opt/tools/gradle/latest/bin:\\$PATH\" \u003e /etc/profile.d/gradle.sh echo \"PATH=/opt/tools/maven/latest/bin:\\$PATH\" \u003e /etc/profile.d/maven.sh chown -R vagrant:vagrant /opt/tools exit exit Verify that maven and gradle has already installed on your machine. ssh -p 2231 vagrant@agent.local -i .vagrant/machines/agent/virtualbox/private_key mvn --version gradle --version ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:5","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Connect Agent to Jenkins Manage jenkins at system configuration. And fill field like below Add nodes and fill like below Add credential Click advanced and set port to 2231 then click save. ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:6","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Create Test Job (Pipeline) Create new pipeline then save Create build script to running jobs. pipeline { agent {label \"linux\"} stages { stage(\"Hello\") { steps { sh \"\"\" mvn --version gradle --version docker info \"\"\" } } } } You can see my example on Github ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:7","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"References Jenkins.io - Jenkins User Documentation CloudBeesTV - How To Install Jenkins on CentOS 7 RedHat - Continuous Delivery to JBoss EAP and OpenShift with the CloudBees Jenkins Platform CloudBees - Prepare Jenkins for Support Jenkins - Jenkins Redhat Packages Docker - Install Docker Engine on CentOS Docker - Post-installation steps for Linux AdoptOpenJDK - Installation ","date":"2021-08-01","objectID":"/2021/08/install-jenkins-on-centos7/:0:8","tags":["Jenkins","DevOps","CI/CD","Continous Integration","Centos7","Centos"],"title":"Install Jenkins on Centos7","uri":"/2021/08/install-jenkins-on-centos7/"},{"categories":["Documentation"],"content":"Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs.","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Overview Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs. Designed for multi-tier deployments since day one, Ansible models your IT infrastructure by describing how all of your systems inter-relate, rather than just managing one system at a time. It uses no agents and no additional custom security infrastructure, so it’s easy to deploy - and most importantly, it uses a very simple language (YAML, in the form of Ansible Playbooks) that allow you to describe your automation jobs in a way that approaches plain English. ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:1","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Prerequisites Installing Virtual Box Install virtual box from Downloads - Oracle VM Virtualbox or with Homebrew. brew install virtualbox Installing Vagrant Vagrant is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the “works on my machine” excuse a relic of the past. We will use vagrant for virtualization server on our machine. Install vagrant with homebrew. brew install vagrant Setup Host Only Adapter Open virtualbox and go to File-\u003eHost Network Adapter and setup private network. Installing Ansible Install ansible using Homebrew. brew install ansible ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:2","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Create Virtual Machine Create Vagrantfile and fill like following code. We will use Centos 8 on our virtual machine. Vagrant.configure(2) do |config| config.vm.box = \"bento/centos-8\" config.vm.define \"server1\" do |server1| server1.vm.network \"private_network\", ip: \"192.168.0.1\", name: 'vboxnet0' server1.vm.hostname = \"server1.local\" server1.vm.network :forwarded_port, guest: 22, host: 2222, id: \"ssh\", disabled: true server1.vm.network :forwarded_port, guest: 22, host: 2230, auto_correct: true server1.ssh.port = 2230 server1.vm.provider \"server1\" do |vb| vb.cpus = 1 vb.memory = 1024 end end config.vm.define \"server2\" do |server2| server2.vm.network \"private_network\", ip: \"192.168.1.1\", name: 'vboxnet1' server2.vm.hostname = \"server2.local\" server2.vm.network :forwarded_port, guest: 22, host: 2222, id: \"ssh\", disabled: true server2.vm.network :forwarded_port, guest: 22, host: 2231, auto_correct: true server2.ssh.port = 2231 server2.vm.provider \"server2\" do |vb| vb.cpus = 1 vb.memory = 1024 end end end Running up the machine with typing vagrant up on terminal. And if all machine are running, we can check the status on our machines with vagrant status on terminal. And we get an output like below. Current machine states: server1 running (virtualbox) server2 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`. ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:3","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Add Ansible Inventory learn-ansible/ └───inventory/ │ │ vagrant.hosts └───Vagrantfile Create file in inventory/vagrant.hosts to register server host to ansible hosts and fill like following code. [all:vars] ansible_ssh_common_args='-o StrictHostKeyChecking=no' [linux] server1 ansible_ssh_host=192.168.0.1 ansible_ssh_port=2230 ansible_user=vagrant ansible_ssh_private_key_file=.vagrant/machines/server1/virtualbox/private_key server2 ansible_ssh_host=192.168.1.1 ansible_ssh_port=2231 ansible_user=vagrant ansible_ssh_private_key_file=.vagrant/machines/server2/virtualbox/private_key ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:4","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Ansible Simple Usage Ping The Servers Test to ping the linux server on ansible hosts by command ansible -i inventory/vagrant.hosts linux -m ping . And we should get an output. If we get SUCCESS response, that means the ping was successful and we can access our servers from ansible. server2 | SUCCESS =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/libexec/platform-python\" }, \"changed\": false, \"ping\": \"pong\" } server1 | SUCCESS =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/libexec/platform-python\" }, \"changed\": false, \"ping\": \"pong\" } Run Linux Command from Ansible We will run a linux command to see the OS Release on our virtual machine using ansible with ansible -i inventory/vagrant.hosts linux -a \"cat /etc/os-release\" and we should get and output. server1 | CHANGED | rc=0 \u003e\u003e NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" server2 | CHANGED | rc=0 \u003e\u003e NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" So, we know that both of our servers use the CentOS-8 operating system. ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:5","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Ansible Playbook An Ansible playbook is an organized unit of scripts that defines work for a server configuration managed by the automation tool Ansible and create by YAML file. --- - name: iloveansible hosts: server1 become: yes tasks: - name: Ensure nano is there yum: name: nano state: latest That yaml file means : Playbook have a PLAY with name iloveansible Running at host server1 and become a root access Run tasks Ensure nano is there will running command sudo yum install nano So, the structure directory should be like below. learn-ansible/ └───inventory/ │ │ vagrant.hosts └───Vagrantfile └───ansible-playbook.yml Running playbook by typing ansible-playbook -i inventory/vagrant.hosts ansible-playbook.yml on terminal. And we get an output. PLAY [iloveansible] ************************************************************************************************************************************************************************************************************************ TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************************* ok: [server1] TASK [Ensure nano is there] **************************************************************************************************************************************************************************************************************** changed: [server1] PLAY RECAP ********************************************************************************************************************************************************************************************************************************* server1 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 See on PLAY RECAP there is an status changed=1 that mean, server1 has been changed because just installed nano on server1. Because a playbook just running on server1 that means on server2 doesn’t have a nano. So what if we install nano on all server group? Here is a yaml file. Change hosts to linux. --- - name: iloveansible hosts: linux become: yes tasks: - name: Ensure nano is there yum: name: nano state: latest Running playbook by typing ansible-playbook -i inventory/vagrant.hosts ansible-playbook.yml on terminal. And we get an output. PLAY [iloveansible] ************************************************************************************************************************************************************************************************************************ TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************************* ok: [server1] ok: [server2] TASK [Ensure nano is there] **************************************************************************************************************************************************************************************************************** ok: [server1] changed: [server2] PLAY RECAP ********************************************************************************************************************************************************************************************************************************* server1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 server2 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 See on PLAY RECAP there is an status changed=0 on server1 that mean, server1 not changed because a nano have installed on server1. And then there is an status changed=1 on server2 that mean, server2 has been changed because just installed nano on server2. Let’s go inside our server and check the nano have installed using ssh on server1 by typing ssh -p 2230 vagrant@192.168.0.1 -i .vagrant/machines/server1/virtualbox/private_key on terminal. Then typing sudo yum install nano o","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:6","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"References RedHat Ansible - Overview How Ansible Works HashiCorp - Introduction to Vagrant HVOPS - Install Ansible on Mac OSX NetworkChuck - you need to learn Ansible RIGHT NOW!! (Linux Automation) TechTarget - Ansible playbook ","date":"2021-07-31","objectID":"/2021/07/why-should-learn-ansible/:0:7","tags":["DevOps","Cloud Computing","Ansible","Ansible Playbook"],"title":"Why Should Learn Ansible - Linux Automation","uri":"/2021/07/why-should-learn-ansible/"},{"categories":["Documentation"],"content":"Apache Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Overview Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments. Servers: Kafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss. Clients: They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka ships with some such clients included, which are augmented by dozens of clients provided by the Kafka community: clients are available for Java and Scala including the higher-level Kafka Streams library, for Go, Python, C/C++, and many other programming languages as well as REST APIs. ","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:1","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Prerequisites Requirement pre-installed docker and docker-compose We use kafka server using docker-compose like following below, or you can use from original docs on Quick Start for Apache Kafka using Confluent Platform (Docker) --- version: '3.8' services: zookeeper: image: confluentinc/cp-zookeeper:6.2.0 hostname: zookeeper container_name: zookeeper ports: - \"2181:2181\" environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 kafka: image: confluentinc/cp-kafka:6.2.0 hostname: kafka container_name: kafka depends_on: - zookeeper ports: - \"29092:29092\" - \"9092:9092\" - \"9101:9101\" restart: always environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 KAFKA_JMX_PORT: 9101 KAFKA_JMX_HOSTNAME: localhost And run on terminal with following command docker-compose up -d For stopping server with following command docker-compose down -v ","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:2","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Project Setup and Dependencies I’m depending Spring Initializr for this as it is much easier. And we have to create two spring boot projects and started with maven project. Our example application will be a Spring Boot application. So we need to add spring-kafka and spring-boot-starter-web dependency to our pom.xml. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.kafka\u003c/groupId\u003e \u003cartifactId\u003espring-kafka\u003c/artifactId\u003e \u003cversion\u003e2.7.4\u003c/version\u003e \u003c/dependency\u003e We also need to create application.yml for configuration file. kafka: bootstrapAddress: \"http://localhost:9092\" server: port: 8080 ","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:3","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Implementation Configuring Topics Create constant com.piinalpin.kafkademo.constant.KafkaTopicConstant. public class KafkaTopicConstant { public final static String HELLO_WORLD = \"hello-world\"; } Create bean configuration com.piinalpin.kafkademo.config.KafkaTopicConfiguration to define topics on Kafka. @Configuration public class KafkaTopicConfiguration { @Value(\"${kafka.bootstrapAddress:}\") private String bootstrapAddress; @Bean public KafkaAdmin kafkaAdmin() { Map\u003cString, Object\u003e config = new HashMap\u003c\u003e(); config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); return new KafkaAdmin(config); } @Bean public NewTopic helloWorld() { return new NewTopic(KafkaTopicConstant.HELLO_WORLD, 0, (short) 1); } } Producer Configuration Create bean configuration com.piinalpin.kafkademo.config.KafkaProducerConfiguration to define producer configuration bean on Kafka. @Configuration public class KafkaProducerConfiguration { @Value(\"${kafka.bootstrapAddress:}\") private String bootstrapAddress; @Bean public ProducerFactory\u003cString, String\u003e producerFactory() { Map\u003cString, Object\u003e configProps = new HashMap\u003c\u003e(); configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return new DefaultKafkaProducerFactory\u003c\u003e(configProps); } @Bean public KafkaTemplate\u003cString, String\u003e kafkaTemplate() { return new KafkaTemplate\u003c\u003e(producerFactory()); } } Publisher Service Create service com.piinalpin.kafkademo.service.KafkaPublisherService to publish a message. @Service public class KafkaPublisherService { public final static Logger log = LoggerFactory.getLogger(KafkaProducerService.class); @Autowired private KafkaTemplate\u003cString, String\u003e kafkaTemplate; public void send(String message) { log.info(\"Sending message to Kafka...\"); log.info(String.format(\"Payload: %s, Topic: %s\", KafkaTopicConstant.HELLO_WORLD, message)); kafkaTemplate.send(KafkaTopicConstant.HELLO_WORLD, message); } } Send Message Create rest controller com.piinalpin.kafkademo.controller.KafkaDemoController to send a message via rest. @RestController public class KafkaDemoController { @Autowired private KafkaPublisherService kafkaPublisherService; @GetMapping(value = \"/\") public Map\u003cString, String\u003e main() { return okMessage(\"ok\"); } @PostMapping(value = \"/greeting\") public Map\u003cString, String\u003e greeting(@RequestBody Map\u003cString, String\u003e request) { if (null != request.get(\"greeting\")) { kafkaPublisherService.send(request.get(\"greeting\")); } return okMessage(\"Sending message...\"); } private Map\u003cString, String\u003e okMessage(String message) { Map\u003cString, String\u003e ret = new HashMap\u003c\u003e(); ret.put(\"message\", message); return ret; } } Try to run by typing mvn spring-boot:run then open Postman like below. URL: http://localhost:8080/greeting (POST) Request Body { \"greeting\": \"Hello my name is Maverick\" } And log will display like below. 2021-07-23 19:27:00.306 INFO 87879 --- [nio-8080-exec-4] c.p.k.service.KafkaProducerService : Sending message to Kafka... 2021-07-23 19:27:00.307 INFO 87879 --- [nio-8080-exec-4] c.p.k.service.KafkaProducerService : Payload: hello-world, Topic: Hello my name is Maverick Consumer Configuration For consuming messages, we need to configure a ConsumerFactory and a KafkaListenerContainerFactory. Once these beans are available in the Spring bean factory, POJO-based consumers can be configured using @KafkaListener annotation. @EnableKafka annotation is required on the configuration class to enable detection of @KafkaListener annotation on spring-managed beans: Create bean configuration com.piinalpin.kafkademo.config.KafkaConsumerConfiguration to define consumer or listener configuration bean on Kafka. @Configuration @EnableKafka public class KafkaConsumerConfiguration { @Value(\"${kafka.bootstrapAddress:}\") private String bootstrapAddress; public ConsumerFactory\u003cString, String\u003e consumerFactory(","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:4","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project https://github.com/piinalpin/kafka-demo.git ","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:5","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Thankyou Baeldung - Intro to Apache Kafka with Spring Medium - Apache Kafka CLI commands cheat sheet Github - Spring Kafka Stack Overflow - How to set groupId to null in @KafkaListeners Programmer Sought - Springboot integrated kafka-No group.id found in consumer config ","date":"2021-07-23","objectID":"/2021/07/messaging-on-kafka-with-spring-boot/:0:6","tags":["Java","Spring Boot","Kafka","Apache Kafka","Message Broker"],"title":"Messaging on Kafka-CLI With Spring Boot","uri":"/2021/07/messaging-on-kafka-with-spring-boot/"},{"categories":["Documentation"],"content":"Spring Cloud Config is Spring's client/server approach for storing and serving distributed configurations across multiple applications and environments","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"What is Configuration as a Service In micro-service world, managing configurations of each service separately is a tedious and time-consuming task. In other words, if there are many number of modules, and managing properties for each module with the traditional approach is very difficult. Central configuration server provides configurations (properties) to each micro service connected. As mentioned in the above diagram, Spring Cloud Config Server can be used as a central cloud config server by integrating to several environments. Environment Repository — Spring uses environment repositories to store the configuration data. it supports various of authentication mechanisms to protect the configuration data when retrieving. Spring Cloud Config is Spring’s client/server approach for storing and serving distributed configurations across multiple applications and environments. This configuration store is ideally versioned under Git version control and can be modified at application runtime. While it fits very well in Spring applications using all the supported configuration file formats together with constructs like Environment, PropertySource or @Value, it can be used in any environment running any programming language. In this write-up, we’ll focus on an example of how to setup a Git-backed config server, use it in a simple REST application server and setup a secure environment including encrypted property values. ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:1","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Project Setup and Dependencies I’m depending Spring Initializr for this as it is much easier. And we have to create two spring boot projects and started with maven project. config-server config-client To get ready for writing some code, we create two new Maven projects first. The server project is relying on the spring-cloud-config-server module, as well as the spring-boot-starter-security and spring-boot-starter-web starter bundles. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-security\u003c/artifactId\u003e \u003cversion\u003e2.5.2\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-config-server\u003c/artifactId\u003e \u003cversion\u003e3.0.4\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e However for the client project we’re going to only need the spring-cloud-starter-config, spring-cloud-starter-bootstrap and the spring-boot-starter-web modules. \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-config\u003c/artifactId\u003e \u003cversion\u003e3.0.4\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-bootstrap\u003c/artifactId\u003e \u003cversion\u003e3.0.3\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:2","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Config Server Implementation The main part of the application is a config class – more specifically a @SpringBootApplication – which pulls in all the required setup through the auto-configure annotation @EnableConfigServer. @SpringBootApplication @EnableConfigServer public class ConfigServerApplication { public static void main(String[] args) { SpringApplication.run(ConfigServerApplication.class, args); } } We also need to set a username and a password for the Basic-Authentication in our application.properties to avoid an auto-generated password on every application restart. spring.cloud.config.server.git.uri=\u003cCHANGE YOUR GITHUB REPOSITORY\u003e spring.cloud.config.server.git.clone-on-start=true spring.security.user.name=root spring.security.user.password=secret spring.application.name=config-server server.port=8080 ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:3","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Git Repository as Configuration Storage To complete our server, we have to initialize a Git repository under the configured url, create some new properties files and popularize them with some values. We will use a configuration yaml file lika a normal Spring application.yml, but instead of the word ‘application’ a configured name, e.g. the value of the property ‘spring.application.name’ of the client is used, followed by a dash and the active profile. We will create 3 yaml files of configuration. config-client-local.yml config-client-development.yml config-client-production.yml config-client-local.yml app: name: \"spring-cloud-config-client.local\" website: \"http://piinalpin.com\" config: profile: \"local\" server: port: 8081 config-client-development.yml app: name: \"spring-cloud-config-client.dev\" website: \"http://piinalpin.com\" config: profile: \"development\" server: port: 7001 config-client-production.yml app: name: \"spring-cloud-config-client.prod\" website: \"http://piinalpin.com\" config: profile: \"production\" server: port: 9001 ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:4","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Querying the Configuration Now we’re able to start our server. The Git-backed configuration API provided by our server can be queried using the following paths. /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties In which the {label} placeholder refers to a Git branch, {application} to the client’s application name and the {profile} to the client’s current active application profile. So we can retrieve the configuration for our planned config client running under local profile in branch master via. curl http://root:secret@localhost:8080/config-client/local/master Then we got a response like below. { \"name\": \"config-client\", \"profiles\": [ \"local\" ], \"label\": \"master\", \"version\": \"de4a15d886d53050ec4bbe80e939106259614803\", \"state\": null, \"propertySources\": [ { \"name\": \"https://github.com/piinalpin/spring-cloud-config.git/config-client-local.yml\", \"source\": { \"app.name\": \"spring-cloud-config-client.local\", \"app.website\": \"http://piinalpin.com\", \"config.profile\": \"local\", \"server.port\": 8081 } } ] } ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:5","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Client Implementation Next, let’s take care of the client. This will be a very simple client application, consisting of a REST controller with one GET method. Now, We will create a controller in com.maverick.configclient.controller.ConfigClientController like below. @RestController public class ConfigClientController { @Value(\"${config.profile:}\") private String profile; @Value(\"${app.name:}\") private String appName; @GetMapping(path = \"/\") public Map\u003cString, String\u003e main() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); map.put(\"activeProfile\", profile); map.put(\"appName\", appName); return map; } } The configuration, to fetch our server, must be placed in a resource file named bootstrap.application, because this file (like the name implies) will be loaded very early while the application starts. spring.application.name=config-client spring.cloud.config.uri=http://localhost:8080 spring.cloud.config.username=root spring.cloud.config.password=secret spring.profiles.active=local To test, if the configuration is properly received from our server and the role value gets injected in our controller method, we simply curl it after booting the client. curl http://localhost:\u003cPORT_ACTIVE_PROFILE\u003e If the response is as follows, our Spring Cloud Config Server and its client are working fine for now. {\"activeProfile\":\"local\",\"appName\":\"spring-cloud-config-client.local\"} ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:6","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project https://github.com/piinalpin/spring-cloud-config.git ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:7","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Thankyou Medium - Microservices Centralized Configuration with Spring Cloud Baeldung - Quick Intro to Spring Cloud Configuration Github - Servidor Configuraciones en Spring Cloud ","date":"2021-07-19","objectID":"/2021/07/centralized-configuration-with-spring-cloud-config/:0:8","tags":["Java","Spring Boot","Spring Cloud Config","Spring Boot Configuration","Spring Cloud Config"],"title":"Centralized Configuration with Spring Cloud Config","uri":"/2021/07/centralized-configuration-with-spring-cloud-config/"},{"categories":["Documentation"],"content":"Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"What is Persistent Volume and Persistent Volume Claim A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system. A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes). While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource. More detail please see Configure a Pod to Use a PersistentVolume for Storage ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:1","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Create a Perstistent Volume In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage. In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning. Create a pv-example.yaml configuration file for the hostPath PersistentVolume: apiVersion: v1 kind: PersistentVolume metadata: name: pv-example labels: app: example spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/var/lib/data\" kubectl create persistent volume pv-example with labels app:example kubectl defines the storage class name manual kubetcl specifies a size of 10 gibibytes and access modes is ReadWriteOnce which means the volume can be mounted as read-write by a single node. kubectl specifies that the volume is at /var/lib/data on the cluster node. Create a persistent volume kubectl apply -f pv-example.yaml Get PersistentVolume was created kubectl get pv pv-example We should get an output NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-example 10Gi RWO Retain Available manual 7s ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:2","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Create a Persistent Volume Claim Pods use PersistentVolumeClaims to request physical storage. In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node. Create a pvc-example.yaml configuration file and fill with following code : apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-claim-example labels: app: example spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Create a persistent volume claim kubectl apply -f pvc-example.yaml Get PersistentVolume was created kubectl get pvc pv-claim-example We should get an output NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pv-claim-example Bound pv-example 10Gi RWO manual 12s Check if persistent volume was claimed kubectl get pv pv-example We should get an output shows a STATUS of Bound and CLAIM of default/pv-claim-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-example 10Gi RWO Retain Bound default/pv-claim-example manual 9m21s ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:3","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Create a Pod Create a pod pod-uses-pvc-example.yaml configuration file that uses the PersistentVolumeClaim as a volume apiVersion: v1 kind: Pod metadata: name: pv-example-pod spec: volumes: - name: pv-storage-example persistentVolumeClaim: claimName: pv-claim-example containers: - name: pv-example-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: pv-storage-example Create the pod kubectl apply -f pod-uses-pvc-example.yaml Check pod is running kubectl get pods pv-example-pod Get cluster where pods is running kubectl describe pods pv-example-pod We should get an output Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned default/pv-example-pod to gke-rattlesnake-cluster-default-pool-5da13591-6wb4 Normal Pulling 10m kubelet Pulling image \"nginx\" Normal Pulled 10m kubelet Successfully pulled image \"nginx\" in 2.156978426s Normal Created 10m kubelet Created container pv-example-container Normal Started 10m kubelet Started container pv-example-container Get external IP for gke-rattlesnake-cluster-default-pool-5da13591-6wb4 node pool kubectl get nodes gke-rattlesnake-cluster-default-pool-5da13591-6wb4 --output wide Then, ssh into cluster node, then create directory /var/lib/data and create file index.html sudo mkdir /var/lib/data/ \u0026\u0026 sudo sh -c \"echo 'Hello from Kubernetes storage' \u003e /var/lib/data/index.html\" Go to a shell to the container running in your Pod : kubectl exec -it pv-example-pod -- /bin/bash In your shell, verify that nginx is serving the index.html file from the hostPath volume : curl http://localhost/ We should get an output Hello from Kubernetes storage ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:4","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Implement Node App Web Service and Database using MySQL with Persistent Volume We continue from Learning Kubernetes with example Node.js app with MySQL database. Create configuration file app-and-database.yml then fill code like following below : Create Persistent Volume and Persistent Volume Claim --- # DB PERSISTENCE VOLUME apiVersion: v1 kind: PersistentVolume metadata: name: pv-learn-nodejs labels: app: mysql type: local spec: storageClassName: manual capacity: storage: 20Gi accessModes: - ReadWriteOnce hostPath: path: \"/var/lib/data\" --- # DB PERSISTENCE VOLUME CLAIM apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-claim-learn-nodejs labels: app: mysql spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 20Gi --- Create Node.js app deployment --- # APP DEPLOYMENT apiVersion: apps/v1 kind: Deployment metadata: name: app-learn-nodejs labels: app: app tier: learn-nodejs spec: replicas: 3 selector: matchLabels: app: app tier: learn-nodejs template: metadata: labels: app: app tier: learn-nodejs spec: containers: - name: learn-nodejs image: piinalpin/learn-nodejs env: - name: \"PORT\" value: \"8080\" - name: \"DB_HOST\" value: \"db-learn-nodejs-svc\" - name: \"DB_USERNAME\" value: \"root\" - name: \"DB_PASSWORD\" value: \"p@s5w0rD\" - name: \"DB_NAME\" value: \"learn_nodejs\" - name: \"DB_PORT\" value: \"3306\" --- Create database MySQL deployment with PersistentVolumeClaim --- # DB DEPLOYMENT apiVersion: apps/v1 kind: Deployment metadata: name: db-learn-nodejs labels: app: db tier: learn-nodejs spec: selector: matchLabels: app: db tier: learn-nodejs strategy: type: Recreate template: metadata: labels: app: db tier: learn-nodejs spec: containers: - image: mysql:latest name: mysql env: - name: MYSQL_ROOT_PASSWORD value: p@s5w0rD - name: \"PORT\" value: \"3306\" volumeMounts: - name: pv-learn-nodejs mountPath: /var/lib/mysql volumes: - name: pv-learn-nodejs persistentVolumeClaim: claimName: pv-claim-learn-nodejs --- Expose App and DB with Service --- # APP SERVICE apiVersion: v1 kind: Service metadata: name: app-learn-nodejs-svc labels: app: app tier: learn-nodejs spec: selector: app: app tier: learn-nodejs ports: - name: app-port protocol: TCP port: 8001 targetPort: 8080 type: LoadBalancer --- # DB SERVICE apiVersion: v1 kind: Service metadata: name: db-learn-nodejs-svc labels: app: db tier: learn-nodejs spec: selector: app: db tier: learn-nodejs ports: - name: db-port protocol: TCP port: 3306 targetPort: 3306 type: LoadBalancer --- Create Ingress for App --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: app-learn-nodejs-ingress spec: rules: - http: paths: - path: /programming-languages pathType: Prefix backend: service: name: app-learn-nodejs-svc port: number: 8001 Apply configuration file kubectl apply -f app-and-database.yaml Create database and table on running container Run container with creating a pod kubectl run -it --rm --image=mysql:latest --restart=Never mysql-client -- mysql -h \u003cDB_SVC_NAME\u003e -p\u003cYOUR_MYSQL_PASSWORD\u003e Create database, table and insert value CREATE TABLE `programming_languages` ( `id` INT(11) NOT NULL auto_increment , `name` VARCHAR(255) NOT NULL , `released_year` INT NOT NULL , `github_rank` INT NULL , `pypl_rank` INT NULL , `tiobe_rank` INT NULL , `created_at` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP , `updated_at` DATETIME on UPDATE CURRENT_TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY (`id`), UNIQUE `idx_name_unique` (`name`(255)) ) engine = innodb charset=utf8mb4 COLLATE utf8mb4_general_ci; --------------------------------- INSERT INTO programming_languages(id, name, released_year, github_rank, pypl_rank, tiobe_rank) VALUES (1,'JavaScript',1995,1,3,7), (2,'Python',1991,2,1,3), (3,'Java',1995,3,2,2), (4,'TypeScript',2012,7,10,42), (5,'C#',2000,9,4,5), (6,'PHP',1995,8,6,8), (7,'C++',1985,5,5,4), (8,'C',1972,10,5,1), (9,'Ruby',1995,6,15,15), (10,'R',1993,33,7,9), (11,'Objective-C',1984,18,8,18), (12,'Swift',2015,16,9,13), (13,'Kotlin',2011,15,12,40","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:5","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Full Example Code You can see my example here https://github.com/piinalpin/learning-kubernetes.git ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:6","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Thankyou LogRocket - Node.js, Express.js, and MySQL: A step-by-step REST API example SQLHack - SQL Database on Kubernetes: Considerations and Best Practices kubernetes.io - Persistent Volumes kubernetes.io - Configure a Pod to Use a PersistentVolume for Storage ","date":"2021-05-28","objectID":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/:0:7","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing","Persistence Volume","Persistence Volume Claim"],"title":"Persistent Volume and Persistent Volume Claim in Kubernetes","uri":"/2021/05/why-should-learn-kubernetes-pv-and-pvc/"},{"categories":["Documentation"],"content":"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.","date":"2021-05-24","objectID":"/2021/05/why-should-learn-kubernetes/","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing"],"title":"Learning Kubernetes","uri":"/2021/05/why-should-learn-kubernetes/"},{"categories":["Documentation"],"content":"What is Kubernetes? Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. More information, please follow this link Kubernetes Overview. ","date":"2021-05-24","objectID":"/2021/05/why-should-learn-kubernetes/:0:1","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing"],"title":"Learning Kubernetes","uri":"/2021/05/why-should-learn-kubernetes/"},{"categories":["Documentation"],"content":"Why we need Kubernetes Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn’t it be easier if this behavior was handled by a system? That’s how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system. Kubernetes can provide with : Service discovery and load balancing Storage orchestration Automated rollouts and rollbacks Automatic bin packing Self healing Secret and configuratio management For more information please follow this link Why you Need Kubernetes. ","date":"2021-05-24","objectID":"/2021/05/why-should-learn-kubernetes/:0:2","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing"],"title":"Learning Kubernetes","uri":"/2021/05/why-should-learn-kubernetes/"},{"categories":["Documentation"],"content":"Start using Kubernetes Getting Start with Kubernetes on Docker Desktop Docker Desktop is the easiest way to run Kubernetes on your local machine - it gives you a fully certified Kubernetes cluster and manages all the components for you. In this lab you’ll learn how to set up Kubernetes on Docker Desktop and run a simple demo app. You’ll gain experience of working with Kubernetes and comparing the app definition syntax to Docker Compose. Install Docker Desktop from official website, please follow this link Install Docker Desktop. Docker Desktop is freely available in a community edition, for Windows and Mac. Enable Kubernetes Kubernetes itself runs in containers. When you deploy a Kubenetes cluster you first install Docker (or another container runtime like containerd) and then use tools like kubeadm which starts all the Kubernetes components in containers. Docker Desktop does all that for you. Verify Kubernetes Cluster If you’ve worked with Docker before, you’re used to managing containers with the docker and docker-compose command lines. Kubernetes uses a different tool called kubectl to manage apps - Docker Desktop installs kubectl for you too. The cluster can be interacted with using the kubectl CLI. This is the main approach used for managing Kubernetes and the applications running on top of the cluster. Details of the cluster and its health status is. kubectl cluster-info We should get an output Kubernetes control plane is running at https://kubernetes.docker.internal:6443 KubeDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. To view the nodes in the cluster with. kubectl get nodes We shoulkd get an output NAME STATUS ROLES AGE VERSION docker-desktop Ready master 4d22h v1.19.7 If the node is marked as NotReady then it is still starting the components. This command shows all nodes that can be used to host our applications. Now we have only one node, and we can see that it’s status is ready (it is ready to accept applications for deployment). Deploy Container With a running Kubernetes cluster, containers can now be deployed. Using kubectl run, it allows containers to be deployed onto the cluster. kubectl create deployment hello-world --image=piinalpin/sample-node-web-app To check status of the deployment. kubectl get pods We should get an output NAME READY STATUS RESTARTS AGE hello-world-6b64556c5b-9pzrp 1/1 Running 0 88s Once the container is running it can be exposed via different networking options, depending on requirements. One possible solution is NodePort, that provides a dynamic port to a container. kubectl expose deployment hello-world --port=8080 --type=NodePort The command below finds the allocated port and executes a HTTP request. export NODE_PORT=$(kubectl get svc hello-world -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{\"\\n\"}}{{end}}{{end}}') echo \"Accessing http://localhost:$NODE_PORT\" curl http://localhost:$NODE_PORT We should get an output {\"status\":\"success\",\"message\":\"Hello World! This api from Node.js\"} Kubernetes Dashboard Deploy latest kubernetes dashboard, you can deploy the dashboard itself with the following single command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kube","date":"2021-05-24","objectID":"/2021/05/why-should-learn-kubernetes/:0:3","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing"],"title":"Learning Kubernetes","uri":"/2021/05/why-should-learn-kubernetes/"},{"categories":["Documentation"],"content":"Thankyou Docker Labs - Getting Started with Kubernetes on Docker Desktop kubernetes.io - What is Kubernetes? KataCoda - Launch Single Node Kubernetes Cluster Mirantis - Introduction to YAML: Creating a Kubernetes deployment Network Chuck - you need to learn Kubernetes RIGHT NOW!! kubernetes.io - Exposing an External IP Address to Access an Application in a Cluster UpCloud - How to deploy Kubernetes Dashboard quickly and easily ","date":"2021-05-24","objectID":"/2021/05/why-should-learn-kubernetes/:0:4","tags":["Kubernetes","Deployment","DevOps","Container","Cluster","Nodes","Pods","K8s","Cloud Computing"],"title":"Learning Kubernetes","uri":"/2021/05/why-should-learn-kubernetes/"},{"categories":["Documentation"],"content":"Docker takes away repetitive, mundane configuration tasks and is used throughout the development lifecycle for fast, easy and portable application development - desktop and cloud. Docker’s comprehensive end to end platform includes UIs, CLIs, APIs and security that are engineered to work together across the entire application delivery lifecycle.","date":"2021-05-20","objectID":"/2021/05/why-should-learn-docker/","tags":["DevOps","Deployment","Container","Docker","Docker Container"],"title":"Why Should Learn Docker","uri":"/2021/05/why-should-learn-docker/"},{"categories":["Documentation"],"content":"What is Docker? Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. ","date":"2021-05-20","objectID":"/2021/05/why-should-learn-docker/:0:1","tags":["DevOps","Deployment","Container","Docker","Docker Container"],"title":"Why Should Learn Docker","uri":"/2021/05/why-should-learn-docker/"},{"categories":["Documentation"],"content":"Docker Architecture Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers. More information please follow Docs Docker. ","date":"2021-05-20","objectID":"/2021/05/why-should-learn-docker/:0:2","tags":["DevOps","Deployment","Container","Docker","Docker Container"],"title":"Why Should Learn Docker","uri":"/2021/05/why-should-learn-docker/"},{"categories":["Documentation"],"content":"Start using Docker Run Docker Command The following command run a hello_world container. docker run --name hello_world -d piinalpin/sample-node-web-app If you don’t have the piinalpin/sample-node-web-app image locally, docker pulls it from your configured registry, as though you had run docker pull piinalpin/sample-node-web-app manually. Docker create a new container hello_world from image piinalpin/sample-node-web-app Let’s get into the container. docker exec -it hello_world /bin/sh Run a simple command to get list folder in work directory by type the following command. ls -l \u0026\u0026 cat server.js We will get an output. total 40 -rw-r--r-- 1 root root 392 May 19 08:30 Dockerfile drwxr-xr-x 62 root root 4096 May 19 08:26 node_modules -rw-r--r-- 1 root root 81 May 19 08:00 package-lock.json -rw-r--r-- 1 root root 300 May 19 08:08 package.json -rw-r--r-- 1 root root 343 May 19 08:16 server.js -rw-r--r-- 1 root root 17266 May 19 08:08 yarn.lock var express = require('express'); var app = express(); const PORT = 8080; const HOST = '0.0.0.0'; app.get('/', (req, res, next) =\u003e { const data = { 'status': 'success', 'message': 'Hello World! This api from Node.js' }; res.json(data); }); app.listen(PORT, HOST); console.log(`Running on http://${HOST}:${PORT}`); Inside the container there is a folder node_modules and Dockerfile package-lock.json package.json server.js yarn.lock files The server is using express.js and running on port 8080 Show linux distribution The following command use to show linux distribution inside container. cat /etc/os-release We will get an output NAME=\"Alpine Linux\" ID=alpine VERSION_ID=3.11.11 PRETTY_NAME=\"Alpine Linux v3.11\" HOME_URL=\"https://alpinelinux.org/\" BUG_REPORT_URL=\"https://bugs.alpinelinux.org/\" This image use Linux Alpine which the operating system is very light. And then let’s testing the server API by type the following command curl http://localhost:8080 We should get an output curl: not found Thats mean, on Linux Alpine distribution didn’t have curl command. We should install it by type the following command. apk update apk search curl apk add curl Then, test again the API server by type a command curl http://localhost:8080. And we should get an output. {\"status\":\"success\",\"message\":\"Hello World! This api from Node.js\"} Expose Container PORT to Host PORT If we want to access the container from host, we should expose the container port to host port by type a command docker run -p \u003cHOST_PORT\u003e:\u003cCONTAINER_PORT\u003e --name \u003cCONTAINER_NAME\u003e -d \u003cIMAGE\u003e docker run -p 8001:8080 --name hello_world2 -d piinalpin/sample-node-web-app Docker create a new container hello_world2 from image piinalpin/sample-node-web-app Docker expose port 8080 from container to 8001 host port and we can access from host Check the container is already running by type following command docker ps We should get an output CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ec819086e2a5 piinalpin/sample-node-web-app \"docker-entrypoint.s…\" 27 minutes ago Up 27 minutes 0.0.0.0:8001-\u003e8080/tcp, :::8001-\u003e8080/tcp hello_world2 d81a2784edaf piinalpin/sample-node-web-app \"docker-entrypoint.s…\" About an hour ago Up About an hour 8080/tcp hello_world Let’s test the server API from the host by type following command curl http://localhost:8001 We should get an output {\"status\":\"success\",\"message\":\"Hello World! This api from Node.js\"}% Build Image We will build an image which running a Node.js server. If you don’t have a Node.js install from Node.js. Create a simple node application. First we will install the dependencies, we will use an express.js module by type a command npm install express or yarn add express Then create a server.js like following script var express = require('express'); var app = express(); const PORT = 8080; const HOST = '0.0.0.0'; app.get('/', (req, res, next) =\u003e { const data = { 'status': 'success', 'message': 'Hello World! This api from Node.js' }; res.json(data); }); app.listen(PORT, HOST); console.log(`Running on http://${HOST}:","date":"2021-05-20","objectID":"/2021/05/why-should-learn-docker/:0:3","tags":["DevOps","Deployment","Container","Docker","Docker Container"],"title":"Why Should Learn Docker","uri":"/2021/05/why-should-learn-docker/"},{"categories":["Documentation"],"content":"Thankyou Docs Docker - Docker Overview ","date":"2021-05-20","objectID":"/2021/05/why-should-learn-docker/:0:4","tags":["DevOps","Deployment","Container","Docker","Docker Container"],"title":"Why Should Learn Docker","uri":"/2021/05/why-should-learn-docker/"},{"categories":["Documentation"],"content":"How to create AirMessage server on MacOS for IMessage on Android with port forwarding using ngrok server.","date":"2021-04-22","objectID":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/","tags":["IMessage","AirMessage","Ngrok.io","Ngrok","AirMessage Server","IMessage on Android"],"title":"iMessage on Android With AirMessage Server for macOS and Ngrok","uri":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/"},{"categories":["Documentation"],"content":"How does AirMessage work? As Android phones and web browsers aren’t allowed to connect to iMessage, AirMessage leverages a Mac computer to handle sending and receiving messages instead. Every message you send from AirMessage is sent to AirMessage Server on your Mac, which is then sent over iMessage. When a new incoming message is received from iMessage, that message is then sent back to your devices. ","date":"2021-04-22","objectID":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/:0:1","tags":["IMessage","AirMessage","Ngrok.io","Ngrok","AirMessage Server","IMessage on Android"],"title":"iMessage on Android With AirMessage Server for macOS and Ngrok","uri":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/"},{"categories":["Documentation"],"content":"What is Ngrok? Ngrok is a cross-platform application that enables developers to expose a local development server to the Internet with minimal effort. The software makes your locally-hosted web server appear to be hosted on a subdomain of ngrok.com, meaning that no public IP or domain name on the local machine is needed. Similar functionality can be achieved with Reverse SSH Tunneling, but this requires more setup as well as hosting of your own remote server. Ngrok is able to bypass NAT Mapping and firewall restrictions by creating a long-lived TCP tunnel from a randomly generated subdomain on ngrok.com (e.g. 3gf892ks.ngrok.com) to the local machine. After specifying the port that your web server listens on, the ngrok client program initiates a secure connection to the ngrok server and then anyone can make requests to your local server with the unique ngrok tunnel address. The ngrok developer’s guide contains more detailed information on how it works. ","date":"2021-04-22","objectID":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/:0:2","tags":["IMessage","AirMessage","Ngrok.io","Ngrok","AirMessage Server","IMessage on Android"],"title":"iMessage on Android With AirMessage Server for macOS and Ngrok","uri":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/"},{"categories":["Documentation"],"content":"Step to create AirMessage server Installing AirMessage Server Go to AirMessage.org and download AirMessage Server for macOS. AirMessage Server is a crucial part of the AirMessage experience - it forwards incoming messages to your Android phone or browser, and sends outgoing messages on their behalf. To get started, simply download the server onto your Mac computer and place it in the Applications folder. When you open the app, you will be greeted with this welcome message. Open the preferences window and click “Edit Password…”, and replace the default password with a password of your choosing. Remember, your messages are only as secure as the password you pick! Enabling Messaging Access If you are on macOS Mojave 10.14 or later, you will have to allow AirMessage automation access in order to send messages. You will be prompted when first running the software, though if you previously rejected this permission, you can re-enable it later under System Preferences \u003e Security \u0026 Privacy \u003e Privacy \u003e Automation. You will also be prompted to allow AirMessage to read your messages on macOS Mojave 10.14 or later. Under System Preferences \u003e Security \u0026 Privacy \u003e Privacy \u003e Full Disk Access, add AirMessage. AirMessage will not read any data other than your Messages data. Adjusting Sleep Settings As AirMessage functions as a server on your Mac, it will need to be available all the time in order to send and receive messages. For this reason, you will have to disable sleep settings on your Mac. Navigate to System Preferences \u003e Energy Saver to change this setting. If you are running AirMessage on a laptop, the system will freeze all software currently running when the lid is shut, regardless of energy saver settings. If you would like to turn your laptop into a stationary server, we recommend that you use a keep-awake utility such as Amphetamine or Caffeinate (built-in commmand). Installing Ngrok Server Download ngrok server at Ngrok.com. On Linux or OSX you can unzip from a terminal with the following command. On Windows, just double click ngrok.zip. unzip /path/to/ngrok.zip Sign Up into Ngrok.com if you’re not registered. Skip this step if you’re already registered. Connecting an account will list your open tunnels in the dashboard, give you longer tunnel timeouts, and more. Visit the dashboard to get your auth token. ./ngrok authtoken \u003cCHANGE_TO_YOUR_AUTH_TOKEN\u003e Try running it from the command line ./ngrok http 8080 Configuring Connection By default, AirMessage running on port 1359, if you changed the default AirMessage server port, enter that number instead of 1359. Run this command line on terminal. ./ngrok tcp 1359 In the new Session Status window that opens, note down the address that is shown after Forwarding, but leave out the tcp://, example: 8.tcp.ngrok.io:17418. Setup AirMessage on Android In the AirMessage Android app, go to the Server address and enter the forwarding address, example: 8.tcp.ngrok.io:17418. Ensure your password is the same as what you set in the AirMessage preferences on your Mac. Connect. To continue using AirMessage, do not close the ngrok Session Status window that opened on your Mac; leave it running at all times, as you will with your Mac. Whenever it is closed, the user is logged out, the Mac is restarted, etc., you must repeat Steps forwarding tcp port. This is a limitation of the free version of ngrok, you must upgrade to use a static/permanent forwarding address. ","date":"2021-04-22","objectID":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/:0:3","tags":["IMessage","AirMessage","Ngrok.io","Ngrok","AirMessage Server","IMessage on Android"],"title":"iMessage on Android With AirMessage Server for macOS and Ngrok","uri":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/"},{"categories":["Documentation"],"content":"Thankyou Reddit - How to use AirMessage without Port Forwarding or Router Access AirMessage About - About AirMessage AirMessage Installation - AirMessage Installation Guide ","date":"2021-04-22","objectID":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/:0:4","tags":["IMessage","AirMessage","Ngrok.io","Ngrok","AirMessage Server","IMessage on Android"],"title":"iMessage on Android With AirMessage Server for macOS and Ngrok","uri":"/2021/04/imessage-on-android-with-air-message-server-and-ngrok/"},{"categories":["Documentation"],"content":"Understanding message broker works using RabbitMQ and Spring Boot with microservice architecture.","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"What is Message Broker? A message broker is software that enables applications, systems, and services to communicate with each other and exchange information. The message broker does this by translating messages between formal messaging protocols. This allows interdependent services to “talk” with one another directly, even if they were written in different languages or implemented on different platforms. Message brokers are software modules within messaging middleware or message-oriented middleware (MOM) solutions. This type of middleware provides developers with a standardized means of handling the flow of data between an application’s components so that they can focus on its core logic. It can serve as a distributed communications layer that allows applications spanning multiple platforms to communicate internally. ","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:1","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Step to create message broker using RabbitMQ First thing You already have a RabbitMQ server and activate RabbitMQ management, please follow this link Install RabbitMQ Starting with spring initializr Generate two project maven from Spring Initializr. First is rabbitmq and second is rabbitmq-listener. Here my pom.xml from rabbitmq project. \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.3.4.RELEASE\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cgroupId\u003ecom.example\u003c/groupId\u003e \u003cartifactId\u003erabbitmq\u003c/artifactId\u003e \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e \u003cname\u003erabbitmq\u003c/name\u003e \u003cdescription\u003eDemo project for Spring Boot\u003c/description\u003e \u003cproperties\u003e \u003cjava.version\u003e11\u003c/java.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-tomcat\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-undertow\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-amqp --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-amqp\u003c/artifactId\u003e \u003cversion\u003e2.4.1\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-databind\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cexcludes\u003e \u003cexclude\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003c/exclude\u003e \u003c/excludes\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/project\u003e And here is pom.xml from rabbitmq-listener project. \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.4.4\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cgroupId\u003ecom.example.\u003c/groupId\u003e \u003cartifactId\u003erabbitmq-listener\u003c/artifactId\u003e \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e \u003cname\u003erabbitmq-listener\u003c/name\u003e \u003cdescription\u003eDemo project for Spring Boot\u003c/description\u003e \u003cproperties\u003e \u003cjava.version\u003e11\u003c/java.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-amqp\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.amqp\u003c/groupId\u003e \u003cartifactId\u003espring-rabbit-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-databind\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cexclud","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:2","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Working with rabbitmq as a producers Update your application.properties like below. info.app.name=RabbitMQ Example info.app.description=RabbitMQ Example info.app.version=1.0.0 spring.jmx.enabled=false spring.rabbitmq.host=localhost spring.rabbitmq.username=guest spring.rabbitmq.password=guest spring.rabbitmq.port=5672 server.port=8080 Create constant value for queue name in package com.example.rabbitmq.constant. public class QueueConstant { public final static String HELLO_WORLD = \"hello-world\"; public final static String EMAIL = \"email-sender\"; } Create bean configuration to register queue in package com.example.rabbitmq.configuration. @Configuration public class QueueConfiguration { @Bean public Queue helloWorld() { return new Queue(QueueConstant.HELLO_WORLD); } @Bean public Queue sendEmail() { return new Queue(QueueConstant.EMAIL); } } Create message sender to send a message to RabbitMQ com.example.rabbitmq.service. @Service public class MessagesSender { private final static Logger log = LoggerFactory.getLogger(MessagesSender.class); @Autowired private RabbitTemplate rabbitTemplate; public void send(String name) { log.info(\"Sending message to RabbitMQ...\"); rabbitTemplate.convertAndSend(QueueConstant.HELLO_WORLD, String.format(\"Hello my name is %s\", name)); } } Create endpoint to send message dinamically from REST com.example.rabbitmq.controller. @RestController public class ApplicationController { private final MessagesSender messagesSender; @Autowired public ApplicationController(MessagesSender messagesSender) { this.messagesSender = messagesSender; } @PostMapping(value = \"/hello-world\", name = \"Hello World\") public Map\u003cString, String\u003e hello(@RequestBody Map\u003cString, String\u003e request) { messagesSender.send(request.get(\"name\")); Map\u003cString, String\u003e ret = new HashMap\u003c\u003e(); ret.put(\"message\", \"sending message...\"); return ret; } } Run spring boot by typing mvn spring-boot:run then open Postman like below. URL: http://localhost:8080/hello-world (POST) 2021-03-28 02:02:54.952 INFO 54639 --- [ XNIO-1 task-1] c.e.rabbitmq.service.HelloWorldSender : Sending message to RabbitMQ... 2021-03-28 02:02:54.957 INFO 54639 --- [ XNIO-1 task-1] o.s.a.r.c.CachingConnectionFactory : Attempting to connect to: [localhost:5672] 2021-03-28 02:02:55.107 INFO 54639 --- [ XNIO-1 task-1] o.s.a.r.c.CachingConnectionFactory : Created new connection: rabbitConnectionFactory#4adc663e:0/SimpleConnection@43156916 [delegate=amqp://guest@127.0.0.1:5672/, localPort= 53933] 2021-03-28 02:02:55.124 INFO 54639 --- [ XNIO-1 task-1] o.s.amqp.rabbit.core.RabbitAdmin : Auto-declaring a non-durable, auto-delete, or exclusive Queue (hello-world) durable:false, auto-delete:false, exclusive:false. It will be redeclared if the broker stops and is restarted while the connection factory is alive, but all messages will be lost. And go to RabbitMQ management by accessing http://localhost:15672 by default username and password is guest then go to Queues tab. Scroll down to Get Message menu, make sure the message is exists in the queue. Should be like below. Try another message to RabbitMQ, we will send a json string message. Update com.example.rabbitmq.service.MessageSender.java like below. @Service public class MessagesSender { private final static Logger log = LoggerFactory.getLogger(MessagesSender.class); @Autowired private RabbitTemplate rabbitTemplate; @Autowired private ObjectMapper mapper; public void send(String name) { log.info(\"Sending message to RabbitMQ...\"); rabbitTemplate.convertAndSend(QueueConstant.HELLO_WORLD, String.format(\"Hello my name is %s\", name)); } public void sendEmail(Map\u003cString, String\u003e map) { log.info(\"Sending message to RabbitMQ...\"); try { rabbitTemplate.convertAndSend(QueueConstant.EMAIL, mapper.writeValueAsString(map)); } catch (JsonProcessingException e) { e.printStackTrace(); } } } Update rest controller com.example.rabbitmq.controller.ApplicationController.java. @RestController public class ApplicationController { private final MessagesSender","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:3","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Working with rabbitmq-listener as a concumers Update your application.properties like below. info.app.name=RabbitMQ Listener Example info.app.description=RabbitMQ Listener Example info.app.version=1.0.0 spring.jmx.enabled=false spring.rabbitmq.host=localhost spring.rabbitmq.username=guest spring.rabbitmq.password=guest spring.rabbitmq.port=5672 Create bean configuration to inject ObjectMapper dependencies in package com.example.rabbitmqlistener.config. @Configuration public class BeanConfiguration { @Bean public ObjectMapper mapper() { return new ObjectMapper(); } } Create constant value for queue name in package com.example.rabbitmqlistener.constant. public class QueueConstant { public final static String HELLO_WORLD = \"hello-world\"; public final static String EMAIL = \"email-sender\"; } Create bean configuration to register queue in package com.example.rabbitmqlistener.configuration. @Configuration public class QueueConfiguration { @Bean public Queue helloWorld() { return new Queue(QueueConstant.HELLO_WORLD); } @Bean public Queue sendEmail() { return new Queue(QueueConstant.EMAIL); } } Create model com.example.rabbitmqlistener.model.EmailModel used mapping message into class object. @Data @NoArgsConstructor @AllArgsConstructor public class EmailModel { private String from; private String to; private String subject; private String message; private String name; } Create listener com.example.rabbitmqlistener.service.MessageListenerService to listen message from RabbitMQ by queues name. @Service public class MessageListenerService { private final static Logger log = LoggerFactory.getLogger(MessageListenerService.class); private final ObjectMapper mapper; @Autowired public MessageListenerService(ObjectMapper mapper) { this.mapper = mapper; } @RabbitListener(queues = QueueConstant.HELLO_WORLD) public void greeting(String message) { log.info(\"Receiving message...\"); log.info(\"Message is: \" + message); } @RabbitListener(queues = QueueConstant.EMAIL) public void sendEmail(String message) { log.info(\"Receiving message: \" + message); try { EmailModel email = mapper.readValue(message, EmailModel.class); log.info(\"Converting to model...\"); log.info(\"EmailModel:: \" + email.toString()); } catch (JsonProcessingException e) { e.printStackTrace(); } } } Run spring boot by typing mvn spring-boot:run and see the logs. 2021-03-28 11:11:56.224 INFO 67938 --- [ntContainer#0-1] c.e.r.service.MessageListenerService : Receiving message... 2021-03-28 11:11:56.226 INFO 67938 --- [ntContainer#0-1] c.e.r.service.MessageListenerService : Message is: Hello my name is Maverick 2021-03-28 11:13:18.721 INFO 68207 --- [ntContainer#1-1] c.e.r.service.MessageListenerService : Receiving message: {\"to\":\"maverick@test.com\",\"from\":\"calvinjoe@test.com\",\"message\":\"Hello, you got message from RabbitMQ\",\"subject\":\"RabbitMQ Information\",\"name\":\"Maverick\"} 2021-03-28 11:13:18.787 INFO 68207 --- [ntContainer#1-1] c.e.r.service.MessageListenerService : Converting to model... 2021-03-28 11:13:18.813 INFO 68207 --- [ntContainer#1-1] c.e.r.service.MessageListenerService : EmailModel:: EmailModel(from=calvinjoe@test.com, to=maverick@test.com, subject=RabbitMQ Information, message=Hello, you got message from RabbitMQ, name=Maverick) ","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:4","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project https://github.com/piinalpin/rabbitmq.git ","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:5","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Thankyou IBM - Message Brokers Baeldung - Messaging with Spring AMQP ","date":"2021-03-28","objectID":"/2021/03/how-messages-broker-work-using-rabbitmq/:0:6","tags":["Java","Spring Boot","Messaging","Broker","Message Broker","RabbitMQ"],"title":"How Messages Broker Work Using RabbitMQ","uri":"/2021/03/how-messages-broker-work-using-rabbitmq/"},{"categories":["Documentation"],"content":"Something I need to do when setting up a laptop.","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Setup Terminal with Oh My Zsh and Powerlevel10k Intalling Oh My Zsh sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Download and Install Nerd Patched Fonts Download and install FuraMono Fonts Installing Powerlevel10k git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/themes/powerlevel10k Change .zshrc Configuration vi .zshrc Change ZSH_THEME to ZSH_THEME=\"powerlevel10k/powerlevel10k\" Enable auto correction uncomment line ENABLE_CORRECTION=\"true\" Then restart your terminal and you will see configuration wizard powerlevel10k If you see [WARNING]: Console output during zsh initialization detected. change your .p10k.zsh and change this line typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet If you see [oh-my-zsh] Insecure completion-dependent directories detected: type command below chmod 755 /usr/local/share/zsh chmod 755 /usr/local/share/zsh/site-functions Add Plugin Auto Suggestions and Syntax Highlighting Download plugins for auto suggestion and syntax highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting Scroll down on .zshrc find plugin=(git) and change to plugins=(git zsh-autosuggestions zsh-syntax-highlighting) ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:1","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Configure Multiple JDK with Jenv Install Jenv Install jenv with brew brew install jenv echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' \u003e\u003e ~/.zshrc echo 'eval \"$(jenv init -)\"' \u003e\u003e ~/.zshrc Install JDK Add brew cask by adding homebrew/cask brew tap homebrew/cask-versions Install JDK 11 and JDK 8 from Azul OpenJDK Add java11 and java8 into jenv jenv add /Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home/ jenv add /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/ See all installed versions java jenv versions Configure global version jenv global 11.0.9 ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:2","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Configure Multiple Python Version with Pyenv Install pyenv Install pyenv with brew brew install pyenv Define environment variable PYENV_ROOT and add pyenv init I will use zsh so here bash command line echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' \u003e\u003e ~/.zshrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.zshrc echo -e 'if command -v pyenv 1\u003e/dev/null 2\u003e\u00261; then\\n eval \"$(pyenv init -)\"\\nfi' \u003e\u003e ~/.zshrc source .zshrc Install Python Configure global environment python pyenv install 3.9.1 pyenv global 3.9.1 ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:3","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Configure docker and database with docker *Installing Docker Download Docker from docker hub Install MySQL database on docker docker run -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:latest Install SQL Server on docker docker run -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=\u003cYourStrong@Passw0rd\u003e\" -p 1433:1433 --name sqlserver -h sqlserver -d mcr.microsoft.com/mssql/server:2019-latest Install Postgresql on docker docker run --name postgresql -d -p 5432:5432 -e POSTGRES_PASSWORD=yoursecretpassword postgres ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:4","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Configuring Auto Connect SSH Tunneling You can read this step on Auto Start SSH Tunneling on Mac ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:5","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Configuring RabbitMQ and RabbitMQ Management Install RabbitMQ with brew brew install rabbitmq Export path for RabbitMQ export PATH=$PATH:/usr/local/sbin Start service when laptop is started automatically in backgroun brew services start rabbitmq Enable management plugin rabbitmq-plugins enable rabbitmq_management Try to access http://localhost:15672 ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:6","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Install Composer brew install composer ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:7","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Install Node.js and Yarn Go to Node.js Org download and install it. sudo chown -R $USER /usr/local/lib/node_modules npm install --global yarn ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:8","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Install Hugo Blog brew install hugo ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:9","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Install Maven brew install maven ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:10","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"References Make your terminal beautiful and fast with ZSH shell and PowerLevel10K Terminal Keren dengan Oh My Zsh dan PowerLevel10k Jenv How to install Java JDK on macOS Simple Python Version Management: pyenv Connecting to a mysql running on a Docker container Quickstart: Run SQL Server container images with Docker Docker container for Postgres 9.1 not exposing port 5432 to host The Homebrew RabbitMQ Formula RabbitMQ Management Plugin ","date":"2021-02-28","objectID":"/2021/02/setup-my-own-mac/:0:11","tags":["Bash","Oh My Zsh","Powerlevel10k","Setup Mac"],"title":"Setup My Own Mac","uri":"/2021/02/setup-my-own-mac/"},{"categories":["Documentation"],"content":"Build a simple microservices gRPC server and client using Spring Boot","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"Prerequisites Spring Initializr gRPC Server gRPC Client Lombok Annotation ","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:1","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"What is gRPC? In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client has a stub (referred to as just a client in some languages) that provides the same methods as the server. ","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:2","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"gRPC Server and Client with Java (Spring Boot) Start with spring initializr I’m depending Spring Initializr for this as it is much easier. And we have to create two spring boot projects and started with maven project also use Lombok plugins. gRPC Server-One gRPC Server-Two gRPC Server One as a Server Add below dependencies on your pom.xml \u003c!-- For the gRPC server --\u003e \u003cdependency\u003e \u003cgroupId\u003enet.devh\u003c/groupId\u003e \u003cartifactId\u003egrpc-server-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e2.5.1.RELEASE\u003c/version\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eio.grpc\u003c/groupId\u003e \u003cartifactId\u003egrpc-netty-shaded\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003c!-- For the gRPC client --\u003e \u003cdependency\u003e \u003cgroupId\u003enet.devh\u003c/groupId\u003e \u003cartifactId\u003egrpc-client-spring-boot-autoconfigure\u003c/artifactId\u003e \u003cversion\u003e2.5.1.RELEASE\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cversion\u003e1.18.16\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e Add below extension and plugins into build section \u003cextensions\u003e \u003cextension\u003e \u003cgroupId\u003ekr.motd.maven\u003c/groupId\u003e \u003cartifactId\u003eos-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e1.6.1\u003c/version\u003e \u003c/extension\u003e \u003c/extensions\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.xolstice.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003eprotobuf-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e0.6.1\u003c/version\u003e \u003cconfiguration\u003e \u003cprotocArtifact\u003e com.google.protobuf:protoc:3.3.0:exe:${os.detected.classifier} \u003c/protocArtifact\u003e \u003cpluginId\u003egrpc-java\u003c/pluginId\u003e \u003cpluginArtifact\u003e io.grpc:protoc-gen-grpc-java:1.4.0:exe:${os.detected.classifier} \u003c/pluginArtifact\u003e \u003cprotoSourceRoot\u003esrc/main/proto\u003c/protoSourceRoot\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cgoals\u003e \u003cgoal\u003ecompile\u003c/goal\u003e \u003cgoal\u003ecompile-custom\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003c/plugins\u003e Add configuration server port and gRPC port number. Create project/application.yml. server: port: 8080 grpc: server: port: 9090 grpcServerTwo: host: localhost port: 9091 The line of grpcServerTwo used to communication channel from gRPC Server-One into gRPC Server-Two. Then create a protocol buffers on gRPC Server-One src/main/proto/HelloServiceServerOne.proto file like below. syntax = \"proto3\"; option java_multiple_files = true; package com.example.grpc.serverone.server; message HelloRequest { string firstName = 1; string lastName = 2; } message HelloResponse { string greeting = 1; } service HelloServiceServerOne { rpc hello(HelloRequest) returns (HelloResponse); } Once the proto file is created, we should package the project. It will generated the classes inside the target folder. mvn clean package -Dmaven.test.skip=true Create an implementation class com.example.grpc.serverone.server.HelloServiceServerOneImpl for the proto service. @GrpcService public class HelloServiceServerOneImpl extends HelloServiceServerOneGrpc.HelloServiceServerOneImplBase { @Override public void hello(HelloRequest request, StreamObserver\u003cHelloResponse\u003e responseObserver) { String greeting = new StringBuilder() .append(\"Hello, \") .append(request.getFirstName()) .append(\" \") .append(request.getLastName()) .append(\". This response from server one.\") .toString(); HelloResponse response = HelloResponse.newBuilder() .setGreeting(greeting) .build(); responseObserver.onNext(response); responseObserver.onCompleted(); } } Update project main application like below. public class ServerOneApplication extends SpringBootServletInitializer { public static void main(String[] args) { SpringApplication.run(ServerOneApplication.class, args); } } Try to run by typing mvn spring-boot:run to run the gRPC Server-One. gRPC Server Two as a Client Add below dependencies on your pom.xml \u003c!-- For the gRPC server --\u003e \u003cdependency\u003e \u003cgroupId\u003enet.devh\u003c/groupId\u003e \u003cartifactId\u003egrpc-server-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e2.5.1.RELEASE\u003c/version\u003e \u003cexclusions\u003e \u003cexclus","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:3","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"gRPC Client with Python This section is optional, because I curious for another programming language. Set up protocol buffers project/HelloServiceServerOne.proto. Note: It should be same for protocol buffers on gRPC Server One because we need a stub service from server one. syntax = \"proto3\"; option java_multiple_files = true; package com.example.grpc.serverone.server; message HelloRequest { string firstName = 1; string lastName = 2; } message HelloResponse { string greeting = 1; } service HelloServiceServerOne { rpc hello(HelloRequest) returns (HelloResponse); } Generate gRPC classes for Python We need some dependencies for gRPC. $ pip install grpcio grpcio-tools $ python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. HelloServiceServerOne.proto The files generated will be as follows: HelloServiceServerOne_pb2.py contains message classes. HelloServiceServerOne_pb2_grpc.py contains server and client classes Create a gRPC Client import grpc ## Import generated classes import HelloServiceServerOne_pb2, HelloServiceServerOne_pb2_grpc # Open a gRPC channel channel = grpc.insecure_channel(target='localhost:9090') # Create a stub for gRPC client stub = HelloServiceServerOne_pb2_grpc.HelloServiceServerOneStub(channel) # Create a valid request request = HelloServiceServerOne_pb2.HelloRequest(firstName=\"Maverick\", lastName=\"John Doe\") # Make the call response = stub.hello(request=request) # Output response print(response.greeting) Thats it! With the server already listening, we simply run our client. $ python client.py Hello, Maverick John Doe. This response from server one. ","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:4","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project https://github.com/piinalpin/gRPC-example.git ","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:5","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"Thankyou Medium - GRPC for Spring Boot Microservices Semantics3 - A simplified guide to gRPC in Python gRPC.io - Introduction to gRPC ","date":"2021-01-30","objectID":"/2021/01/create-simple-microservice-with-grpc/:0:6","tags":["Java","Spring Boot","Python","gRPC","gRPC Client","gRPCServer","Microservices"],"title":"Create Simple Microservice With gRPC","uri":"/2021/01/create-simple-microservice-with-grpc/"},{"categories":["Documentation"],"content":"A documentation for create React application to consume data from GraphQL server using Apollo Client","date":"2021-01-06","objectID":"/2021/01/consume-graphql-react/","tags":["React","Apollo-React","Javascript","GraphQL","GraphQL Client"],"title":"GraphQL Client using React","uri":"/2021/01/consume-graphql-react/"},{"categories":["Documentation"],"content":"Prerequisites GraphQL Apollo Client React Apollo GraphQL Tag Semantic UI React Sematic UI CSS ","date":"2021-01-06","objectID":"/2021/01/consume-graphql-react/:0:1","tags":["React","Apollo-React","Javascript","GraphQL","GraphQL Client"],"title":"GraphQL Client using React","uri":"/2021/01/consume-graphql-react/"},{"categories":["Documentation"],"content":"Step to build GraphQL client using React Note: This documentation will use GraphQL server from GraphQL Server with Spring Boot Installing React npx create-react-app react-graphql-example or if using yarn create-react-app react-graphql-example Installing Package Install some package that we need, especially those related to GraphQL and we will use Apollo, a GraphQL wrapper that makes it easy to interact with GraphQL. And for user interface we will use Semantic UI. npm install --save graphql apollo-client-preset react-apollo graphql-tag semantic-ui-react semantic-ui-css or yarn add graphql apollo-client-preset react-apollo graphql-tag semantic-ui-react semantic-ui-css Connect with GraphQL Lets connect GraphQL server using ApolloClient that we add into src/index.js import React from 'react'; import ReactDOM from 'react-dom'; import { ApolloClient } from 'apollo-client'; import { HttpLink } from 'apollo-link-http'; import { InMemoryCache } from 'apollo-cache-inmemory'; import { ApolloProvider } from 'react-apollo'; import './index.css'; import App from './App'; import reportWebVitals from './reportWebVitals'; const client = new ApolloClient({ link: new HttpLink({ uri: 'http://localhost:8080/graphql' }), cache: new InMemoryCache() }); ReactDOM.render( \u003cApolloProvider client={client}\u003e \u003cApp /\u003e \u003c/ApolloProvider\u003e, document.getElementById(\"root\") ); // If you want to start measuring performance in your app, pass a function // to log results (for example: reportWebVitals(console.log)) // or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals reportWebVitals(); Query to GraphQL Server Do query to GraphQL server from React component and makesure we have response from backend. So we first use console.log in src/App.js. import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import gql from 'graphql-tag'; import { Container, Feed, Card } from 'semantic-ui-react'; import 'semantic-ui-css/semantic.min.css' import './App.css'; class App extends Component { render() { console.log(this.props.data); return ( \u003cContainer text textAlign=\"center\"\u003e \u003cCard centered fluid\u003e \u003cCard.Content\u003e \u003cCard.Header\u003ePerson Data\u003c/Card.Header\u003e \u003c/Card.Content\u003e \u003cCard.Content\u003e {this.props.data.getAllPerson \u0026\u0026 ( \u003cFeed\u003e {this.props.data.getAllPerson.map(person =\u003e ( \u003cFeed.Event key={person.id}\u003e \u003cFeed.Content\u003e \u003cFeed.Label content={person.firstName} /\u003e \u003cFeed.Date content={person.createdAt} /\u003e \u003cFeed.Summary\u003e{person.address}\u003c/Feed.Summary\u003e \u003c/Feed.Content\u003e \u003c/Feed.Event\u003e ))} \u003c/Feed\u003e ) } \u003c/Card.Content\u003e \u003c/Card\u003e \u003cCard centered fluid\u003e \u003cCard.Content\u003e \u003cCard.Header\u003eBook Data\u003c/Card.Header\u003e \u003c/Card.Content\u003e \u003cCard.Content\u003e {/* Iterate data here later */} \u003c/Card.Content\u003e \u003c/Card\u003e \u003c/Container\u003e ); } } const queries = gql` query { getAllPerson { id firstName lastName address createdAt } getAllBook { id title releaseDate description author { id firstName address createdAt } createdAt } } `; export default graphql(queries, { options: { variables: { } } })(App); Read Data from GraphQL Server After we got data from backend, then we will render it into React component src/App.js. import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import gql from 'graphql-tag'; import { Container, Feed, Card } from 'semantic-ui-react'; import 'semantic-ui-css/semantic.min.css' import './App.css'; class App extends Component { render() { console.log(this.props.data); return ( \u003cContainer text textAlign=\"center\"\u003e \u003cCard centered fluid\u003e \u003cCard.Content\u003e \u003cCard.Header\u003ePerson Data\u003c/Card.Header\u003e \u003c/Card.Content\u003e \u003cCard.Content\u003e {this.props.data.getAllPerson \u0026\u0026 ( \u003cFeed\u003e {this.props.data.getAllPerson.map(person =\u003e ( \u003cFeed.Event key={person.id}\u003e \u003cFeed.Content\u003e \u003cFeed.Label content={person.firstName} /\u003e \u003cFeed.Date content={person.createdAt} /\u003e \u003cFeed.Summary\u003e{person.address}\u003c/Feed.Summary\u003e \u003c/Feed.Content\u003e \u003c/Feed.Event\u003e ))} \u003c/Feed\u003e ) } \u003c/Card.Content\u003e \u003c/Card\u003e \u003cCard centered fluid\u003e \u003cCard.Content\u003e \u003cCard.Header\u003eBook Data\u003c/Card.Header\u003e \u003c/Card.Content\u003e \u003cCard.Content\u003e {this.","date":"2021-01-06","objectID":"/2021/01/consume-graphql-react/:0:2","tags":["React","Apollo-React","Javascript","GraphQL","GraphQL Client"],"title":"GraphQL Client using React","uri":"/2021/01/consume-graphql-react/"},{"categories":["Documentation"],"content":"Thankyou Medium - GraphQL Client Dengan React ","date":"2021-01-06","objectID":"/2021/01/consume-graphql-react/:0:3","tags":["React","Apollo-React","Javascript","GraphQL","GraphQL Client"],"title":"GraphQL Client using React","uri":"/2021/01/consume-graphql-react/"},{"categories":["Documentation"],"content":"Build simple GraphQL server using Spring Boot with relational mapping MySQL database and Flyway database migration from scratch.","date":"2021-01-06","objectID":"/2021/01/graphql-spring-boot-relation/","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server using Spring Boot with Relational Mapping","uri":"/2021/01/graphql-spring-boot-relation/"},{"categories":["Documentation"],"content":"Prerequisites GraphQL Server with Spring Boot ","date":"2021-01-06","objectID":"/2021/01/graphql-spring-boot-relation/:0:1","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server using Spring Boot with Relational Mapping","uri":"/2021/01/graphql-spring-boot-relation/"},{"categories":["Documentation"],"content":"Step to build GraphQL server using Spring Boot with Relational Mapping Note: You should have done the previous step GraphQL Server with Spring Boot Setting Up Model Create database migration resource/db/migration/V2_2__Book.sql see Flyway Documentation for versioning migration. SET AUTOCOMMIT = false; START TRANSACTION; CREATE TABLE m_book ( id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY, created_at DATETIME NOT NULL, created_by BIGINT NOT NULL, updated_at DATETIME, deleted_at DATETIME, author_id BIGINT NOT NULL, title VARCHAR(255) NOT NULL, publisher VARCHAR(255) NOT NULL, description VARCHAR(255), release_date DATETIME NOT NULL, FOREIGN KEY (author_id) REFERENCES m_person(id) ); COMMIT; Create book model com.maverick.graphql.model.BookModel which is inheritence from BaseModel. @Entity @Table(name = \"M_BOOK\") @Builder @NoArgsConstructor @AllArgsConstructor @Where(clause = \"deleted_at is null\") public class BookModel extends BaseModel { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Getter @Setter private Long id; @ManyToOne @Getter @Setter private PersonModel author; @Column(name = \"TITLE\", nullable = false) @Getter @Setter private String title; @Column(name = \"PUBLISHER\", nullable = false) @Getter @Setter private String publisher; @Column(name = \"DESCRIPTION\") @Getter @Setter private String description; @Column(name = \"RELEASE_DATE\", nullable = false) @Getter @Setter private Timestamp releaseDate; } Edit person model com.maverick.graphql.model.PersonModel for mapping one to many from BookModel, add this line into PersonModel @JsonIgnore @OneToMany(cascade = CascadeType.ALL, fetch = FetchType.LAZY, mappedBy = \"author\") @Getter @Setter private List\u003cBookModel\u003e authors; Create repository com.maverick.graphql.repository.BookRepository extends from JpaRepository @Repository @Transactional public interface BookRepository extends JpaRepository\u003cBookModel, Long\u003e { @Query(value = \"SELECT mb.* FROM m_book mb JOIN m_person mp ON mb.author_id = mp.id \" + \"WHERE UPPER(mp.first_name) = UPPER(?1)\", nativeQuery = true) List\u003cBookModel\u003e findAllByAuthor(String name); @Query(value = \"UPDATE m_book SET deleted_at = CURRENT_DATE WHERE id = ?1\", nativeQuery = true) @Modifying void softDelete(Long id); } Create form com.maverick.graphql.form.BookForm for input request. @Data public class BookForm { private Long authorId; private String title; private String publisher; private String description; private LocalDate releaseDate; } Create service com.maverick.graphql.service.BookService to store book model into database using repository. @Service public class BookService { private final BookRepository bookRepository; @Autowired public BookService(BookRepository bookRepository) { this.bookRepository = bookRepository; } public BookModel save(BookModel book) { return bookRepository.save(book); } public List\u003cBookModel\u003e getAll() { return bookRepository.findAll(); } public List\u003cBookModel\u003e getAllByAuthor(String name) { return bookRepository.findAllByAuthor(name); } public BookModel getById(Long id) { return bookRepository.findById(id).orElse(null); } public void delete(Long id) { bookRepository.softDelete(id); } } Mutation Resolver Create mutation service com.maverick.graphql.mutation.BookMutation use for anychange data from GraphQL schema. @Service public class BookMutation implements GraphQLMutationResolver { private final BookService bookService; private final PersonService personService; @Autowired public BookMutation(BookService bookService, PersonService personService) { this.bookService = bookService; this.personService = personService; } public BookModel addBook(BookForm form) { PersonModel author = personService.getById(form.getAuthorId()).orElse(null); if (author == null) throw new DataNotFoundException(\"author record: not found\"); BookModel book = BookModel.builder() .author(author) .description(form.getDescription()) .publisher(form.getPublisher()) .releaseDate(Timestamp.valueOf(form.getReleaseDate().atStartOfDay())) .title(form.getTitle()) .bu","date":"2021-01-06","objectID":"/2021/01/graphql-spring-boot-relation/:0:2","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server using Spring Boot with Relational Mapping","uri":"/2021/01/graphql-spring-boot-relation/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project git@github.com:piinalpin/graphql-spring-boot.git ","date":"2021-01-06","objectID":"/2021/01/graphql-spring-boot-relation/:0:3","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server using Spring Boot with Relational Mapping","uri":"/2021/01/graphql-spring-boot-relation/"},{"categories":["Documentation"],"content":"Thankyou Medium - GraphQL server using Spring Boot, Part I Baeldung - Getting Started with GraphQL and Spring Boot Github - Team Supercharge ","date":"2021-01-06","objectID":"/2021/01/graphql-spring-boot-relation/:0:4","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server using Spring Boot with Relational Mapping","uri":"/2021/01/graphql-spring-boot-relation/"},{"categories":["Documentation"],"content":"Build simple GraphQL server using Spring Boot with MySQL database and Flyway database migration from scratch.","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"Prerequisites MySQL Database Spring Initializr GraphQL Spring Boot Starter Flyway Database Migration MySQL-Connector Lombok Annotation ","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/:0:1","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"What is GraphQL? GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. Note: This tutorial assumes that you are familiar with the Java programming language, the Spring Boot framework, and REST APIs in general. No prior GraphQL experience is required. GraphQL is a new specification (originally developed by Facebook for internal use, later open-sourced in 2015) that describes how to implement APIs. Unlike REST, which uses different endpoints to retrieve different types of data (e. g. users, comments, blog posts…), GraphQL exposes a single endpoint that receives a query from the front-end as part of the request, returning exactly the requested pieces of data in a single response. The server defines a schema describing what queries are available. Note: Unlike REST, GraphQL as a specification is not tied to the HTTP protocol, however, HTTP is most commonly used. In this case, GraphQL queries are simple HTTP GET or POST requests with special query parameters or request bodies, respectively. You can use Postman which recently received GraphQL support. ","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/:0:2","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"Step to build GraphQL server using Spring Boot Starting with spring initializr For all Spring applications, you should start with the Spring Initializr. The Initializr offers a fast way to pull in all the dependencies you need for an application and does a lot of the set up for you. And we will started with maven project. The following listing shows the pom.xml file created when you choose Maven. And here my pom.xml: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.4.1\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cgroupId\u003ecom.maverick\u003c/groupId\u003e \u003cartifactId\u003egraphiql\u003c/artifactId\u003e \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e \u003cname\u003egraphiql\u003c/name\u003e \u003cdescription\u003eDemo project for Spring Boot with GraphQL\u003c/description\u003e \u003cproperties\u003e \u003cjava.version\u003e11\u003c/java.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-jpa\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/com.graphql-java-kickstart/graphql-spring-boot-starter --\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.graphql-java-kickstart\u003c/groupId\u003e \u003cartifactId\u003egraphql-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e6.0.1\u003c/version\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/org.flywaydb/flyway-core --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.flywaydb\u003c/groupId\u003e \u003cartifactId\u003eflyway-core\u003c/artifactId\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cexcludes\u003e \u003cexclude\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003c/exclude\u003e \u003c/excludes\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/project\u003e Edit application.properties #Basic Spring Boot Config for Oracle spring.jmx.enabled=false info.app.name=GraphQL Example info.app.description=GraphQL Example info.app.version=1.0.0 management.security.enabled=false spring.http.multipart.max-file-size=10485760 spring.jackson.serialization.FAIL_ON_EMPTY_BEANS=false spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5Dialect graphql.servlet.enabled=true graphql.servlet.exception-handlers-enabled=true graphql.servlet.contextSetting= PER_REQUEST_WITH_INSTRUMENTATION spring.jpa.hibernate.use-new-id-generator-mappings=false And here my project/application.yml for external configuration of database connection, flyway migration, etc. app: name: \"Maverick GraphQL Example\" website: \"blog.piinalpin.com\" origin: \"http://localhost:3000\" upload_dir: \"./tmp/\" production: false spring: datasource: url: \"jdbc:mysql://localhost:3306/DB_NAME?allowPublicKeyRetrieval=true\u0026useSSL=false\" username: DB_USERNAME password: DB_PASSWORD driverClassName: com.mysql.cj.jdbc.Driver flyway: enabled: true baselineOnMigrate: true url: jdbc:mysql://localhost:3306/DB_NAME?allowPublicKeyRetrieval=true\u0026useSSL=false user: DB_USERNAME password: DB_PASSWORD validateOnMigrate: false server: port: 8080 Create resource/db/migration/V1_1__Initial_Commit.sql for initial migration script Edit project main application and add annotation @EnableJpaRepositories. The file should be like below. @SpringBootApplicatio","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/:0:3","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project https://github.com/piinalpin/graphql-spring-boot.git ","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/:0:4","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"Thankyou Medium - GraphQL server using Spring Boot, Part I Baeldung - Getting Started with GraphQL and Spring Boot Github - Team Supercharge ","date":"2021-01-05","objectID":"/2021/01/graphql-spring-boot/:0:5","tags":["Java","Spring Boot","GraphQL","GraphiQL","MySQL"],"title":"GraphQL Server with Spring Boot","uri":"/2021/01/graphql-spring-boot/"},{"categories":["Documentation"],"content":"How to create SQL insert generator using python with pandas and click library","date":"2020-12-12","objectID":"/2020/12/sql-generator/","tags":["SQL","MySQL","Oracle","Postgresql","MariaDB","SQL Server","Python"],"title":"SQL Insert Generator from Excel File using Python","uri":"/2020/12/sql-generator/"},{"categories":["Documentation"],"content":"Prerequisites Python Pandas Click pip install pandas click ","date":"2020-12-12","objectID":"/2020/12/sql-generator/:0:1","tags":["SQL","MySQL","Oracle","Postgresql","MariaDB","SQL Server","Python"],"title":"SQL Insert Generator from Excel File using Python","uri":"/2020/12/sql-generator/"},{"categories":["Documentation"],"content":"Step to create a SQL insert generator using python Create data in excel file, we’ll create an example data Sheet name : M_ACCOUNT ID FULLNAME ADDRESS IDENTITY_NUMBER IDENTITY_TYPE COUNTRY 1 John Doe Yogyakarta 34754354986 KTP Indonesia 2 Maverick Jakarta 43589743545 KTP Indonesia 3 Al Sah-Him Semarang 58479846645 KTP Indonesia Sheet name : M_USER ID USERNAME PASSWORD M_ACCOUNT_ID 1 johndoe $2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC 1 2 maverick $2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC 2 3 alsahhim $2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC 3 Create python file call sql_generator.py import click import errno import os import pandas as pd @click.command() @click.option('--generate', '-g', help='Change TEXT to generate excel file into SQL insert') @click.option('--outputdir', '-o', help='Change TEXT to create directory output file') def main(generate, outputdir): try: # Validate generate file can not be None if generate is None: raise TypeError # Check if outputdir is not None if outputdir != None: try: # Create a directory os.makedirs(outputdir) outputdir = \"{}/\".format(outputdir) except OSError as exc: # If directory is exists use this directory if exc.errno == errno.EEXIST: outputdir = \"{}/\".format(outputdir) file = pd.ExcelFile(generate) for sheet_name in file.sheet_names: data = file.parse(sheet_name) filename = \"{}{}.sql\".format(outputdir, sheet_name) click.echo(\"### {}:\".format(filename)) write_file = open(filename, \"w\") for i, _ in data.iterrows(): field_names = \", \".join(list(data.columns)) rows = list() for column in data.columns: rows.append(str(data[column][i])) row_values = \"'\" + \"', '\".join(rows) + \"'\" click.echo(\"INSERT INTO {} ({}) VALUES ({});\".format(sheet_name, field_names, row_values)) write_file.write(\"INSERT INTO {} ({}) VALUES ({});\\n\".format(sheet_name, field_names, row_values)) write_file.close() except TypeError as e: click.echo(\"Error: Unknown generate file! Type -h for help.\") if __name__ == \"__main__\": main() This file will create command sql_generator.py --generate filename.xlsx --outputdir dir Type sql_generator.py --help to show help command Generator will be create a sql file according sheet name File M_ACCOUNT.sql INSERT INTO M_ACCOUNT (ID, FULLNAME, ADDRESS, IDENTITY_NUMBER, IDENTITY_TYPE, COUNTRY) VALUES ('1', 'John Doe', 'Yogyakarta', '34754354986', 'KTP', 'Indonesia'); INSERT INTO M_ACCOUNT (ID, FULLNAME, ADDRESS, IDENTITY_NUMBER, IDENTITY_TYPE, COUNTRY) VALUES ('2', 'Maverick', 'Jakarta', '43589743545', 'KTP', 'Indonesia'); INSERT INTO M_ACCOUNT (ID, FULLNAME, ADDRESS, IDENTITY_NUMBER, IDENTITY_TYPE, COUNTRY) VALUES ('3', 'Al Sah-Him', 'Semarang', '58479846645', 'KTP', 'Indonesia'); File M_USER.sql INSERT INTO M_USER (ID, USERNAME, PASSWORD, M_ACCOUNT_ID) VALUES ('1', 'johndoe', '$2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC', '1'); INSERT INTO M_USER (ID, USERNAME, PASSWORD, M_ACCOUNT_ID) VALUES ('2', 'maverick', '$2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC', '2'); INSERT INTO M_USER (ID, USERNAME, PASSWORD, M_ACCOUNT_ID) VALUES ('3', 'alsahhim', '$2y$12$tRgbrmjdyytEyv8ceakIc.7vUCjLfpEi6K/Ube0hB5X4c7vPcMMQC', '3'); ","date":"2020-12-12","objectID":"/2020/12/sql-generator/:0:2","tags":["SQL","MySQL","Oracle","Postgresql","MariaDB","SQL Server","Python"],"title":"SQL Insert Generator from Excel File using Python","uri":"/2020/12/sql-generator/"},{"categories":["Documentation"],"content":"Thankyou codeburst.io - Building Beautiful Command Line Interfaces with Python $ click_ - Commands and Groups ","date":"2020-12-12","objectID":"/2020/12/sql-generator/:0:3","tags":["SQL","MySQL","Oracle","Postgresql","MariaDB","SQL Server","Python"],"title":"SQL Insert Generator from Excel File using Python","uri":"/2020/12/sql-generator/"},{"categories":["Documentation"],"content":"How to create spring batch with quartz scheduling and MySQL database from scratch.","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"Prerequisites MySQL Database Spring Initializr Spring Boot Starter Batch Spring Boot Starter Quartz MySQL-Connector Lombok Annotation ","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/:0:1","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"What is Spring Batch? Spring Batch is a lightweight, comprehensive batch framework designed to enable the development of robust batch applications vital for the daily operations of enterprise systems. Spring Batch builds upon the characteristics of the Spring Framework that people have come to expect (productivity, POJO-based development approach, and general ease of use), while making it easy for developers to access and leverage more advance enterprise services when necessary. Spring Batch is not a scheduling framework. There are many good enterprise schedulers (such as Quartz, Tivoli, Control-M, etc.) available in both the commercial and open source spaces. It is intended to work in conjunction with a scheduler, not replace a scheduler. ","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/:0:2","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"Step to create spring batch Create business data Typically, your customer or a business analyst supplies a spreadsheet. For this simple example, you can find some made-up data in src/main/resources/sample-data.csv: Maverick,24,M Al Sah-Him,21,M Felicia,24,F Jessica,22,F Calvin Joe,25,M Next, write an SQL script to create a table and store data. CREATE TABLE M_HUMAN ( id bigint auto_increment primary key, created_at timestamp default CURRENT_TIMESTAMP not null, updated_at timestamp null on update CURRENT_TIMESTAMP, deleted_at timestamp null, name varchar(255) not null, age varchar(3) not null, gender varchar(1) not null ); Starting with spring initializr For all Spring applications, you should start with the Spring Initializr. The Initializr offers a fast way to pull in all the dependencies you need for an application and does a lot of the set up for you. This example needs the Spring Batch. And we will started with maven project. The following listing shows the pom.xml file created when you choose Maven: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.4.0\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cgroupId\u003ecom.maverick\u003c/groupId\u003e \u003cartifactId\u003espring-batch-example\u003c/artifactId\u003e \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e \u003cname\u003espring-batch-example\u003c/name\u003e \u003cdescription\u003eDemo project for Spring Boot\u003c/description\u003e \u003cproperties\u003e \u003cjava.version\u003e11\u003c/java.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-batch\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.batch\u003c/groupId\u003e \u003cartifactId\u003espring-batch-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e8.0.22\u003c/version\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cversion\u003e1.18.16\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-quartz\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/project\u003e Create a model class as the following example src/main/java/com/maverick/springbatchexample/model/Person.java package com.maverick.springbatchexample.model; import lombok.AllArgsConstructor; import lombok.Builder; import lombok.Data; import lombok.NoArgsConstructor; @Data @AllArgsConstructor @NoArgsConstructor @Builder public class Person { private String name; private String age; private String gender; } Create an intermediate processor as the following example src/main/java/com/maverick/springbatchexample/processor/PersonItemProcessor.java package com.maverick.springbatchexample.processor; import com.maverick.springbatchexample.model.Person; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.batch.item.ItemProcessor; public class PersonItemProcessor implements ItemProcessor\u003cPerson, Person\u003e { private static final Logger LOG = LoggerFactory.getLogger(PersonItemProcessor.class); @Override public Person process(Person person) throws Exception { LOG.info(\"### Process: \" + person.getName()); return person; } } Create datasource configuration as the following example src/mai","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/:0:3","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project git@github.com:piinalpin/spring-batch-quartz-example.git ","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/:0:4","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"Thankyou Spring.io - Create a Batch Service ","date":"2020-12-08","objectID":"/2020/12/spring-batch-example/:0:5","tags":["Java","Spring Batch","Spring Boot","MySQL"],"title":"Simple Spring Batch with Quartz Scheduling","uri":"/2020/12/spring-batch-example/"},{"categories":["Documentation"],"content":"How to implement a custom sequence like Oracle in MySQL using a stored function and triggers","date":"2020-12-04","objectID":"/2020/12/create-sequence-like-oracle-in-mysql/","tags":["Sequence","SQL","MySQL","Nextval","Sequence MySQL","Custom Sequence"],"title":"Create Sequence Like Oracle in MySQL","uri":"/2020/12/create-sequence-like-oracle-in-mysql/"},{"categories":["Documentation"],"content":"Prerequisites MySQL ","date":"2020-12-04","objectID":"/2020/12/create-sequence-like-oracle-in-mysql/:0:1","tags":["Sequence","SQL","MySQL","Nextval","Sequence MySQL","Custom Sequence"],"title":"Create Sequence Like Oracle in MySQL","uri":"/2020/12/create-sequence-like-oracle-in-mysql/"},{"categories":["Documentation"],"content":"Step to create a custom sequence like Oracle in MySQL Set global log_bin_trust_function_creator to 1 SET GLOBAL log_bin_trust_function_creators = 1; Create a sequence table CREATE TABLE IF NOT EXISTS SEQUENCE (name VARCHAR(255) PRIMARY KEY, value INT UNSIGNED); Drop function nextval() if exists on your database DROP FUNCTION IF EXISTS nextval; Create a custom sequence call nextval('sequence_name'), and will returns the next value. If name of sequence does not exists, it will created automatically by initial value 1. DELIMITER // CREATE FUNCTION nextval (sequence_name VARCHAR(255)) RETURNS INT UNSIGNED BEGIN INSERT INTO SEQUENCE VALUES (sequence_name, LAST_INSERT_ID(1)) ON DUPLICATE KEY UPDATE value=LAST_INSERT_ID(value+1); RETURN LAST_INSERT_ID(); END // Changing back delimiter to semicolon DELIMITER ; Create table to test a custom sequence, default id defined by zero (0) CREATE TABLE IF NOT EXISTS HUMAN (id int UNSIGNED NOT NULL PRIMARY KEY DEFAULT 0, name VARCHAR(50)); Drop nextval trigger if exists. DROP TRIGGER IF EXISTS nextval; Create a custom trigger for nextval() function The trigger only generated a new id if 0 is inserted. So, if you create a new table and field id with default value by zero (0) that makes it implicit. CREATE TRIGGER nextval_human BEFORE INSERT ON HUMAN FOR EACH ROW SET new.id=IF(new.id=0,nextval('ID_HUMAN_SEQ'),new.id); Let’s try a sample data on HUMAN table INSERT INTO HUMAN (name) VALUES ('Maverick'), ('John Doe'), ('Al Sah-Him'); Inserted data look likes Let’s try another table BOOK to test a sequence CREATE TABLE IF NOT EXISTS BOOK (id int UNSIGNED NOT NULL PRIMARY KEY DEFAULT 0, name VARCHAR(50), code VARCHAR(50)); Create ID_BOOK_SEQ trigger to a new sequence CREATE TRIGGER nextval_book BEFORE INSERT ON BOOK FOR EACH ROW SET new.id=IF(new.id=0,nextval('ID_BOOK_SEQ'),new.id); Insert data into BOOK table INSERT INTO BOOK (name, code) VALUES ('Book 1', 'BK-01'), ('Book 2', 'BK-02'), ('Book 3', 'BK-03'); Show the data Let’s check generated custom sequence ","date":"2020-12-04","objectID":"/2020/12/create-sequence-like-oracle-in-mysql/:0:2","tags":["Sequence","SQL","MySQL","Nextval","Sequence MySQL","Custom Sequence"],"title":"Create Sequence Like Oracle in MySQL","uri":"/2020/12/create-sequence-like-oracle-in-mysql/"},{"categories":["Documentation"],"content":"Thankyou Open Query - Implementing Sequences using a Stored Function and Triggers ","date":"2020-12-04","objectID":"/2020/12/create-sequence-like-oracle-in-mysql/:0:3","tags":["Sequence","SQL","MySQL","Nextval","Sequence MySQL","Custom Sequence"],"title":"Create Sequence Like Oracle in MySQL","uri":"/2020/12/create-sequence-like-oracle-in-mysql/"},{"categories":["Documentation"],"content":"How to create a simple REST application using Go/Golang","date":"2020-11-20","objectID":"/2020/11/simple-rest-golang/","tags":["Golang","REST"],"title":"Simple REST Application using Go/Golang","uri":"/2020/11/simple-rest-golang/"},{"categories":["Documentation"],"content":"Introduction Go is a statically typed language, The compiler produces optimized machine code, so CPU-intensive code is significantly more efficient than languages like Python or Ruby, which have byte-code compilers and use virtual machines for execution. ","date":"2020-11-20","objectID":"/2020/11/simple-rest-golang/:0:1","tags":["Golang","REST"],"title":"Simple REST Application using Go/Golang","uri":"/2020/11/simple-rest-golang/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Golang on your device ","date":"2020-11-20","objectID":"/2020/11/simple-rest-golang/:0:2","tags":["Golang","REST"],"title":"Simple REST Application using Go/Golang","uri":"/2020/11/simple-rest-golang/"},{"categories":["Documentation"],"content":"Step to create a simple REST application using Golang Importing packages or modules Install GorillaMux for it’s efficiency and simplicity for learning go get github.com/gorilla/mux package main import ( \"encoding/json\" \"log\" \"net/http\" // Mux package for routing \"github.com/gorilla/mux\" ) Setting up our model On golang, model as known as struct like a class in Java or Python, so create a struct and name it Contact. // Create contact Model type Contact struct { Name string `json:\"name\"` Phone string `json:\"phone\"` Email string `json:\"email\"` } Setting up our utilities We will create an utilities for headers response and custom message. // Common header conforms to the http.HandlerFunc interface, and // adds the Content-Type: application/json header to each response. func CommonHeaders(handler http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") handler(w, r) } } // Ok message type OkMessage struct { Message string `json:\"message\"` } Create dummy data for contact model // Construct contacts func constructContacts() []Contact{ var contacts []Contact // Hardcoded data - @todo: add database contacts = append(contacts, Contact{Name: \"Maverick\", Phone: \"98xxx-xxxxx\", Email: \"person1@mail.com\"}) contacts = append(contacts, Contact{Name: \"Enoge\", Phone: \"96xxx-xxxxx\", Email: \"person2@mail.com\"}) contacts = append(contacts, Contact{Name: \"Martin\", Phone: \"97xxx-xxxxx\", Email: \"person3@mail.com\"}) return contacts } Create get all contacts and detail contact function // Get all contacts func getAllContacts(w http.ResponseWriter, r *http.Request) { json.NewEncoder(w).Encode(constructContacts()) } func getDetail(w http.ResponseWriter, r *http.Request) { // Get params params := mux.Vars(r) // Looping for through contacts and find one by id for _, item := range constructContacts() { if item.Name == params[\"name\"] { json.NewEncoder(w).Encode(item) return } } json.NewEncoder(w).Encode(\u0026Contact{}) } Create function to add new contact // Create contact func createContact(w http.ResponseWriter, r *http.Request) { var contact Contact _ = json.NewDecoder(r.Body).Decode(\u0026contact) json.NewEncoder(w).Encode(contact) } Create function to update contact by name // Update contact func updateContact(w http.ResponseWriter, r *http.Request) { // Get params params := mux.Vars(r) // Looping for through contacts and fine one by id for _, item := range constructContacts() { if item.Name == params[\"name\"] { var contact Contact _ = json.NewDecoder(r.Body).Decode(\u0026contact) json.NewEncoder(w).Encode(contact) return } } } Create function to delete contact by name // Delete contact func deleteContact(w http.ResponseWriter, r *http.Request) { var msg OkMessage msg.Message = \"ok\" // TODO: Add handle to filter by id json.NewEncoder(w).Encode(msg) } Create main function Firstly we initialize the router variable as r and the GorillaMux is used by calling mux.NewRouter(). Then we add all the HandleFunc() methods which a basic CRUD application will have. Here, I have used some named-parameters {name}, so that we can access the contact details using the name of the person. And serve on ports 8000. func main() { // Init router route := mux.NewRouter() route.HandleFunc(\"/contacts\", CommonHeaders(getAllContacts)).Methods(\"GET\") route.HandleFunc(\"/contacts\", CommonHeaders(createContact)).Methods(\"POST\") route.HandleFunc(\"/contacts/{name}\", CommonHeaders(getDetail)).Methods(\"GET\") route.HandleFunc(\"/contacts/{name}\", CommonHeaders(updateContact)).Methods(\"PUT\") route.HandleFunc(\"/contacts/{name}\", CommonHeaders(deleteContact)).Methods(\"DELETE\") // Start server log.Fatal(http.ListenAndServe(\":8000\", route)) } ","date":"2020-11-20","objectID":"/2020/11/simple-rest-golang/:0:3","tags":["Golang","REST"],"title":"Simple REST Application using Go/Golang","uri":"/2020/11/simple-rest-golang/"},{"categories":["Documentation"],"content":"Thankyou Medium - Create REST API in Minutes With Go/Golang Github - REST Api using GO ","date":"2020-11-20","objectID":"/2020/11/simple-rest-golang/:0:4","tags":["Golang","REST"],"title":"Simple REST Application using Go/Golang","uri":"/2020/11/simple-rest-golang/"},{"categories":["Documentation"],"content":"The usage of Java 8 Streams from creation to parallel execution","date":"2020-09-15","objectID":"/2020/09/java-stream-api/","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Prerequisites Basic knowledge of Java 8 (lambda expressions, Optional, method references) of the Stream API. This tutorial is using Gson to pretty print JSON string. ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:1","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Object and Data Sampling Create class Pojo.java public class Pojo { private String numberId; private String name; private String gender; public Pojo(String numberId, String name, String gender) { this.numberId = numberId; this.name = name; this.gender = gender; } public String getNumberId() { return numberId; } public void setNumberId(String numberId) { this.numberId = numberId; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getGender() { return gender; } public void setGender(String gender) { this.gender = gender; } } In main application, create list pojo. public static void main(String[] args) { Gson gson = new GsonBuilder().setPrettyPrinting().create(); List\u003cPojo\u003e pojoList = new ArrayList\u003cPojo\u003e(); // Define object Pojo pojo1 = new Pojo(\"1\", \"John Doe\", \"M\"); Pojo pojo2 = new Pojo(\"2\", \"Maverick\", \"M\"); Pojo pojo3 = new Pojo(\"3\", \"Natalie\", \"F\"); Pojo pojo4 = new Pojo(\"4\", \"Gracia\", \"F\"); Pojo pojo5 = new Pojo(\"5\", \"Patrick\", \"M\"); // Add to array list pojoList.add(pojo1); pojoList.add(pojo2); pojoList.add(pojo3); pojoList.add(pojo4); pojoList.add(pojo5); } The json data is : [ { \"numberId\": \"1\", \"name\": \"John Doe\", \"gender\": \"M\" }, { \"numberId\": \"2\", \"name\": \"Maverick\", \"gender\": \"M\" }, { \"numberId\": \"3\", \"name\": \"Natalie\", \"gender\": \"F\" }, { \"numberId\": \"4\", \"name\": \"Gracia\", \"gender\": \"F\" }, { \"numberId\": \"5\", \"name\": \"Patrick\", \"gender\": \"M\" } ] ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:2","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Grouping Collection Using import static java.util.stream.Collectors.groupingBy; fo grouping array list pojo. Map\u003cString, List\u003cPojo\u003e\u003e groupingPojo = pojoList.stream().collect(groupingBy(Pojo::getGender)); It will give an output : { \"F\": [ { \"numberId\": \"3\", \"name\": \"Natalie\", \"gender\": \"F\" }, { \"numberId\": \"4\", \"name\": \"Gracia\", \"gender\": \"F\" } ], \"M\": [ { \"numberId\": \"1\", \"name\": \"John Doe\", \"gender\": \"M\" }, { \"numberId\": \"2\", \"name\": \"Maverick\", \"gender\": \"M\" }, { \"numberId\": \"5\", \"name\": \"Patrick\", \"gender\": \"M\" } ] } ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:3","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Filtering Collection Let’s try filter the collection by Number ID and if not finding data then return null or you can fill new object from Pojo Pojo filterByNumber = pojoList.stream().filter(v -\u003e v.getNumberId().equalsIgnoreCase(\"2\")).findAny().orElse(null); It will give an output : { \"numberId\": \"2\", \"name\": \"Maverick\", \"gender\": \"M\" } ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:4","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Counting Collection Let’s try counting object by Gender. Long countByGender = pojoList.stream().filter(v -\u003e v.getGender().equalsIgnoreCase(\"M\")).count(); It will give an output : 3 ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:5","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Convert ArrayList to String Let’s try convert ArrayList to String and output it. String listToString = pojoList.stream().map(Pojo::getName).collect(Collectors.joining(\", \", \"\", \"\")); It will give an output : John Doe, Maverick, Natalie, Gracia, Patrick ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:6","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Returning Boolean using Any Match Let’s try using anyMatch on java Stream API. boolean findMaverick = pojoList.stream().anyMatch(v -\u003e v.getName().equalsIgnoreCase(\"Maverick\")); It will give an output : true ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:7","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Thankyou Baeldung - The Java 8 Stream API Tutorial ","date":"2020-09-15","objectID":"/2020/09/java-stream-api/:0:8","tags":["Java","Java 8 Stream API"],"title":"Java 8 Stream API Tutorial","uri":"/2020/09/java-stream-api/"},{"categories":["Documentation"],"content":"Documentation for auto start SSH Tunneling when device startup on Mac","date":"2020-09-14","objectID":"/2020/09/auto-start-ssh-tunneling-mac/","tags":["Bash","Auto Startup","SSH","Tunneling"],"title":"Auto Start SSH Tunneling on Mac","uri":"/2020/09/auto-start-ssh-tunneling-mac/"},{"categories":["Documentation"],"content":"Prerequisites Skip this step if you have it. Install Homebrew : /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Install SSH-Key using Keygen : ssh-keygen -t rsa Add SSH-Key fingerprint into the tunneling : ssh -nNt -D port username@host ","date":"2020-09-14","objectID":"/2020/09/auto-start-ssh-tunneling-mac/:0:1","tags":["Bash","Auto Startup","SSH","Tunneling"],"title":"Auto Start SSH Tunneling on Mac","uri":"/2020/09/auto-start-ssh-tunneling-mac/"},{"categories":["Documentation"],"content":"Step to auto connect SSH Tunneling Install sshpass using Homebrew brew install hudochenkov/sshpass/sshpass Create file ~/scripts/startup/startup.sh to connect ssh tunneling automatically, and type code like below. #!/bin/bash #Start SSH Tunneling on IST Yogyakarta if is not running echo \"Entering SSH Tunneling\" sshpass -p \"your_password\" ssh -nNt -D port username@host echo \"Connection closed!\" Type chmod +x ~/scripts/startup/startup.sh on your terminal to change the file can be execute. Create file ~/Library/LaunchAgents/com.startup.plist to run startup.sh automatically when the laptop is starting, and type code like below. \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e \u003cplist version=\"1.0\"\u003e \u003cdict\u003e \u003ckey\u003eEnvironmentVariables\u003c/key\u003e \u003cdict\u003e \u003ckey\u003ePATH\u003c/key\u003e \u003cstring\u003e/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:\u003c/string\u003e \u003c/dict\u003e \u003ckey\u003eLabel\u003c/key\u003e \u003cstring\u003ecom.startup\u003c/string\u003e \u003ckey\u003eProgram\u003c/key\u003e \u003cstring\u003e/Users/your_username_laptop/scripts/startup/startup.sh\u003c/string\u003e \u003ckey\u003eRunAtLoad\u003c/key\u003e \u003ctrue/\u003e \u003ckey\u003eStandardOutPath\u003c/key\u003e \u003cstring\u003e/tmp/startup.stdout\u003c/string\u003e \u003ckey\u003eStandardErrorPath\u003c/key\u003e \u003cstring\u003e/tmp/startup.stderr\u003c/string\u003e \u003c/dict\u003e \u003c/plist\u003e Create file ~/reload.sh to handle when disconnected from tunnel and type code like below. #!/bin/bash #Start SSH Tunneling on IST Yogyakarta if is not running echo \"Reloading SSH...\" launchctl unload -w ~/Library/LaunchAgents/com.startup.plist launchctl load -w ~/Library/LaunchAgents/com.startup.plist echo \"Done Reload!\" Type chmod +x ~/reload.sh on your terminal to change the file can be executed. Type launchctl load -w ~/Library/LaunchAgents/com.startup.plist to run startup agent. Setup your browser with proxy SOCKS port forwarding. If you using Chrome, you can use SOCKS plugin and forward it to tunneling port. ","date":"2020-09-14","objectID":"/2020/09/auto-start-ssh-tunneling-mac/:0:2","tags":["Bash","Auto Startup","SSH","Tunneling"],"title":"Auto Start SSH Tunneling on Mac","uri":"/2020/09/auto-start-ssh-tunneling-mac/"},{"categories":["Documentation"],"content":"Thankyou Medium - Adding Startup Scripts to Launch Daemon SSHPASS - SSHPASS non Interactive SSH Login Shell Script SSH Password ","date":"2020-09-14","objectID":"/2020/09/auto-start-ssh-tunneling-mac/:0:3","tags":["Bash","Auto Startup","SSH","Tunneling"],"title":"Auto Start SSH Tunneling on Mac","uri":"/2020/09/auto-start-ssh-tunneling-mac/"},{"categories":["Documentation"],"content":"Laravel is a web application framework with expressive, elegant syntax. This base project is to assist in the development project on the laravel based on RestFul API.","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"Features Basic Authentication Json Web Token Custom Error Handling CORS Filter Authority Access Custom Middleware Soft Deletes Service Custom Form Validation Auto Refresh Token Every 1 Hour (Ajax) User Management ","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/:0:1","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"Documentation Clone this project https://github.com/piinalpin/laravel-base.git. Change.env.docker file to change database configuration to your database configuration and add basic auth configuration. Then copy .env.docker to .env AUTH_USERNAME=\u003cBASIC_AUTH_USERNAME\u003e AUTH_PASSWORD=\u003cBASIC_AUTH_PASSWORD\u003e DB_CONNECTION=mysql DB_HOST=\u003cDATABASE_HOST\u003e DB_PORT=\u003cDATABASE_PORT\u003e DB_DATABASE=\u003cDATABASE_NAME\u003e DB_USERNAME=\u003cDATABASE_USERNAME\u003e DB_PASSWORD=\u003cDATABASE_PASSWORD\u003e Run php artisan migrate to migrate table on database. Change database/AppUserSeeder.php according what do you want (optional), for this case I have two default user. DB::table('APP_USER')-\u003einsert([ 'created_at' =\u003e DB::raw('CURRENT_TIMESTAMP'), 'created_by' =\u003e 0, 'username' =\u003e 'admin', 'full_name' =\u003e 'Administrator', 'email' =\u003e 'admin@test.com', 'password' =\u003e Hash::make('password'), 'enabled' =\u003e true, 'role' =\u003e 'ADMINISTRATOR' ]); Run php artisan db:seed to seed data in database. Run php artisan serve to run this project. ","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/:0:2","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"Run Using Docker Install Docker desktop from Docker Hub Install MySQL Docker if you want to use MySQL as container docker run -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:latest Create database and add user for laravel docker, default IP Address for docker is 172.17.0.1 shell\u003e docker exec -it mysql mysql -u root -p mysql\u003e CREATE DATABASE database_name; mysql\u003e CREATE USER 'newuser'@'172.17.0.1' IDENTIFIED BY 'user_password'; mysql\u003e GRANT ALL PRIVILEGES ON database_name.* TO 'newuser'@'172.17.0.1'; mysql\u003e FLUSH PRIVILEGES; Change .env.docker to change database connection for mysql docker DB_CONNECTION=mysql DB_HOST=172.17.0.1 DB_PORT=3306 DB_DATABASE=database_name DB_USERNAME=new_user DB_PASSWORD=user_password Run migration and seed data php artisan migrate \u0026\u0026 php artisan db:seed Build web server on docker, you can see Dockerfile for web server at web.dockerfile docker build -t laravel_web:latest -f web.dockerfile . Build laravel application on docker, you can see Dockerfile for laravel application at app.dockerfile docker build -t laravel_app:latest -f app.dockerfile . Create network on docker and create connection for mysql docker on your network. docker network create my-network docker network connect my-network mysql Adjust docker-compose.yml to run web server, application and connect to external mysql container. version: '3' services: web: image: laravel_web:latest volumes: - ./:/var/www restart: always ports: - \"8080:80\" - \"443:443\" links: - app networks: - my-network app: image: laravel_app:latest env_file: '.env.docker' environment: - \"DB_HOST=172.17.0.1\" - \"APP_URL=http://localhost:8080/api/v1\" volumes: - ./:/var/www restart: always networks: - my-network networks: my-network: external: true Run docker-compose up -d to deploy docker image to container and docker-compose down to stop it. Application should be can access at localhost:8080 ","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/:0:3","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"API Documentation Login /api/v1/oauth/token POST Header Authorization: Basic username and password as same as value of basic auth on .env Form Data username: admin and password: password User Currently Logged In /api/vi/user/me GET Header Authorization: Bearer Token Get All User /api/vi/user GET Header Authorization: Bearer Token Create New User /api/vi/user/me POST Header Authorization: Bearer Token Request: application/json { \"username\": \"someuser\", \"email\": \"someemail@test.com\", \"fullName\": \"Some Name\", \"password\": \"somepassword\", \"confirmPassword\": \"somepassword\", \"enabled\": true, \"role\": \"SOME_ROLE\" } Get Single User /api/vi/user/{id} GET Header Authorization: Bearer Token Update User /api/vi/user/{id} POST Header Authorization: Bearer Token Request: application/json { \"username\": \"someuser\", \"email\": \"someemail@test.com\", \"fullName\": \"Some Name\", \"password\": \"somepassword\", \"confirmPassword\": \"somepassword\", \"enabled\": true, \"role\": \"SOME_ROLE\" } Delete User /api/vi/user/{id} DELETE Header Authorization: Bearer Token ","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/:0:4","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"License The Laravel framework is open-sourced software licensed under the MIT license. ","date":"2020-03-06","objectID":"/2020/03/laravel-custom-base-project/:0:5","tags":["PHP","Laravel"],"title":"Laravel Custom Base Project","uri":"/2020/03/laravel-custom-base-project/"},{"categories":["Documentation"],"content":"This automation test is used as an example of testing applications automatically from https://github.com/piinalpin/FE-flask-rest-api.git making it easier for users to check whether the application is running according to business flow or not.","date":"2020-02-19","objectID":"/2020/02/automation-test-cypress/","tags":["Cypress","Automation Test","End to End Test"],"title":"Automation Test Using Cypress Example","uri":"/2020/02/automation-test-cypress/"},{"categories":["Documentation"],"content":"Prerequisites Install Cypress using NPM : cd /your/project/path npm install cypress --save-dev or using Yarn : cd /your/project/path yarn add cypress --dev ","date":"2020-02-19","objectID":"/2020/02/automation-test-cypress/:0:1","tags":["Cypress","Automation Test","End to End Test"],"title":"Automation Test Using Cypress Example","uri":"/2020/02/automation-test-cypress/"},{"categories":["Documentation"],"content":"Step to create test case Open cypress to get examples of test case from cypress module using npx : npx cypress open using yarn yarn run cypress open Cypress should be like image below Create test case data, in this case I will use my simple CRUP web app which is deployed on heroku https://github.com/piinalpin/FE-flask-rest-api.git. Create new file student-data.json at /my-project/cypress/integration/my-app. This file will be use for add new record and update record in the application. { \"student\": { \"name\": \"Kirito\", \"identityNumber\": \"2483483\" }, \"editStudent\": { \"name\": \"Kirigaya Kazuto\", \"identityNumber\": \"8374323\" } } Create test case student.spec.js inside /my-project/cypress/integration/myapp extension should be .spec.js because legible automatically on cypress as test case. Lets start with get list student. describe('Student CRUD Test', function() { // Define variable of new record and update record let student = {} let editStudent = {} // Before each will be run automatically when run a test case beforeEach(function() { // Visit website with follow url cy.visit('https://fe-flask-rest-api-maverick.herokuapp.com/') // Read sudent-data.json which is use for add record cy.readFile('cypress/integration/fe-flask-rest-api/student-data.json').its('student').then(value =\u003e { student = { name: value.name, identityNumber: value.identityNumber } }) // Read sudent-data.json which is use for update record cy.readFile('cypress/integration/fe-flask-rest-api/student-data.json').its('editStudent').then(value =\u003e { editStudent = { name: value.name, identityNumber: value.identityNumber } }) }) // Create function get list student case it('Get List Student', function() { // Find class nav-link on htmnl where text contains List Student then click it cy.get('.nav-link').contains('List Student').click() // Assertion url should be equals cy.url().should('eq', 'https://fe-flask-rest-api-maverick.herokuapp.com/#/mahasiswa') // Assertion table of list student has table\u003ethead\u003etr cy.get('table\u003ethead').should('have', 'tr') }) }) You can see on Cypress application then click Run all spec. Create case for Add Student below get list student function. Then see this run spec. You can remove all data from my web app before run spec at FE Flask Rest API it('Add New Student', function() { cy.get('.nav-link').contains('Add Student').click() cy.url().should('eq', 'https://fe-flask-rest-api-maverick.herokuapp.com/#/mahasiswa/add') cy.get('input[id=\"name\"]').type(student.name).should('have.value', student.name) cy.get('input[id=\"nim\"]').type(student.identityNumber).should('have.value', student.identityNumber) cy.get('button').contains('Submit').click() cy.get('button').contains('Yes, save it!').click() cy.get('button').contains('OK').click() cy.wait(2000) cy.url().should('eq', 'https://fe-flask-rest-api-maverick.herokuapp.com/#/mahasiswa') cy.get('table\u003etbody\u003etr').eq(0).should('contain', student.name) }) Create update test case, before you run this spec please remove all data from web application. Because cypress run all spec of student.spec.js automatically. it('Edit Student', function() { cy.get('.nav-link').contains('List Student').click() cy.url().should('eq', 'https://fe-flask-rest-api-maverick.herokuapp.com/#/mahasiswa') cy.get('button.btn.btn-warning.btn-secondary').children('i').should('have.class', 'fa-pencil').click() cy.get('input[id=\"name\"]').clear().type(editStudent.name).clear().type(editStudent.name).should('have.value', editStudent.name) cy.get('input[id=\"nim\"]').clear().type(editStudent.identityNumber).clear().type(editStudent.identityNumber).should('have.value', editStudent.identityNumber) cy.get('button').contains('Submit').click() cy.get('button').contains('Yes, save it!').click() cy.get('button').contains('OK').click() cy.wait(2000) cy.url().should('eq', 'https://fe-flask-rest-api-maverick.herokuapp.com/#/mahasiswa') cy.get('table\u003etbody\u003etr').eq(0).should('contain', editStudent.name) }) Create delete test case, like step 6th please remove all d","date":"2020-02-19","objectID":"/2020/02/automation-test-cypress/:0:2","tags":["Cypress","Automation Test","End to End Test"],"title":"Automation Test Using Cypress Example","uri":"/2020/02/automation-test-cypress/"},{"categories":["Documentation"],"content":"Thankyou Cypress - The official site of Cypress and Documentation ","date":"2020-02-19","objectID":"/2020/02/automation-test-cypress/:0:3","tags":["Cypress","Automation Test","End to End Test"],"title":"Automation Test Using Cypress Example","uri":"/2020/02/automation-test-cypress/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download then run all test case git clone https://github.com/piinalpin/cypress-example.git cd cypress-example npm i npx cypress open ","date":"2020-02-19","objectID":"/2020/02/automation-test-cypress/:0:4","tags":["Cypress","Automation Test","End to End Test"],"title":"Automation Test Using Cypress Example","uri":"/2020/02/automation-test-cypress/"},{"categories":["Documentation"],"content":"A simple tutorial for send an email using Rabbit-MQ and celery worker with Sendgrid API","date":"2020-02-12","objectID":"/2020/02/django-mail-sender/","tags":["Django","Python","Celery","RabbitMQ"],"title":"Django Mail Sender With Rabbit-MQ and Celery Worker","uri":"/2020/02/django-mail-sender/"},{"categories":["Documentation"],"content":"Prerequisites Erlang (Open Telecom Platform) RabbitMQ (Message Broker) Pyhton 3.6 (Base Compiler) Sendgrid Account (SMTP Client) Virtualenv ","date":"2020-02-12","objectID":"/2020/02/django-mail-sender/:0:1","tags":["Django","Python","Celery","RabbitMQ"],"title":"Django Mail Sender With Rabbit-MQ and Celery Worker","uri":"/2020/02/django-mail-sender/"},{"categories":["Documentation"],"content":"Step to create Django mail sender Create virtual environtment in root directory on your project and activate it virtualenv env . env/bin/activate Install Django 2.1 pip install Django==2.1.2 Create project and application in Django django-admin startproject your_project_name django-admin startapp your_apps_name Install library celery and celery-message-consumer pip install celery pip install celery-message-consumer Edit django base settings project_name/settings.py Change DEBUG to False and ALLOWED_HOST to localhost # SECURITY WARNING: don't run with debug turned on in production! DEBUG = False ALLOWED_HOSTS = ['localhost'] Add email configuration EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend' EMAIL_HOST = 'smtp.sendgrid.net' EMAIL_HOST_USER = \u003cYour_Username\u003e EMAIL_HOST_PASSWORD = \u003cYour_Password\u003e EMAIL_PORT = 587 EMAIL_USE_TLS = True DEFAULT_FROM_EMAIL = \"Info KS-Linux UAD \u003cinfo@kslinux.tif.uad.ac.id\u003e\" Add RabbitMQ configuration # RabbitMQ Configuration RABBIT_HOST = \"localhost\" RABBIT_PORT = \"5672\" RABBIT_VIRTUAL_HOST = \"/\" RABBITMQ_ROUTING_KEY = \"mail_consumer\" # RabbitMQ Credentials RABBIT_USERNAME = \"guest\" RABBIT_PASSWORD = \"guest\" Add application at configuration INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'django', 'mail_consumer', ] Create logging configuration and create logs directory # Logging Configuratino LOGGING = { 'version': 1, 'disable_existing_loggers': False, 'filters': { 'require_debug_false': { '()': 'django.utils.log.RequireDebugFalse' }, 'require_debug_true': { '()': 'django.utils.log.RequireDebugTrue' } }, 'formatters': { 'main_formatter': { 'format': '%(levelname)s:%(name)s: %(message)s ' '(%(asctime)s; %(filename)s:%(lineno)d)', 'datefmt': \"%Y-%m-%d %H:%M:%S\", }, }, 'handlers': { 'mail_admins': { 'level': 'ERROR', 'filters': ['require_debug_false'], 'class': 'django.utils.log.AdminEmailHandler' }, 'console': { 'level': 'DEBUG', 'filters': ['require_debug_true'], 'class': 'logging.StreamHandler', 'formatter': 'main_formatter', }, 'production_file': { 'level': 'INFO', 'class': 'logging.handlers.RotatingFileHandler', 'filename': 'logs/main.log', 'maxBytes': 1024 * 1024 * 5, # 5 MB 'backupCount': 7, 'formatter': 'main_formatter', 'filters': ['require_debug_false'], }, 'debug_file': { 'level': 'DEBUG', 'class': 'logging.handlers.RotatingFileHandler', 'filename': 'logs/main_debug.log', 'maxBytes': 1024 * 1024 * 5, # 5 MB 'backupCount': 7, 'formatter': 'main_formatter', 'filters': ['require_debug_true'], }, 'null': { \"class\": 'logging.NullHandler', } }, 'loggers': { 'django.request': { 'handlers': ['mail_admins', 'console'], 'level': 'ERROR', 'propagate': True, }, 'django': { 'handlers': ['null', ], }, 'py.warnings': { 'handlers': ['null', ], }, '': { 'handlers': ['console', 'production_file', 'debug_file'], 'level': \"DEBUG\", }, } } Create queue name or exchange to RabbitMQ and Celery by-pass log # ADD CELERY BYPASS LOG CELERYD_HIJACK_ROOT_LOGGER = False # CREATE QUEUE TO RABBITMQ EXCHANGES = { # a reference name for this config, used when attaching handlers 'default': { 'name': 'data', # actual name of exchange in RabbitMQ 'type': 'mail_consumer', # an AMQP exchange type }, } Create function send in app_name/views.py from trinity import settings from django.core.mail import EmailMultiAlternatives def sender(data): try: subject = data['subject'] body = data['text'] from_email = settings.DEFAULT_FROM_EMAIL to = data['to'] html_body = data['html'] messages = EmailMultiAlternatives(subject, body, from_email, to) messages.attach_alternative(html_body, \"text/html\") messages.attach_file(data['file'], 'image/jpg') messages.send() print(\"Email has been sent!\") except: print(\"Can not sent email, something wrong!\") Create task listener to RabbitMQ queue app_name/tasks.py import json from django.conf import settings from event_consumer import message_handler @message_ha","date":"2020-02-12","objectID":"/2020/02/django-mail-sender/:0:2","tags":["Django","Python","Celery","RabbitMQ"],"title":"Django Mail Sender With Rabbit-MQ and Celery Worker","uri":"/2020/02/django-mail-sender/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Django 2 - The web framework used Virtualenv - The virtual environment used Celery - Celery worker to create connection with RabbitMQ ","date":"2020-02-12","objectID":"/2020/02/django-mail-sender/:0:3","tags":["Django","Python","Celery","RabbitMQ"],"title":"Django Mail Sender With Rabbit-MQ and Celery Worker","uri":"/2020/02/django-mail-sender/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project git clone https://github.com/piinalpin/trinity.git ","date":"2020-02-12","objectID":"/2020/02/django-mail-sender/:0:4","tags":["Django","Python","Celery","RabbitMQ"],"title":"Django Mail Sender With Rabbit-MQ and Celery Worker","uri":"/2020/02/django-mail-sender/"},{"categories":["Documentation"],"content":"This tutorial will consume TwitterAPI to get Twitter Trendings with [Tweepy](https://www.tweepy.org/) Library","date":"2020-02-04","objectID":"/2020/02/get-twitter-trending/","tags":["Jupyter Notebook","Python","Anaconda","Data Mining"],"title":"Get Twitter Trending with World of Earth Identity (WOEID)","uri":"/2020/02/get-twitter-trending/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Python 3 or Anaconda on your device. ","date":"2020-02-04","objectID":"/2020/02/get-twitter-trending/:0:1","tags":["Jupyter Notebook","Python","Anaconda","Data Mining"],"title":"Get Twitter Trending with World of Earth Identity (WOEID)","uri":"/2020/02/get-twitter-trending/"},{"categories":["Documentation"],"content":"Step to create TwitterAPI Consumer Install Tweepy pip install tweepy Create an App on Twitter Developer to get API Key, then find apiKey, apiSecretKey, accessToken, and accessTokenSecret on tabs Keys and tokens Create custom library TwitterTrendingsAPI.py import json import tweepy class TwitterTrendingsAPI: \"\"\" How to use? Change apiKey, apiSecretKey, accessToken, accessTokenSecret with your own Twitter Apps from https://developer.twitter.com Use this class api = TwitterAPI(woeId=23424846, limit=10) api.getTrending() \"\"\" def __init__(self, woeId, limit): self.apiKey = \"\u003cYOUR_API_KEY\u003e\" self.apiSecretKey = \"\u003cYOUR_API_SECRET_KEY\u003e\" self.accessToken = \"\u003cYOUR_ACCESS_TOKEN\u003e\" self.accessTokenSecret = \"\u003cYOUR_ACCESS_TOKEN_SECRET\u003e\" self.woeId = woeId self.limit = limit def getApiAuth(self): auth = tweepy.OAuthHandler(self.apiKey, self.apiSecretKey) auth.set_access_token(self.accessToken, self.accessTokenSecret) return tweepy.API(auth) def getTrending(self): trends = self.getApiAuth().trends_place(self.woeId) trending = json.loads(json.dumps(trends, indent=1)) return trending[0][\"trends\"][:self.limit] Create python file or jupyter notebook file to call these custom library which is already created # Import custom twitter trendings API from TwitterTrendingsAPI import TwitterTrendingsAPI # Define WOEID \"\"\" More World of Earth Id : https://codebeautify.org/jsonviewer/f83352 Find WOEID by your self \"\"\" INDONESIA_WOE_ID = 23424846 UNITED_KINGDOM_WOE_ID = 23424975 # Define TwitterTrendingsAPI Object with Limit 10 Trendings api = TwitterTrendingsAPI(woeId=INDONESIA_WOE_ID, limit=10) # Get Trendings trendings = api.getTrending() print(trendings) # Filter Trendings By Hashtag trends = list() for trend in trendings: getTrendName = trend[\"name\"].strip(\"#\") trends.append(getTrendName) print(trends) ","date":"2020-02-04","objectID":"/2020/02/get-twitter-trending/:0:2","tags":["Jupyter Notebook","Python","Anaconda","Data Mining"],"title":"Get Twitter Trending with World of Earth Identity (WOEID)","uri":"/2020/02/get-twitter-trending/"},{"categories":["Documentation"],"content":"Conclusion This tutorial can be use for data mining or machine learning, what is trending topic now? ","date":"2020-02-04","objectID":"/2020/02/get-twitter-trending/:0:3","tags":["Jupyter Notebook","Python","Anaconda","Data Mining"],"title":"Get Twitter Trending with World of Earth Identity (WOEID)","uri":"/2020/02/get-twitter-trending/"},{"categories":["Documentation"],"content":"This project will create search engine bot on telegram using TF IDF and Cosine Similarity","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Getting Started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system. ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:1:0","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Python 3 on your device ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:1:1","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"File Structure bot.py this file to serve get updates and send message from request config.cfg token telegram bot from Bot Father database.py define database structure with object oriented mapping search_engine.py custom library to get result of cosine similarity document server.py serve message and result from engine tensor_flow.py natural language processing for greeting response intents.json greeting or intents json data ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:1:2","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Step for get dataset Scraping data from http://digilib.uad.ac.id/penelitian/Penelitian/index see on Scrapping.ipynb Update dataset ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:1:3","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"How To Run Install requirement pip install -r requirements.txt Install punkt with nltk.download() \u003e\u003e\u003e import nltk \u003e\u003e\u003e nltk.download(\"punkt\") Run server.py python server.py Go to Telegram Application or access from Telegram Web, then chat with this bot. ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:1:4","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Virtualenv - The virtual environment used SQL Alchemy - The database library NLTK - Natural Language Toolkit Tensor Flow - Tensor Flow Tf Learn - Tensor flow for learning Scikit Learn - Scikit Learn use Cosine Similarity ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:2:0","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project \u003e Clone : git clone https://github.com/piinalpin/research-references-bot.git ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:3:0","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Chat with Him Telegram Bot Chat : https://t.me/research_references_bot LINE Bot Chat : http://line.me/ti/p/@437nryhw ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:4:0","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"Authors Alvinditya Saputra - LinkedIn Instagram Twitter ","date":"2020-01-17","objectID":"/2020/01/research-reference-bot/:5:0","tags":["Python","Chat Bot","NLP","Machine Learning","Tensorflow","TF-IDF","Cosine Similarity"],"title":"Research Reference Bot","uri":"/2020/01/research-reference-bot/"},{"categories":["Documentation"],"content":"This project will create CRUD application with Object Relational Mapping on MySQL using flask and sqlalchemy.","date":"2019-12-26","objectID":"/2019/12/flask-mysql/","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system. ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:0:0","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Python 3 on your device ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:1:0","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Project structure * flask-mysql/ |--- app/ | |--- config/ | | |--- __init__.py | | |--- Database.py | |--- controller/ | | |--- __init__.py | | |--- CollegerController.py | | |--- CoursesController.py | | |--- Main.py | | |--- TakeCourseController.py | |--- model/ | | |--- CollegerModel.py | | |--- CoursesModel.py | | |--- TakeCourseModel.py | |--- templates/ | |--- __init__.py |--- run.py ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:1:1","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Step to create this project A step by step series of examples that tell you how to get a development env running Install virtual environment if you dont have virtual environment pip install virtualenv Create virtual environment and activate inside your flask-rest-api directory according the above structure virtualenv venv \u003e On windows -\u003e venv\\Scripts\\activate \u003e On linux -\u003e . env/bin/activate Install some third party libraries on your virtual environment with pip pip install flask flask-sqlalchemy flask-migrate mysql-connector-python Install MySQL database if you don’t have, but if you have MySQL you can skip this step Create user and grant privilege for user was created mysql\u003e CREATE USER 'newuser'@'localhost' IDENTIFIED BY 'password'; mysql\u003e GRANT ALL PRIVILEGES ON * . * TO 'newuser'@'localhost'; mysql\u003e FLUSH PRIVILEGES; Create database on MySQL mysql\u003e CREATE DATABASE YOUR_DATABASE_NAME Create project_name/run.py directory inside flask-project according the above structure from app import app if __name__ == \"__main__\": app.run(host=\"localhost\", port=5000, debug=True) Create project_name/app/config/Database.py to create configuration for database class DbConfig: def __init__(self): self.DB_USERNAME = \"\u003cYOUR_USERNAME\u003e\" self.DB_PASSWORD = \"\u003cYOUR_PASSWORD\u003e\" self.DB_HOST = \"localhost\" self.DB_PORT = 3306 self.DB_NAME = \"\u003cYOUR_DATABASE_NAME\u003e\" def getUri(self): return \"mysql+mysqlconnector://{}:{}@{}:{}/{}\".format(self.DB_USERNAME, self.DB_PASSWORD, self.DB_HOST, self.DB_PORT, self.DB_NAME) Create project_name/app/__init__.py inside app directory according the above structure project_name/app/. This step will setup for SQLAlchemy config. from flask import Flask from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate from app.config.Database import DbConfig app = Flask(__name__) app.config[\"SQLALCHEMY_DATABASE_URI\"] = DbConfig().getUri() app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False db = SQLAlchemy(app) migrate = Migrate(app, db) Define colleger model to application and create database migration, create python file on app/model/ you can see defined model on here Update app/__init__.py should like as follows from flask import Flask from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate from app.config.Database import DbConfig app = Flask(__name__) app.config[\"SQLALCHEMY_DATABASE_URI\"] = DbConfig().getUri() app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False db = SQLAlchemy(app) from app.model import CoursesModel, CollegerModel, TakeCourseModel migrate = Migrate(app, db) Run migration with flask-migrate, type in terminal as below flask db init flask db migrate flask db upgrade The structure of database should like as follows Create controller to application, create python file on app/controller/ you can see defined controller here Update app/__init__.py add this import into end line of file, this step to import the controller from app.controller.Main import * from app.controller.CollegerController import * from app.controller.CoursesController import * from app.controller.TakeCourseController import * Create templates to application on app/templates/ you can see defined templates here Then, you can run this application by terminal python run.py Homepage Colleger Page Courses Page Take Course Page ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:1:2","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Flask - The web framework used Flask Migrate - The database migration Virtualenv - The virtual environment used SQL Alchemy - The database library Flask-SQLAlchemy - Flask and SQL Alchemy connector MySQL Connector Python - Connector MySQL for Python MySQL - MySQL Database ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:2:0","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project \u003e Clone : git clone https://github.com/piinalpin/flask-mysql.git ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:3:0","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Authors Alvinditya Saputra - LinkedIn Instagram Twitter ","date":"2019-12-26","objectID":"/2019/12/flask-mysql/:4:0","tags":["Python","Flask","MySQL"],"title":"Simple Flask App with Mysql","uri":"/2019/12/flask-mysql/"},{"categories":["Documentation"],"content":"Tutorial for building website application with Django and SQLite (default django database)","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Codename : Rattlesnake ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:0:0","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Python 3 and virtual environment on your device ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:1:0","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Project structure File structure in django by default has a structure like below * django-crud-sqlite/ |--- rattlesnake/ | |--- app/ | | |--- migrations/ | | |--- templates/ | | |--- __init__.py | | |--- admin.py | | |--- apps.py | | |--- models.py | | |--- tests.py | | |--- views.py | |--- rattlesnake/ | | |--- __init__.py | | |--- settings.py | | |--- urls.py | | |--- wsgi.py | |--- manage.py |--- venv/ ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:1:1","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Step to create django crud A step by step series of examples that tell you how to get a development env running Create virtual environment and activate inside your django-crud-sqlite/ directory according the above structure virtualenv venv \u003e On windows -\u003e venv\\Scripts\\activate \u003e On linux -\u003e . env/bin/activate Install django and start new project inside your django-crud-sqlite/ directory according the above structure pip install django django-admin startproject rattlesnake cd rattlesnake Create new app, from rattlesnake/ directory will create create new app/ to store the collection \u003e On Windows -\u003e manage.py startapp app \u003e On Linux, etc -\u003e ./manage.py startapp app Register your app into rattlesnake project, the app to INSTALLED_APP in rattlesnake/settings.py INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', : 'app', : ] Create the model to define the table structure of database and save the collection into database app/models.py from django.db import models from django.urls import reverse # Create your models here. class Student(models.Model): name = models.CharField(max_length=200, null=False) identityNumber = models.CharField(max_length=200, null=False) address = models.CharField(max_length=200, null=True) department = models.CharField(max_length=200, null=True) def __str__(self): return self.name # The absolute path to get the url then reverse into 'student_edit' with keyword arguments (kwargs) primary key def get_absolute_url(self): return reverse('student_edit', kwargs={'pk': self.pk}) Every after change models.py you need to make migrations into db.sqlite3 (database) to create the table for the new model manage.py makemigrations manage.py migrate Create the views to create app pages on browser, the file is app/views.py according the above structure from django.http import HttpResponse from django.shortcuts import render from django.views.generic import ListView, DetailView from django.views.generic.edit import CreateView, UpdateView, DeleteView from django.urls import reverse_lazy from .models import Student # Create your views here. class StudentList(ListView): model = Student class StudentDetail(DetailView): model = Student class StudentCreate(CreateView): model = Student # Field must be same as the model attribute fields = ['name', 'identityNumber', 'address', 'department'] success_url = reverse_lazy('student_list') class StudentUpdate(UpdateView): model = Student # Field must be same as the model attribute fields = ['name', 'identityNumber', 'address', 'department'] success_url = reverse_lazy('student_list') class StudentDelete(DeleteView): model = Student success_url = reverse_lazy('student_list') Then, create file app/urls.py to define app url path (in CI as same as route function) from django.urls import path from . import views urlpatterns = [ path('', views.StudentList.as_view(), name='student_list'), path('view/\u003cint:pk\u003e', views.StudentDetail.as_view(), name='student_detail'), path('new', views.StudentCreate.as_view(), name='student_new'), path('edit/\u003cint:pk\u003e', views.StudentUpdate.as_view(), name='student_edit'), path('delete/\u003cint:pk\u003e', views.StudentDelete.as_view(), name='student_delete'), ] The app/urls.py would not work unless you include that into the main url rattlesnake/urls.py from django.contrib import admin from django.urls import path, include urlpatterns = [ : path('student/', include('app.urls')), : ] Create the html file to display user interface, you need create directory app/templates/app/ like below * django-crud-sqlite/ |--- rattlesnake/ | |--- app/ | | |--- migrations/ | | |--- templates/ | | | |--- app/ | | |--- __init__.py | | |--- admin.py | | |--- apps.py | | |--- models.py | | |--- tests.py | | |--- views.py | |--- rattlesnake/ | | |--- __init__.py | | |--- settings.py | | |--- urls.py | | |--- wsgi.py | |--- manage.py |--- venv/ Create file app/templates/app/student_list.html to display or parsing student list data with ListView library Create file app/templates/app/student_","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:1:2","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"After change structure of flask project * django-crud-sqlite/ |--- rattlesnake/ | |--- app/ | | |--- migrations/ | | |--- templates/ | | | |--- app/ | | | | |--- student_confirm_delete.html | | | | |--- student_detail.html | | | | |--- student_form.html | | | | |--- student_list.html | | |--- __init__.py | | |--- admin.py | | |--- apps.py | | |--- models.py | | |--- tests.py | | |--- urls.py | | |--- views.py | |--- rattlesnake/ | | |--- __init__.py | | |--- settings.py | | |--- urls.py | | |--- wsgi.py | |--- db.sqlite3 | |--- manage.py |--- venv/ ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:1:3","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Django 2 - The web framework used Virtualenv - The virtual environment used SQLite 3 - The database library ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:2:0","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project \u003e Clone : git clone https://github.com/piinalpin/django-crud-sqlite.git ","date":"2019-12-24","objectID":"/2019/12/django-crud-sqlite/:3:0","tags":["Django","Python","SQLite"],"title":"Simple Django CRUD App With SQLite","uri":"/2019/12/django-crud-sqlite/"},{"categories":["Story"],"content":"A few stories about my experience while being a speaker.","date":"2019-12-24","objectID":"/2019/12/my-first-workshop/","tags":["Workshop","Sharing"],"title":"My First Workshop","uri":"/2019/12/my-first-workshop/"},{"categories":["Story"],"content":"A few stories about my experience while being a speaker in the Asynchronous Programming and the Build CRUD with Flask workshop at some time ago. As an introduction, I like to share my experience with others. So I want to make a workshop with the purpose of so I can share my experience with others and also when it coincides with my return to the hometown although only briefly, so I think I’m not only go home only, but I can sharing with each other. At first I was confused by whom I would share my experience, then I contacted the Linux Study Group at the University of Ahmad Dahlan which became the beginning of my knowledge of information technology and some technologies in Programming language. At that time I chose the theory about Asynchrounous Programming because during my lecture I did not get that knowledge. The response from friends was very positive so it was very fun at the time. Then after the workshop, a few months later I wanted to hold an activity again with different theory. The target is the friends who are working on the final task with the topic Data Mining and machine learning. Most use the Python programming language, so I want to share about how to create an application that can create, read, update and delete with the microframework flask. And the workshop this time is not less exciting with the previous, because the enthusiasm of friends is very high. So it starts from this where I learn not to fear doing positive things even in the slightest way. Although not an international workshop but I am very pleased to be able to share my experience to others. ","date":"2019-12-24","objectID":"/2019/12/my-first-workshop/:0:0","tags":["Workshop","Sharing"],"title":"My First Workshop","uri":"/2019/12/my-first-workshop/"},{"categories":["Documentation"],"content":"Tutorial for building Create, Read, Update and Delete using REST Full API with Flask and SQLAlchemy","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"How To Build Make sure you have installed Python 3 on your device ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:1:0","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Project structure * flask-rest-api/ |--- app/ | |--- module/ | | |--- __init__.py | | |--- const.py | | |--- controller.py | | |--- models.py | |--- __init__.py |--- venv/ |--- run.py ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:1:1","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Step to create flask rest api A step by step series of examples that tell you how to get a development env running Install virtual environment pip install virtualenv Create virtual environment and activate inside your flask-rest-api directory according the above structure virtualenv venv \u003e On windows -\u003e venv\\Scripts\\activate \u003e On linux -\u003e . env/bin/activate Install some third party librares on your virtual environment with pip pip install flask sqlalchemy flask-sqlalchemy flask-migrate Create run.py directory inside flask-project according the above structure from app import app app.run(debug=True, host='127.0.0.1', port=5000) Create controller.py according the abpove structure flask-rest-api/app/module/ from flask import request, jsonify from app import app @app.route('/') def index(): return \"\u003ch1\u003eWelcome to Flask Restful API\u003c/h1\u003e\u003cp\u003eCreated By: Alvinditya Saputra\u003c/p\u003e\" Create __init__.py inside app directory according the above structure flask-rest-api/app/ from flask import Flask app = Flask(__name__) from app.module.controller import * Run first this application to make sure can running with terminal or command promt python run.py Access localhost:5000 according port that created in run.py Configure the database with SQLAlchemy, you should create directory db/ inside app/ directory and modify __init__.py and it will be created flask-api.db inside app directory * flask-rest-api/ |--- app/ | |--- db/ | |--- module/ | | |--- __init__.py | | |--- controller.py | | |--- models.py | |--- __init__.py |--- venv/ |--- run.py import os from flask import Flask project_dir = os.path.dirname(os.path.abspath(__file__)) database_file = \"sqlite:///{}\".format(os.path.join(project_dir, \"db/flask-api.db\")) app = Flask(__name__) app.config[\"SQLALCHEMY_DATABASE_URI\"] = database_file app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = True from app.module.controller import * Define model to application and create database migration, you should create models.py file inside module directory according the above structure. from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate from app import app db = SQLAlchemy(app) migrate = Migrate(app, db) class Mahasiswa(db.Model): __tablename__ = 'mahasiswa' #Must be defined the table name id = db.Column(db.Integer, unique=True, primary_key=True, nullable=False) nim = db.Column(db.String, nullable=False) name = db.Column(db.String, nullable=False) def __init__(self, nim, name): self.nim = nim self.name = name def __repr__(self): return \"\u003cName: {}, Nim: {}\u003e\".format(self.name, self.nim) def save(self): db.session.add(self) db.session.commit() @staticmethod def getAll(): students = Mahasiswa.query.all() result = [] for student in students: obj = { 'id': student.id, 'nim': student.nim, 'name': student.name } result.append(obj) return result def delete(self): db.session.delete(self) db.session.commit() Run migration with flask-migrate, type in terminal as below flask db init flask db migrate flask db upgrade The structure of database should like as follows Mahasiswa id (Integer, PK, Autoincrement, NOT NULL) name (String, NOT NULL) nim (String, NOT NULL) Create constant class to define constant variable for example variable to HTTP status, you should create file const.py inside app/module/ according the above structure class HttpStatus: OK = 200 CREATED = 201 NOT_FOUND = 404 BAD_REQUEST = 400 The structure project will be look as follows * flask-rest-api/ |--- app/ | |--- db/ | |--- module/ | | |--- __init__.py | | |--- const.py | | |--- controller.py | | |--- models.py | |--- __init__.py |--- venv/ |--- run.py Import database from models.py and constant class const.py add this line from .models import * and from .const import * to the controller.py, it’s mean import all class, function or variables from models.py and const.py Create function to get data from Http Request GET to retrieve all data from database with endpoint /mahasiswa @app.route('/api/v1/mahasiswa', methods=['GET', 'POST']) def mahasisw","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:1:2","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"After change structure of flask project * flask-rest-api/ |--- app/ | |--- db/ | | |--- flask-api.db | |--- module/ | | |--- __init__.py | | |--- const.py | | |--- controller.py | | |--- models.py | |--- __init__.py |--- migrations/ |--- venv/ |--- run.py ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:1:3","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Want to demo online? Backend Flask REST API ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:1:4","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Flask - The web framework used Flask Migrate - The database migration Virtualenv - The virtual environment used SQL Alchemy - The database library Flask-SQLAlchemy - Flask and SQL Alchemy connector ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:2:0","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project \u003e Clone : git clone https://github.com/piinalpin/flask-rest-api.git ","date":"2019-12-24","objectID":"/2019/12/flask-rest-api/:3:0","tags":["Flask","RestFul","Python","SQL-Alchemy"],"title":"Simple Build Flask Restful Based","uri":"/2019/12/flask-rest-api/"},{"categories":["Documentation"],"content":"Tutorial for building Create, Read, Update and Delete Website Application With Flask and SQLAlchemy","date":"2019-12-24","objectID":"/2019/12/flask-crud/","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Prerequisites Make sure you have installed Python 3 on your device ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:1","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Project structure * flask-project/ |--- app/ | |--- module/ | | |--- __init__.py | | |--- controller.py | | |--- models.py | |--- templates/ (html file) | |--- __init__.py |--- venv/ |--- run.py ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:2","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Step to create flask crud A step by step series of examples that tell you how to get a development env running Install virtual environment pip install virtualenv Create virtual environment and activate inside your flask-crud directory according the above structure virtualenv venv \u003e On windows -\u003e venv\\Scripts\\activate \u003e On linux -\u003e . env/bin/activate Install some third party librares on your virtual environment with pip pip install flask sqlalchemy flask-sqlalchemy Create run.py directory inside flask-project according the above structure from app import app app.run(debug=True, host='127.0.0.1', port=5000) Create controller.py according the abpove structure flask-crud/app/module/ from flask import render_template, request from app import app @app.route('/') def index(): return \"My CRUD Flask App\" Create __init__.py inside app directory according the above structure flask-crud/app/ from flask import Flask app = Flask(__name__) from app.module.controller import * Run first this application to make sure can running with terminal or command promt python run.py Access localhost:5000 according port that created in run.py Create an input form called home.html inside templates directory according the above structure \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eFlask Crud\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch3\u003eForm Add Mahasiswa\u003c/h3\u003e \u003cform action=\"/\" method=\"POST\"\u003e \u003ctable\u003e \u003ctr\u003e \u003ctd\u003eNama Lengkap\u003c/td\u003e \u003ctd\u003e:\u003c/td\u003e \u003ctd\u003e\u003cinput type=\"text\" name=\"name\"\u003e\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eNomor Induk Mahasiswa\u003c/td\u003e \u003ctd\u003e:\u003c/td\u003e \u003ctd\u003e\u003cinput type=\"text\" name=\"nim\"\u003e\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e\u003cbutton type=\"submit\"\u003eSave\u003c/button\u003e\u003c/td\u003e \u003c/tr\u003e \u003c/table\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e Change return \"My CRUD Flask App\" in controller.py to return render_template(\"home.html\") from flask import render_template, request from app import app @app.route('/') def index(): return render_template(\"home.html\") Then modify function index() to accept method POST request @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': print(request.form) return render_template(\"home.html\") Configure the database with SQLAlchemy, you should modify __init__.py and it will be created flaskcrud.db inside app directory import os from flask import Flask project_dir = os.path.dirname(os.path.abspath(__file__)) database_file = \"sqlite:///{}\".format(os.path.join(project_dir, \"flaskcrud.db\")) app = Flask(__name__) app.config[\"SQLALCHEMY_DATABASE_URI\"] = database_file app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = True from app.module.controller import * Define model to application, you should create models.py file inside module directory according the above structure. from flask_sqlalchemy import SQLAlchemy from app import app db = SQLAlchemy(app) class Mahasiswa(db.Model): id = db.Column(db.Integer, unique=True, primary_key=True, nullable=False) nim = db.Column(db.String, nullable=False) name = db.Column(db.String, nullable=False) def __repr__(self): return \"\u003cName: {}\u003e\".format(self.name) The structure of database should like as follows Mahasiswa id (Integer, PK, Autoincrement, NOT NULL) name (String, NOT NULL) nim (String, NOT NULL) Stop app if that is still running, press CTRL+C key to quit and type python to go to python terminal Type command bellow to create database file flaskcrud.db \u003e\u003e\u003e from app.module.models import db \u003e\u003e\u003e db.create_all() \u003e\u003e\u003e exit() The structure project will be look as follows * flask-project/ |--- app/ | |--- module/ | | |--- __init__.py | | |--- controller.py | | |--- models.py | |--- templates/ (html file) | |--- __init__.py | |--- flaskcrud.db |--- venv/ |--- run.py Import database from models.py add this line from .models import db, Mahasiswa to the controller.py, it’s mean import from models.py for db variable and class Mahasiswa Modify controller.py to create function to storing data of Mahasiswa then save to the database that is already made and retrieving data with Mahasiswa.query.all() it will be retrieving all data from database then made with tr","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:3","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"After change structure of flask project * flask-project/ |--- app/ | |--- module/ | | |--- __init__.py | | |--- controller.py | | |--- models.py | |--- templates/ | | |--- form-update.html | | |--- home.html | |--- __init__.py | |--- flaskcrud.db |--- venv/ |--- run.py ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:4","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Built With Python 3 - The language programming used Flask - The web framework used Virtualenv - The virtual environment used SQL Alchemy - The database library Flask-SQLAlchemy - Flask and SQL Alchemy connector ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:5","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Want to demo online? Flask Crud With SQL Alchemy Built in Python 3 ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:6","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Clone or Download You can clone or download this project \u003e Clone : git clone https://github.com/piinalpin/flask-crud.git ","date":"2019-12-24","objectID":"/2019/12/flask-crud/:0:7","tags":["Flask","Python","SQL-Alchemy"],"title":"Simple CRUD App with Flask And SQL-Alchemy","uri":"/2019/12/flask-crud/"},{"categories":["Documentation"],"content":"Collection of Any Research of Jupyter Notebook Sampling on Data and Text Mining","date":"2019-08-25","objectID":"/2019/08/research-collection/","tags":["Jupyter Notebook","Python","Anaconda"],"title":"List Research Which Used Python","uri":"/2019/08/research-collection/"},{"categories":["Documentation"],"content":"List of Research Search The Mpasi Menu with The Dice Coefficient Method (Luthvi Rizkawati, 2019) Data Mining Analysis on Pharmacy Transaction Data Uses The Eclat Algorithm (Utami Merdekawati, 2019) Clustering Times Series Student Portal Data Logs (Siti Aniszah, 2019) Grouping Trend Research Title Using K-Means Clustering (Nila Hutami Putri, 2018) Classification of Lecturers’ Research Titles Using the K-Medoid Method (Ade Antika, 2019) Grouping of Waste Banks Using the K-Medoid Method (Mugi Astuti, 2019) Sentiment Analysis Index Performance of Lecturer Using Multinomial Naive Bayes (Alvinditya Saputra, 2018) Grouping Topic Titles Trend Using The Single Linkage Method with Manhattan Distance Similarity (Tsani Elvia Nita, 2018) Would you want to contribute? Contact me on email alvindityas@gmail.com ","date":"2019-08-25","objectID":"/2019/08/research-collection/:0:0","tags":["Jupyter Notebook","Python","Anaconda"],"title":"List Research Which Used Python","uri":"/2019/08/research-collection/"},{"categories":null,"content":" A Backend Developer | DevOps Enthusiast | Java • Golang • Python | A Competitive Programmer  \r \r \rAbout\r💡 Like to explore new technologies and develope software 🔭 Have a high curiosity and easily adapt to new environments 🍀 Currently interested in DevOps tools and server automation provisioning 📔 In my free time, I usually use it for research and write documentation in my blog 💬 Ask me about Software Development / Microservices 🤔 I’m looking for help with how to excel in Competitive Programming ⚡ Fun fact: I like custom motorcycle Experiences\r💼 Software Developer | Mandrill Tech Sdn Bhd (March 2023 - present) Develope and enhancement existing application as a Backend Writing API Documentation and specification Involved in CO-OP Bank Pertama for financing origination system 👷 Middle Java Developer | PT. Infosys Solusi Terpadu (January 2020 - April 2023) Lead in the development and implementation of web services Designing microservice api backends for new or existing projects Delegate tasks to developer members and provide suggestions and all aspects of the project Supervise member-written source code assessment Develope application of CIMB Niaga such as Octo Mobile, Octo Clicks and ATM Business Develope some feature of Victoria Bank (Mobile Banking) 👨🏻‍🏫 Mentor of Backend Java Spring Boot [Freelance] | PT. Rahwana Teknologi Nusantara (January 2023 - March 2023) Guiding, teaching, provide direction and give feedback to mentee Teaches how to be a backend java developer Assess and provide feedback for all tasks. Answering mentee questions both inside and outside class sessions. 👷 Golang Developer [Freelance] | PT. Rahwana Teknologi Nusantara (October 2022 - March 2023) Enhance and develop for existing ADSG Peruri project using Golang (Echo Framework) Create technical documentation and api documentation 👨🏻‍🏫 Mentor Java Bootcamp [Kampus Merdeka] | Alterra Academy (February 2020 - July 2022) Guiding, teaching, and providing direction and feedback to mentees Assess and provide feedback for all tasks Answering mentee questions both inside and outside class sessions Teaching in live sessions according to the agreed time 💼 Software Developer | Code Coffee Creative (February 2020 - July 2022) Develope web API with java spring boot Develope meeting system for internal office Develope startup project (Pensiunku) 💼 Backend Developer | PT. DSS Consulting (February 2020 - July 2022) Develope a backend API for DPLK system using spring boot Develope email sender service for DPLK system using Django Projects\r  \r  \r  \r  \r  \r  \r  \r  \rSkills and Language\rTools and Platforms\rGithub Statistics\r \rGithub Profile Trophy\r","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"}]